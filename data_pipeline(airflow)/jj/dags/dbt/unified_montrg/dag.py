"""
Unified Monitoring ì‹ ê·œ ê¸°ëŠ¥ dbt í”„ë¡œì íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” Airflow DAG

ì‹ ê·œ ëª¨ë‹ˆí„°ë§/ë¶„ì„ ê¸°ëŠ¥ë“¤ì„ dbt ëª¨ë¸ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
ì¦ë¶„ ì²˜ë¦¬ ë° ë°±í•„ ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
"""

from datetime import datetime, timedelta, timezone, time
from typing import Optional, Dict, Any
import logging
import sys
import json
import subprocess

from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from cosmos import DbtTaskGroup, ProjectConfig, ProfileConfig, ExecutionConfig
from cosmos.profiles import PostgresUserPasswordProfileMapping

# ============================================================================
# ìƒìˆ˜ ì •ì˜
# ============================================================================

# ê²½ë¡œ ì„¤ì •
DBT_PROJECT_DIR = "/opt/airflow/dags/dbt/unified_montrg"
if DBT_PROJECT_DIR not in sys.path:
    sys.path.insert(0, DBT_PROJECT_DIR)

# Airflow ì„¤ì •
INCREMENT_KEY = "last_extract_time_unified_montrg"  # JSON í˜•íƒœë¡œ ëª¨ë“  ëª¨ë¸ì˜ ë‚ ì§œ ì €ì¥
# JSON í˜•ì‹: {"mart_productivity_by_op_cd": "20251203", "mart_productivity_by_op_group": "20251203", "mart_productivity_by_ip_zone": "20241201"}
KST_TZ = timezone(timedelta(hours=9))  # KST (UTC+9)
INITIAL_START_DATE = datetime(2024, 1, 1, 6, 30, 0, tzinfo=KST_TZ)  # 06:30:00 ì‹œì‘
DAILY_START_HOUR = 6  # ì¼ì¼ ë°ì´í„° ì‹œì‘ ì‹œê°„ (ì‹œ)
DAILY_START_MINUTE = 30  # ì¼ì¼ ë°ì´í„° ì‹œì‘ ì‹œê°„ (ë¶„)
DAYS_OFFSET_FOR_INCREMENTAL = 1  # ì¦ë¶„ ì²˜ë¦¬: ì‹¤í–‰ ë‚ ì§œ - 1ì¼ê¹Œì§€ë§Œ
DAYS_OFFSET_FOR_BACKFILL = 2  # ë°±í•„ ì²˜ë¦¬: ì‹¤í–‰ ë‚ ì§œ - 2ì¼ê¹Œì§€ë§Œ

# ëª¨ë¸ë³„ ê¸°ë³¸ ë‚ ì§œ (ìƒˆ ëª¨ë¸ ì¶”ê°€ ì‹œ ì—¬ê¸°ì— ì •ì˜)
MODEL_DEFAULT_DATES = {
    "mart_productivity_by_op_cd": INITIAL_START_DATE.date(),
    "mart_productivity_by_op_group": INITIAL_START_DATE.date(),
    "mart_productivity_by_ip_zone": INITIAL_START_DATE.date(),
    "mart_productivity_by_ph_zone": INITIAL_START_DATE.date(),
    "mart_productivity_by_ip_machine": INITIAL_START_DATE.date(),
    "mart_productivity_by_ph_machine": INITIAL_START_DATE.date(),
    "mart_downtime_by_line": INITIAL_START_DATE.date(),
    "mart_downtime_by_machine": INITIAL_START_DATE.date(),
}

# Incremental DAGì—ì„œ ì‹¤í–‰í•  ëª¨ë¸ ëª©ë¡ (productivity ëª¨ë¸ë“¤ë§Œ)
INCREMENTAL_MODELS = [
    "mart_productivity_by_op_cd",
    "mart_productivity_by_op_group",
    "mart_productivity_by_ip_zone",
    "mart_productivity_by_ph_zone",
    "mart_productivity_by_ip_machine",
    "mart_productivity_by_ph_machine",
    "mart_downtime_by_line",
    "mart_downtime_by_machine",
]

# Backfill DAGì—ì„œ ì‹¤í–‰í•  ëª¨ë¸ ëª©ë¡ (select ì˜µì…˜ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)
BACKFILL_MODELS = [
    "mart_productivity_by_op_cd",
    "mart_productivity_by_op_group",
    "mart_productivity_by_ip_zone",
    "mart_productivity_by_ph_zone",
    "mart_productivity_by_ip_machine",
    "mart_productivity_by_ph_machine",
    "mart_downtime_by_line",
    "mart_downtime_by_machine",
]  # downtime ëª¨ë¸ë“¤ë§Œ backfill

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
# ì£¼ì˜: ì‹¤ì œ ì—°ê²°ì€ profiles.ymlì˜ target ì„¤ì •ì— ë”°ë¼ ê²°ì •ë¨
POSTGRES_CONN_ID = "pg_jj_unified_montrg_dw"  # ê¸°ë³¸ Connection (ì‹¤ì œë¡œëŠ” profiles.ymlì˜ targetì´ ìš°ì„ )
SCHEMA = "silver"  # dbt ëª¨ë¸ ì €ì¥ ìŠ¤í‚¤ë§ˆ

# DAG ê¸°ë³¸ ì„¤ì •
DEFAULT_ARGS = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),  # timezone-naive (UTC ê¸°ì¤€)
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# dbt Profile ì„¤ì •
# unified_montrg_realtimeê³¼ ë™ì¼í•˜ê²Œ profile_mapping ì‚¬ìš©
# profiles.yml íŒŒì¼ì„ ì§ì ‘ ì‚¬ìš©í•˜ë ¤ë©´ profiles_yml_filepathë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ,
# profile_mappingì´ ë” ì•ˆì •ì ì´ê³  ê¶Œì¥ë˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤
PROFILE_CONFIG = ProfileConfig(
    profile_name="unified_montrg",
    target_name="dev",
    profile_mapping=PostgresUserPasswordProfileMapping(
        conn_id=POSTGRES_CONN_ID,
        profile_args={"schema": SCHEMA}
    ),
)

# dbt Execution ì„¤ì •
EXECUTION_CONFIG = ExecutionConfig(
    dbt_executable_path="dbt",
)

# ============================================================================
# ë‚ ì§œ ë²”ìœ„ ê³„ì‚° í•¨ìˆ˜
# ============================================================================

def parse_datetime(dt_str: str) -> datetime:
    """Parse datetime string and ensure timezone is set."""
    from dateutil.parser import parse
    dt = parse(dt_str)
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=KST_TZ)
    return dt


def get_model_date_from_variable(model_name: str) -> Optional[datetime.date]:
    """Variableì—ì„œ íŠ¹ì • ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ì²˜ë¦¬ ë‚ ì§œë¥¼ ê°€ì ¸ì˜´ (JSON í˜•íƒœ ë˜ëŠ” ê¸°ì¡´ ë‹¨ì¼ ë‚ ì§œ).
    
    ê¸°ì¡´ ë‹¨ì¼ ë‚ ì§œ ë¬¸ìì—´ í˜•ì‹ë„ ìë™ìœ¼ë¡œ JSONìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "mart_productivity_by_op_cd")
    
    Returns:
        ë§ˆì§€ë§‰ ì²˜ë¦¬ ë‚ ì§œ (date ê°ì²´), ì—†ìœ¼ë©´ None
    """
    try:
        var_value = Variable.get(INCREMENT_KEY, default_var=None)
        if not var_value:
            return None
        
        # ê¸°ì¡´ ë‹¨ì¼ ë‚ ì§œ ë¬¸ìì—´ í˜•ì‹ì¸ì§€ í™•ì¸ (yyyyMMdd ë˜ëŠ” yyyy-mm-dd)
        if isinstance(var_value, str):
            # JSON í˜•ì‹ì¸ì§€ í™•ì¸ (ì¤‘ê´„í˜¸ë¡œ ì‹œì‘)
            if var_value.strip().startswith('{'):
                try:
                    var_dict = json.loads(var_value)
                except json.JSONDecodeError:
                    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬
                    var_dict = None
            else:
                # ê¸°ì¡´ ë‹¨ì¼ ë‚ ì§œ ë¬¸ìì—´ í˜•ì‹
                var_dict = None
        else:
            var_dict = var_value
        
        # ê¸°ì¡´ ë‹¨ì¼ ë‚ ì§œ í˜•ì‹ì¸ ê²½ìš°, ëª¨ë“  ëª¨ë¸ì— ë™ì¼í•œ ë‚ ì§œ ì ìš©í•˜ê³  JSONìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
        if var_dict is None:
            # ê¸°ì¡´ ë‚ ì§œ íŒŒì‹±
            if isinstance(var_value, str) and len(var_value) == 8 and var_value.isdigit():
                # yyyyMMdd í˜•ì‹
                old_date = datetime.strptime(var_value, "%Y%m%d").date()
            else:
                # ê¸°ì¡´ í˜•ì‹ (yyyy-mm-dd ë˜ëŠ” datetime string)
                old_date = parse_datetime(var_value).date()
            
            # ëª¨ë“  ëª¨ë¸ì— ë™ì¼í•œ ë‚ ì§œë¡œ JSON ìƒì„± ë° ì €ì¥ (ë§ˆì´ê·¸ë ˆì´ì…˜)
            var_dict = {}
            for model in MODEL_DEFAULT_DATES.keys():
                var_dict[model] = old_date.strftime("%Y%m%d")
            Variable.set(INCREMENT_KEY, json.dumps(var_dict, ensure_ascii=False))
            logging.info(f"ğŸ”„ Variable ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: ê¸°ì¡´ ë‚ ì§œ '{old_date}'ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜")
        
        # ëª¨ë¸ë³„ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
        model_date_str = var_dict.get(model_name)
        if not model_date_str:
            return None
        
        # yyyyMMdd í˜•ì‹ íŒŒì‹±
        if isinstance(model_date_str, str) and len(model_date_str) == 8 and model_date_str.isdigit():
            return datetime.strptime(model_date_str, "%Y%m%d").date()
        else:
            # ê¸°ì¡´ í˜•ì‹ (yyyy-mm-dd)
            return parse_datetime(model_date_str).date()
    except (json.JSONDecodeError, KeyError, ValueError) as e:
        logging.warning(f"âš ï¸ Variable íŒŒì‹± ì˜¤ë¥˜ (ëª¨ë¸: {model_name}): {e}")
        return None


def update_model_date_in_variable(model_name: str, processed_date: datetime.date) -> None:
    """Variableì— íŠ¹ì • ëª¨ë¸ì˜ ì²˜ë¦¬ ë‚ ì§œë¥¼ ì—…ë°ì´íŠ¸ (JSON í˜•íƒœ).
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "mart_productivity_by_op_cd")
        processed_date: ì²˜ë¦¬ ì™„ë£Œ ë‚ ì§œ
    """
    try:
        # ê¸°ì¡´ Variable ê°€ì ¸ì˜¤ê¸°
        var_value = Variable.get(INCREMENT_KEY, default_var=None)
        if var_value:
            if isinstance(var_value, str):
                var_dict = json.loads(var_value)
            else:
                var_dict = var_value
        else:
            var_dict = {}
        
        # ëª¨ë¸ë³„ ë‚ ì§œ ì—…ë°ì´íŠ¸ (yyyyMMdd í˜•ì‹)
        var_dict[model_name] = processed_date.strftime("%Y%m%d")
        
        # Variable ì €ì¥
        Variable.set(INCREMENT_KEY, json.dumps(var_dict, ensure_ascii=False))
        logging.info(f"âœ… Variable '{INCREMENT_KEY}' ì—…ë°ì´íŠ¸: {model_name} = {processed_date.strftime('%Y%m%d')}")
    except Exception as e:
        logging.error(f"âŒ Variable ì—…ë°ì´íŠ¸ ì˜¤ë¥˜ (ëª¨ë¸: {model_name}): {e}")
        raise


def get_incremental_date_range(**context) -> Optional[Dict[str, str]]:
    """ì¦ë¶„ ì²˜ë¦¬ìš© ë‚ ì§œ ë²”ìœ„ ê³„ì‚° - Variable ë‚ ì§œë¶€í„° ë‹¤ìŒë‚ ê¹Œì§€ ì²˜ë¦¬.
    
    Variableì— ì €ì¥ëœ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ, í•´ë‹¹ ë‚ ì§œ 06:30:00ë¶€í„° ë‹¤ìŒë‚  06:30:00ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ì˜ˆ: Variableì´ 2025-12-03ì´ë©´, 2025-12-04 ê°’ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ 2025-12-03 06:30:00 ~ 2025-12-04 06:30:00 ë²”ìœ„ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ìµœëŒ€ (ì‹¤ì œ ì˜¤ëŠ˜ ë‚ ì§œ - 1ì¼)ê¹Œì§€ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ì‹¤í–‰ ì‹œì ê³¼ ë¬´ê´€í•˜ê²Œ, ì‹¤ì œ ì˜¤ëŠ˜ ë‚ ì§œ(KST) ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.
    ì¼ìš”ì¼ íœ´ë¬´ì¼ ë“± ë°ì´í„°ê°€ ì—†ì–´ë„ 0 ê°’ìœ¼ë¡œ ë ˆì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Returns:
        Dictionary with 'start_date' and 'end_date' strings (yyyyMMdd format), or None if no data to process
    """
    # DAG ì‹¤í–‰ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚° (data_interval_start ì‚¬ìš©, ì—†ìœ¼ë©´ execution_date)
    dag_run_date = context.get('data_interval_start') or context.get('execution_date')
    
    if dag_run_date:
        # data_interval_start/execution_dateê°€ timezone-awareê°€ ì•„ë‹ˆë©´ KSTë¡œ ì„¤ì •
        if isinstance(dag_run_date, str):
            dag_run_date = parse_datetime(dag_run_date)
        if dag_run_date.tzinfo is None:
            dag_run_date = dag_run_date.replace(tzinfo=KST_TZ)
        # UTCì—ì„œ KSTë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
        if dag_run_date.tzinfo != KST_TZ:
            dag_run_date = dag_run_date.astimezone(KST_TZ)
        execution_date_kst = dag_run_date.date()
    else:
        # Fallback: í˜„ì¬ ì‹œê°„ ê¸°ì¤€ (í•˜ì§€ë§Œ ì´ ê²½ìš°ëŠ” ë“œë¬¾)
        execution_date_kst = datetime.now(KST_TZ).date()
    
    logging.info(f"ğŸ“… DAG ì‹¤í–‰ ë‚ ì§œ (KST): {execution_date_kst}")
    
    # Incremental DAGì—ì„œ ì‹¤í–‰í•  ëª¨ë¸ë“¤ì˜ ìµœì†Œ ë‚ ì§œë¥¼ ì°¾ì•„ì„œ ì²˜ë¦¬ (ê°€ì¥ ë’¤ì²˜ì§„ ëª¨ë¸ ê¸°ì¤€)
    # Incremental DAGëŠ” INCREMENTAL_MODELSì— ì •ì˜ëœ ëª¨ë¸ë“¤ë§Œ ì‹¤í–‰í•˜ë¯€ë¡œ, í•´ë‹¹ ëª¨ë¸ë“¤ì˜ ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œë¶€í„° ì²˜ë¦¬
    all_model_dates = {}
    min_date = None
    
    for model_name in INCREMENTAL_MODELS:
        model_date = get_model_date_from_variable(model_name)
        if model_date:
            all_model_dates[model_name] = model_date
            if min_date is None or model_date < min_date:
                min_date = model_date
        else:
            # Variableì— ì—†ìœ¼ë©´ ê¸°ë³¸ ë‚ ì§œ ì‚¬ìš©
            default_date = MODEL_DEFAULT_DATES[model_name]
            all_model_dates[model_name] = default_date
            if min_date is None or default_date < min_date:
                min_date = default_date
    
    if min_date is None:
        min_date = INITIAL_START_DATE.date()
    
    logging.info(f"ğŸ“… ëª¨ë¸ë³„ ë‚ ì§œ: {all_model_dates}")
    logging.info(f"ğŸ“… ìµœì†Œ ë‚ ì§œ (ì²˜ë¦¬ ê¸°ì¤€): {min_date}")
    
    # ì²˜ë¦¬í•  ë‚ ì§œ: ìµœì†Œ ë‚ ì§œ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë‹¤ìŒë‚ ì´ ì•„ë‹˜)
    # min_dateê°€ 2025-12-03ì´ë©´, 2025-12-04 ê°’ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ 2025-12-03 ~ 2025-12-04 ë²”ìœ„ ì²˜ë¦¬
    target_date = min_date
    
    # ìµœëŒ€ ì œí•œ: (ì‹¤ì œ ì˜¤ëŠ˜ ë‚ ì§œ - 1ì¼)ê¹Œì§€ë§Œ ì²˜ë¦¬
    # ì‹¤í–‰ ì‹œì ê³¼ ë¬´ê´€í•˜ê²Œ, ì‹¤ì œ ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
    today_kst = datetime.now(KST_TZ).date()
    max_end_date = today_kst - timedelta(days=DAYS_OFFSET_FOR_INCREMENTAL)
    
    logging.info(f"ğŸ“… ì˜¤ëŠ˜ ë‚ ì§œ (KST): {today_kst}, ìµœëŒ€ ì²˜ë¦¬ ê°€ëŠ¥ ë‚ ì§œ: {max_end_date}")
    
    # target_dateê°€ max_end_dateë³´ë‹¤ í¬ë©´ ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ
    # Variableì´ 2025-12-04ì´ê³  max_end_dateê°€ 2025-12-04ì´ë©´, 2025-12-04 ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
    if target_date > max_end_date:
        logging.info(f"âš ï¸ ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ: target_date({target_date}) > max_end_date({max_end_date})")
        return None
    
    # 06:30:00 ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ë²”ìœ„ ì„¤ì •
    # Variableì´ 2025-12-03ì´ë©´, 2025-12-03 06:30:00 ~ 2025-12-04 06:30:00
    start_date = datetime.combine(target_date, time(DAILY_START_HOUR, DAILY_START_MINUTE)).replace(tzinfo=KST_TZ)
    end_date = start_date + timedelta(days=1)
    
    logging.info(f"ğŸ“… ì¦ë¶„ ì²˜ë¦¬: {target_date} 06:30:00 ~ {target_date + timedelta(days=1)} 06:30:00 ë°ì´í„° ì²˜ë¦¬ (ìµœì†Œ ë‚ ì§œ: {min_date}, DAG ì‹¤í–‰ ë‚ ì§œ: {execution_date_kst})")
    
    # yyyyMMdd í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return {
        "start_date": target_date.strftime("%Y%m%d"),
        "end_date": (target_date + timedelta(days=1)).strftime("%Y%m%d")
    }


def get_backfill_date_range(**context) -> Optional[Dict[str, str]]:
    """ë°±í•„ ì²˜ë¦¬ìš© ë‚ ì§œ ë²”ìœ„ ê³„ì‚° - ì´ˆê¸° ë‚ ì§œë¶€í„° (ì‹¤í–‰ ë‚ ì§œ-2ì¼)ê¹Œì§€.
    
    Backfill DAGëŠ” selectëœ ëª¨ë¸ë§Œ ì‹¤í–‰í•˜ë¯€ë¡œ, í•´ë‹¹ ëª¨ë¸ì˜ Variableë§Œ ì‚¬ìš©.
    ëª¨ë¸ ì´ë¦„ì€ dbt selectì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜, ê¸°ë³¸ê°’ìœ¼ë¡œ mart_productivity_by_ip_zone ì‚¬ìš©.
    
    Returns:
        Dictionary with 'backfill_start_date' and 'backfill_end_date' strings (yyyyMMdd format), or None if no data to process
    """
    # DAG ì‹¤í–‰ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
    dag_run_date = context.get('data_interval_start') or context.get('execution_date')
    
    if dag_run_date:
        if isinstance(dag_run_date, str):
            dag_run_date = parse_datetime(dag_run_date)
        if dag_run_date.tzinfo is None:
            dag_run_date = dag_run_date.replace(tzinfo=KST_TZ)
        if dag_run_date.tzinfo != KST_TZ:
            dag_run_date = dag_run_date.astimezone(KST_TZ)
        execution_date_kst = dag_run_date.date()
    else:
        execution_date_kst = datetime.now(KST_TZ).date()
    
    # Backfill DAGëŠ” BACKFILL_MODELSì— ì •ì˜ëœ ëª¨ë¸ë“¤ë§Œ ì‹¤í–‰
    # ëª¨ë“  ëª¨ë¸ì˜ ìµœì†Œ ë‚ ì§œë¥¼ ì°¾ì•„ì„œ ì²˜ë¦¬ (ê°€ì¥ ë’¤ì²˜ì§„ ëª¨ë¸ ê¸°ì¤€)
    all_model_dates = {}
    min_date = None
    min_model_name = None
    
    for model_name in BACKFILL_MODELS:
        model_date = get_model_date_from_variable(model_name)
        if not model_date:
            # Variableì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë‚ ì§œ ì‚¬ìš©
            model_date = MODEL_DEFAULT_DATES.get(model_name, INITIAL_START_DATE.date())
            logging.info(f"ğŸ“… Variableì´ ì—†ìŒ. ëª¨ë¸ '{model_name}' ê¸°ë³¸ ë‚ ì§œ ì‚¬ìš©: {model_date}")
        else:
            logging.info(f"ğŸ“… Variableì—ì„œ ëª¨ë¸ '{model_name}' ë§ˆì§€ë§‰ ì²˜ë¦¬ ë‚ ì§œ ê°€ì ¸ì˜´: {model_date}")
        
        all_model_dates[model_name] = model_date
        if min_date is None or model_date < min_date:
            min_date = model_date
            min_model_name = model_name
    
    if min_date is None:
        min_date = INITIAL_START_DATE.date()
        min_model_name = BACKFILL_MODELS[0] if BACKFILL_MODELS else None
    
    # ì¢…ë£Œ ë‚ ì§œ: (ì‹¤í–‰ ë‚ ì§œ - 2ì¼)
    end_date = execution_date_kst - timedelta(days=DAYS_OFFSET_FOR_BACKFILL)
    
    if min_date >= end_date:
        logging.info(f"âš ï¸ ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ: {min_date} >= {end_date}")
        return None
    
    logging.info(f"ğŸ“… ë°±í•„ ì²˜ë¦¬ ë²”ìœ„ (ëª¨ë¸: {BACKFILL_MODELS}, ìµœì†Œ ë‚ ì§œ: {min_date} from {min_model_name}): {min_date} ~ {end_date}")
    
    return {
        "backfill_start_date": min_date.strftime("%Y%m%d"),
        "backfill_end_date": end_date.strftime("%Y%m%d"),
        "model_names": BACKFILL_MODELS  # ëª¨ë¸ ì´ë¦„ ëª©ë¡ ë°˜í™˜
    }


# ============================================================================
# Variable ì—…ë°ì´íŠ¸ í•¨ìˆ˜
# ============================================================================

def update_variable_after_run(**context) -> None:
    """ì²˜ë¦¬ ì™„ë£Œ í›„ Variable ì—…ë°ì´íŠ¸ (Incremental).
    
    Incremental DAGëŠ” INCREMENTAL_MODELSì— ì •ì˜ëœ ëª¨ë¸ë“¤ë§Œ ì‹¤í–‰í•˜ë¯€ë¡œ, í•´ë‹¹ ëª¨ë¸ë“¤ì˜ ë‚ ì§œë§Œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    ì²˜ë¦¬í•œ ë‚ ì§œì˜ ë‹¤ìŒë‚ ë¡œ ê° ëª¨ë¸ì˜ Variableì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    ì˜ˆ: 2025-12-03 ~ 2025-12-04ë¥¼ ì²˜ë¦¬í–ˆë‹¤ë©´, productivity ëª¨ë¸ë“¤ì˜ Variableì„ 2025-12-04ë¡œ ì—…ë°ì´íŠ¸
    ë°ì´í„°ê°€ ì—†ì–´ë„ (ì˜ˆ: ì¼ìš”ì¼ íœ´ë¬´) Variableì„ ì—…ë°ì´íŠ¸í•˜ì—¬ ë‹¤ìŒë‚  ì •ìƒ ì²˜ë¦¬ë˜ë„ë¡ í•¨.
    """
    date_range = context['ti'].xcom_pull(task_ids='get_date_range')
    if date_range:
        # ì²˜ë¦¬í•œ ë‚ ì§œëŠ” end_date (ë‹¤ìŒë‚  ë‚ ì§œ)
        # yyyyMMdd í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ë˜ë¯€ë¡œ íŒŒì‹±
        end_date_str = date_range["end_date"]
        if len(end_date_str) == 8 and end_date_str.isdigit():
            # yyyyMMdd í˜•ì‹
            processed_date = datetime.strptime(end_date_str, "%Y%m%d").date()
        else:
            # ê¸°ì¡´ í˜•ì‹ (yyyy-mm-dd)
            processed_date = parse_datetime(end_date_str).date()
        
        # Incremental DAGì—ì„œ ì‹¤í–‰í•œ ëª¨ë¸ë“¤ì˜ ë‚ ì§œë§Œ ì—…ë°ì´íŠ¸
        for model_name in INCREMENTAL_MODELS:
            update_model_date_in_variable(model_name, processed_date)
        
        logging.info(f"âœ… Incremental ëª¨ë¸ë“¤ Variable ì—…ë°ì´íŠ¸ ì™„ë£Œ: {processed_date.strftime('%Y%m%d')}")
    else:
        # ë‚ ì§œ ë²”ìœ„ê°€ ì—†ìœ¼ë©´ Variable ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ (ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ì—ˆìŒ)
        logging.warning(f"âš ï¸ ë‚ ì§œ ë²”ìœ„ê°€ ì—†ì–´ Variableì„ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ")


def update_backfill_variable(**context) -> None:
    """ë°±í•„ ì²˜ë¦¬ ì™„ë£Œ í›„ Variable ì—…ë°ì´íŠ¸.
    
    Backfill DAGëŠ” BACKFILL_MODELSì— ì •ì˜ëœ ëª¨ë¸ë“¤ë§Œ ì‹¤í–‰í•˜ë¯€ë¡œ, í•´ë‹¹ ëª¨ë¸ë“¤ì˜ Variableë§Œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    ë°ì´í„°ê°€ ì—†ì–´ë„ (ì˜ˆ: ì¼ìš”ì¼ íœ´ë¬´) Variableì„ ì—…ë°ì´íŠ¸í•˜ì—¬ ë‹¤ìŒë‚  ì •ìƒ ì²˜ë¦¬ë˜ë„ë¡ í•¨.
    """
    date_range = context['ti'].xcom_pull(task_ids='get_backfill_date_range')
    if date_range:
        # ëª¨ë¸ ì´ë¦„ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        model_names = date_range.get("model_names", BACKFILL_MODELS)
        
        # yyyyMMdd í˜•ì‹ìœ¼ë¡œ íŒŒì‹±
        end_date_str = date_range["backfill_end_date"]
        if len(end_date_str) == 8 and end_date_str.isdigit():
            processed_date = datetime.strptime(end_date_str, "%Y%m%d").date()
        else:
            processed_date = parse_datetime(end_date_str).date()
        
        # ëª¨ë“  backfill ëª¨ë¸ì˜ Variable ì—…ë°ì´íŠ¸
        for model_name in model_names:
            update_model_date_in_variable(model_name, processed_date)
        
        logging.info(f"âœ… Backfill ëª¨ë¸ë“¤ Variable ì—…ë°ì´íŠ¸ ì™„ë£Œ: {model_names} = {processed_date.strftime('%Y%m%d')}")
    else:
        logging.warning(f"âš ï¸ ë‚ ì§œ ë²”ìœ„ê°€ ì—†ì–´ Variableì„ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ")


# ============================================================================
# DAG ì •ì˜
# ============================================================================

# Incremental DAG
with DAG(
    dag_id="dbt_unified_montrg_incremental",
    default_args=DEFAULT_ARGS,
    description="Unified Monitoring ì‹ ê·œ ê¸°ëŠ¥ - ì¦ë¶„ ì²˜ë¦¬",
    schedule_interval="30 0 * * *",  # ë§¤ì¼ 00:30 (UTC) ì‹¤í–‰ 
    catchup=False,
    tags=["dbt", "unified_montrg", "monitoring", "incremental"],
) as incremental_dag:
    
    get_date_range = PythonOperator(
        task_id="get_date_range",
        python_callable=get_incremental_date_range,
    )
    
    def prepare_dbt_vars(**context):
        """dbt ì‹¤í–‰ì— í•„ìš”í•œ ë³€ìˆ˜ ì¤€ë¹„."""
        date_range = context['ti'].xcom_pull(task_ids='get_date_range')
        if date_range:
            return {
                "start_date": date_range["start_date"],
                "end_date": date_range["end_date"]
            }
        return {}
    
    prepare_vars = PythonOperator(
        task_id="prepare_dbt_vars",
        python_callable=prepare_dbt_vars,
    )
    
    dbt_task = DbtTaskGroup(
        group_id="dbt_unified_montrg",
        project_config=ProjectConfig(DBT_PROJECT_DIR),
        profile_config=PROFILE_CONFIG,
        execution_config=EXECUTION_CONFIG,
        operator_args={
            "vars": "{{ ti.xcom_pull(task_ids='prepare_dbt_vars') }}",
            "select": INCREMENTAL_MODELS,  # Productivity ëª¨ë¸ë“¤ë§Œ ì‹¤í–‰
        },
    )
    
    # dbt test ì‹¤í–‰ (ëª¨ë¸ ì‹¤í–‰ í›„ í…ŒìŠ¤íŠ¸)
    def run_dbt_test(**context):
        """dbt test ì‹¤í–‰ (ì¶œë ¥ ë¡œê¹… í¬í•¨)"""
        import subprocess
        test_models_str = ' '.join(INCREMENTAL_MODELS)
        cmd = [
            "dbt", "test",
            "--profiles-dir", DBT_PROJECT_DIR,
            "--profile", "unified_montrg",
            "--target", "dev",
            "--select", test_models_str
        ]
        
        logging.info(f"ğŸ” dbt test ì‹¤í–‰: {' '.join(cmd)}")
        
        try:
            result = subprocess.run(
                cmd,
                cwd=DBT_PROJECT_DIR,
                capture_output=True,
                text=True,
                check=False  # ì‹¤íŒ¨í•´ë„ ì˜ˆì™¸ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ
            )
            
            # ì¶œë ¥ ë¡œê¹…
            if result.stdout:
                logging.info(f"âœ… dbt test stdout:\n{result.stdout}")
            if result.stderr:
                logging.warning(f"âš ï¸ dbt test stderr:\n{result.stderr}")
            
            # ë°˜í™˜ ì½”ë“œ í™•ì¸
            if result.returncode != 0:
                logging.error(f"âŒ dbt test ì‹¤íŒ¨ (return code: {result.returncode})")
                # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰ (ì„ íƒì‚¬í•­)
                # raise Exception(f"dbt test ì‹¤íŒ¨: {result.stderr}")
            else:
                logging.info("âœ… dbt test ì„±ê³µ")
                
            return {"returncode": result.returncode, "output": result.stdout}
        except Exception as e:
            logging.error(f"âŒ dbt test ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    dbt_test = PythonOperator(
        task_id="dbt_test",
        python_callable=run_dbt_test,
    )
    
    update_var = PythonOperator(
        task_id="update_variable",
        python_callable=update_variable_after_run,
    )
    
    get_date_range >> prepare_vars >> dbt_task >> dbt_test >> update_var


# Backfill DAG
with DAG(
    dag_id="dbt_unified_montrg_backfill",
    default_args=DEFAULT_ARGS,
    description="Unified Monitoring ì‹ ê·œ ê¸°ëŠ¥ - ë°±í•„ ì²˜ë¦¬ (ì´ˆê¸° ë‚ ì§œë¶€í„° ì˜¤ëŠ˜-2ì¼ê¹Œì§€)",
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰
    catchup=False,
    tags=["dbt", "unified_montrg", "monitoring", "backfill"],
) as backfill_dag:
    
    get_backfill_range = PythonOperator(
        task_id="get_backfill_date_range",
        python_callable=get_backfill_date_range,
    )
    
    def prepare_backfill_vars(**context):
        """dbt ì‹¤í–‰ì— í•„ìš”í•œ ë³€ìˆ˜ ì¤€ë¹„."""
        date_range = context['ti'].xcom_pull(task_ids='get_backfill_date_range')
        if date_range:
            return {
                "backfill_start_date": date_range["backfill_start_date"],
                "backfill_end_date": date_range["backfill_end_date"]
            }
        return {}
    
    prepare_backfill_vars_task = PythonOperator(
        task_id="prepare_backfill_vars",
        python_callable=prepare_backfill_vars,
    )
    
    dbt_backfill_task = DbtTaskGroup(
        group_id="dbt_unified_montrg_backfill",
        project_config=ProjectConfig(DBT_PROJECT_DIR),
        profile_config=PROFILE_CONFIG,
        execution_config=EXECUTION_CONFIG,
        operator_args={
            "vars": "{{ ti.xcom_pull(task_ids='prepare_backfill_vars') }}",
            "full_refresh": True,
            # BACKFILL_MODELSì— ì •ì˜ëœ ëª¨ë¸ë“¤ë§Œ backfill ì‹¤í–‰
            "select": BACKFILL_MODELS,  # ìƒˆ ëª¨ë¸ë§Œ ì‹¤í–‰
        },
    )
    
    # dbt test ì‹¤í–‰ (ëª¨ë¸ ì‹¤í–‰ í›„ í…ŒìŠ¤íŠ¸)
    def run_dbt_backfill_test(**context):
        """dbt test ì‹¤í–‰ (ì¶œë ¥ ë¡œê¹… í¬í•¨)"""
        import subprocess
        test_models_str = ' '.join(BACKFILL_MODELS)
        cmd = [
            "dbt", "test",
            "--profiles-dir", DBT_PROJECT_DIR,
            "--profile", "unified_montrg",
            "--target", "dev",
            "--select", test_models_str
        ]
        
        logging.info(f"ğŸ” dbt test ì‹¤í–‰ (backfill): {' '.join(cmd)}")
        
        try:
            result = subprocess.run(
                cmd,
                cwd=DBT_PROJECT_DIR,
                capture_output=True,
                text=True,
                check=False  # ì‹¤íŒ¨í•´ë„ ì˜ˆì™¸ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ
            )
            
            # ì¶œë ¥ ë¡œê¹…
            if result.stdout:
                logging.info(f"âœ… dbt test stdout:\n{result.stdout}")
            if result.stderr:
                logging.warning(f"âš ï¸ dbt test stderr:\n{result.stderr}")
            
            # ë°˜í™˜ ì½”ë“œ í™•ì¸
            if result.returncode != 0:
                logging.error(f"âŒ dbt test ì‹¤íŒ¨ (return code: {result.returncode})")
                # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰ (ì„ íƒì‚¬í•­)
                # raise Exception(f"dbt test ì‹¤íŒ¨: {result.stderr}")
            else:
                logging.info("âœ… dbt test ì„±ê³µ")
                
            return {"returncode": result.returncode, "output": result.stdout}
        except Exception as e:
            logging.error(f"âŒ dbt test ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    dbt_backfill_test = PythonOperator(
        task_id="dbt_test",
        python_callable=run_dbt_backfill_test,
    )
    
    update_backfill_var = PythonOperator(
        task_id="update_backfill_variable",
        python_callable=update_backfill_variable,
    )
    
    get_backfill_range >> prepare_backfill_vars_task >> dbt_backfill_task >> dbt_backfill_test >> update_backfill_var

