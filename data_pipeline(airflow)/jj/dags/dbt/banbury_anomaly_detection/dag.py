from __future__ import annotations

"""
Banbury ê³µì • ì´ìƒ ê°ì§€ dbt í”„ë¡œì íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” Airflow DAG

ì¦ë¶„ ì²˜ë¦¬ ë° ë°±í•„ ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
- Incremental: Airflow Variableì—ì„œ ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ì„ ì½ì–´ì„œ ì¦ë¶„ ì²˜ë¦¬
- Backfill: ì´ˆê¸° ë‚ ì§œë¶€í„° ì§€ì •ëœ ë‚ ì§œê¹Œì§€ ì¼ê´„ ì²˜ë¦¬
"""

from datetime import datetime, timedelta, timezone
from typing import Optional, Dict, Any, Tuple
import logging
import sys
from dateutil.parser import parse

from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from cosmos import DbtTaskGroup, ProjectConfig, ProfileConfig, ExecutionConfig
from cosmos.profiles import PostgresUserPasswordProfileMapping
from plugins.hooks.postgres_hook import PostgresHelper

# ============================================================================
# ìƒìˆ˜ ì •ì˜
# ============================================================================

# ê²½ë¡œ ì„¤ì •
DBT_PROJECT_DIR = "/opt/airflow/dags/dbt/banbury_anomaly_detection"
if DBT_PROJECT_DIR not in sys.path:
    sys.path.insert(0, DBT_PROJECT_DIR)

# ì›ë³¸ ë¡œì§ ëª¨ë“ˆì€ ëŸ°íƒ€ì„ì— import (DAG import ì‹œê°„ ë‹¨ì¶•)

# Airflow ì„¤ì •
INCREMENT_KEY = "last_extract_time_banbury_anomaly_detection"
INDO_TZ = timezone(timedelta(hours=7))
INITIAL_START_DATE = datetime(2025, 1, 1, 6, 30, 0, tzinfo=INDO_TZ)
DAYS_OFFSET_FOR_INCREMENTAL = 1  # ì¦ë¶„ ì²˜ë¦¬: ì˜¤ëŠ˜ - 1ì¼ê¹Œì§€ë§Œ
DAYS_OFFSET_FOR_BACKFILL = 2  # ë°±í•„ ì²˜ë¦¬: ì˜¤ëŠ˜ - 2ì¼ê¹Œì§€ë§Œ

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
POSTGRES_CONN_ID = "pg_jj_telemetry_dw"
SCHEMA = "silver"  # dbt ëª¨ë¸ ì €ì¥ ìŠ¤í‚¤ë§ˆ

# CNN ëª¨ë¸ ì„¤ì •
MODEL_PATH = "/opt/airflow/models/cnn_anomaly_classifier.h5"
ANOMALY_THRESHOLD = 0.1
NUM_CHANNELS = 2
SEQUENCE_LENGTH = 500  # CNN ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´

# ì»¬ëŸ¼ëª… ì •ì˜
CYCLE_ID_COL = "cycle_id"
MOTOR_COL = "motor"
TEMP_COL = "temperature"

# dbt ëª¨ë¸ í…Œì´ë¸”ëª…
TABLE_BANBURY_CYCLES = "banbury_cycles"

# DAG ê¸°ë³¸ ì„¤ì •
DEFAULT_ARGS = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1, tzinfo=INDO_TZ),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# dbt Profile ì„¤ì •
PROFILE_CONFIG = ProfileConfig(
    profile_name="banbury_anomaly_detection",
    target_name="dev",
    profile_mapping=PostgresUserPasswordProfileMapping(
        conn_id=POSTGRES_CONN_ID,
        profile_args={"schema": SCHEMA}
    ),
)

# dbt Execution ì„¤ì •
EXECUTION_CONFIG = ExecutionConfig(
    dbt_executable_path="dbt",
)

# ============================================================================
# ë‚ ì§œ ë²”ìœ„ ê³„ì‚° í•¨ìˆ˜
# ============================================================================

def parse_datetime(dt_str: str) -> datetime:
    """Parse datetime string and ensure timezone is set.
    
    Args:
        dt_str: Datetime string to parse
    
    Returns:
        Datetime object with INDO_TZ timezone
    """
    dt = parse(dt_str)
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=INDO_TZ)
    return dt


def _normalize_to_0630(dt: datetime) -> datetime:
    """Normalize datetime to 06:30:00.
    
    Args:
        dt: Datetime to normalize
    
    Returns:
        Datetime with hour=6, minute=30, second=0, microsecond=0
    """
    return dt.replace(hour=6, minute=30, second=0, microsecond=0)


def get_incremental_date_range(**context) -> Optional[Dict[str, str]]:
    """ì¦ë¶„ ì²˜ë¦¬ìš© ë‚ ì§œ ë²”ìœ„ ê³„ì‚° - Variable ì‹œì ì—ì„œ 1ì¼ì¹˜ ì²˜ë¦¬.
    
    Variableì´ 2025-12-15 06:30:00ì´ë©´ 2025-12-15 06:30:00 ~ 2025-12-16 06:30:00 ì²˜ë¦¬
    ìµœëŒ€ ì œí•œ: í˜„ì¬ ì‹œê°„ - 1ì¼ê¹Œì§€ë§Œ ì²˜ë¦¬ (ë„ˆë¬´ ìµœì‹  ë°ì´í„°ëŠ” ì œì™¸)
    
    Returns:
        Dictionary with 'start_date' and 'end_date' strings, or None if no data to process
    """
    last_extract_time = Variable.get(INCREMENT_KEY, default_var=None)
    
    if not last_extract_time:
        raise ValueError(f"Variable '{INCREMENT_KEY}' not found. Please run backfill DAG first.")
    
    # ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ ì´í›„ë¶€í„° í•˜ë£¨ì¹˜ ì²˜ë¦¬
    last_time = parse_datetime(last_extract_time)
    start_date = _normalize_to_0630(last_time)
    end_date = _normalize_to_0630(last_time + timedelta(days=1))
    
    logging.info(f"ğŸ“Œ ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„: {last_extract_time} â†’ ì²˜ë¦¬ ë²”ìœ„: {start_date} ~ {end_date}")
    
    # ìµœëŒ€ ì œí•œ: í˜„ì¬ ì¸ë„ë„¤ì‹œì•„ ì‹œê°„ê¹Œì§€ë§Œ ì²˜ë¦¬ (ì˜¤ëŠ˜ì„ ë„˜ì§€ ì•ŠìŒ)
    now_indo = datetime.now(INDO_TZ)
    max_end_date = _normalize_to_0630(now_indo)
    
    # end_dateê°€ í˜„ì¬ ì‹œê°„ì„ ë„˜ìœ¼ë©´ í˜„ì¬ ì‹œê°„ê¹Œì§€ë§Œ ì²˜ë¦¬
    if end_date > max_end_date:
        logging.info(f"âš ï¸ end_date({end_date})ê°€ í˜„ì¬ ì‹œê°„({max_end_date})ì„ ì´ˆê³¼í•˜ì—¬ ì¡°ì •")
        end_date = max_end_date
        # ì¡°ì • í›„ì—ë„ start_date >= end_dateê°€ ë˜ë©´ ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ
        if start_date >= end_date:
            logging.info(f"âš ï¸ ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ: start_date({start_date}) >= end_date({end_date})")
            return None
    
    if start_date >= end_date:
        logging.info(f"âš ï¸ ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ: {start_date} >= {end_date}")
        return None
    
    logging.info(f"ğŸ“… ì¦ë¶„ ì²˜ë¦¬ ë²”ìœ„: {start_date} ~ {end_date}")
    
    return {
        "start_date": start_date.strftime("%Y-%m-%d %H:%M:%S"),
        "end_date": end_date.strftime("%Y-%m-%d %H:%M:%S")
    }


def get_backfill_date_range(**context) -> Optional[Dict[str, str]]:
    """ë°±í•„ ì²˜ë¦¬ìš© ë‚ ì§œ ë²”ìœ„ ê³„ì‚° - 2025-01-01ë¶€í„° (ì˜¤ëŠ˜-2ì¼)ê¹Œì§€.
    
    Returns:
        Dictionary with 'backfill_start_date' and 'backfill_end_date' strings, or None if no data to process
    """
    last_extract_time = Variable.get(INCREMENT_KEY, default_var=None)
    
    if not last_extract_time:
        start_date = INITIAL_START_DATE
        logging.info(f"ì´ˆê¸° ì‹œì‘ ë‚ ì§œ ì‚¬ìš©: {start_date}")
    else:
        last_time = parse_datetime(last_extract_time)
        start_date = _normalize_to_0630(last_time + timedelta(days=1))
        logging.info(f"ì´ì „ ì§„í–‰ ì§€ì  ì‚¬ìš©: {start_date}")
    
    # ì¢…ë£Œ ì‹œê°„: (ì˜¤ëŠ˜ - 2ì¼) 06:30
    end_date = _normalize_to_0630(
        datetime.now(INDO_TZ) - timedelta(days=DAYS_OFFSET_FOR_BACKFILL)
    )
    
    if start_date >= end_date:
        logging.info(f"âš ï¸ ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ: {start_date} >= {end_date}")
        return None
    
    logging.info(f"ğŸ“… ë°±í•„ ì²˜ë¦¬ ë²”ìœ„: {start_date} ~ {end_date}")
    
    return {
        "backfill_start_date": start_date.strftime("%Y-%m-%d %H:%M:%S"),
        "backfill_end_date": end_date.strftime("%Y-%m-%d %H:%M:%S")
    }


# ============================================================================
# Variable ì—…ë°ì´íŠ¸ í•¨ìˆ˜
# ============================================================================

def update_variable_after_run(**context) -> None:
    """ì²˜ë¦¬ ì™„ë£Œ í›„ Variable ì—…ë°ì´íŠ¸ (Incremental)."""
    date_range = context['ti'].xcom_pull(task_ids='get_date_range')
    if date_range:
        Variable.set(INCREMENT_KEY, date_range["end_date"])
        logging.info(f"âœ… Variable '{INCREMENT_KEY}' ì—…ë°ì´íŠ¸: {date_range['end_date']}")


def update_backfill_variable(**context) -> None:
    """ë°±í•„ ì²˜ë¦¬ ì™„ë£Œ í›„ Variable ì—…ë°ì´íŠ¸."""
    date_range = context['ti'].xcom_pull(task_ids='get_backfill_date_range')
    if date_range:
        Variable.set(INCREMENT_KEY, date_range["backfill_end_date"])
        logging.info(f"âœ… Variable '{INCREMENT_KEY}' ì—…ë°ì´íŠ¸: {date_range['backfill_end_date']}")


# ============================================================================
# ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================

# ============================================================================
# í—¬í¼ í•¨ìˆ˜
# ============================================================================

def _get_date_range_from_context(context: Dict[str, Any]) -> tuple[str, str] | None:
    """ë‚ ì§œ ë²”ìœ„ë¥¼ contextì—ì„œ ê°€ì ¸ì˜¤ê¸° (incremental ë˜ëŠ” backfill).
    
    Args:
        context: Airflow context
    
    Returns:
        Tuple of (start_date, end_date) or None if not found
    """
    date_range = (
        context['ti'].xcom_pull(task_ids='get_date_range') 
        or context['ti'].xcom_pull(task_ids='get_backfill_date_range')
    )
    
    if not date_range:
        return None
    
    start_date = date_range.get("start_date") or date_range.get("backfill_start_date")
    end_date = date_range.get("end_date") or date_range.get("backfill_end_date")
    
    if not start_date or not end_date:
        return None
    
    return start_date, end_date

def run_cnn_inference(**context) -> None:
    """CNN ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ìƒ ê°ì§€ ì¶”ë¡  ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥.
    
    ì£¼ìš” ë‹¨ê³„:
    1. banbury_segments ë¡œë“œ
    2. ì‚¬ì´í´ ê³„ì‚° ë° ì €ì¥
    3. PLC ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
    4. CNN ì¶”ë¡ 
    5. ê²°ê³¼ ì €ì¥
    """
    pg = PostgresHelper(conn_id=POSTGRES_CONN_ID)
    
    # ë‚ ì§œ ë²”ìœ„ ê°€ì ¸ì˜¤ê¸°
    date_range = _get_date_range_from_context(context)
    if not date_range:
        logging.warning("âš ï¸ ë‚ ì§œ ë²”ìœ„ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    start_date, end_date = date_range
    
    # 1. banbury_segments ë¡œë“œ
    df_segments = _load_segments_from_db(pg, start_date, end_date, context)
    if df_segments is None:
        return
    
    # 2. ì‚¬ì´í´ ê³„ì‚° ë° ì €ì¥
    df_result = _calculate_cycles(df_segments)
    if df_result is None or df_result.empty:
        logging.warning(f"âš ï¸ ì‚¬ì´í´ì´ ì—†ì–´ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œ ë²”ìœ„: {start_date} ~ {end_date}")
        return
    
    _save_cycles_to_db(pg, df_result, start_date, end_date)
    
    # 3. PLC ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ë° í–‰ë ¬ ë³€í™˜
    df_plc_seg = _build_plc_segments(df_segments, df_result)
    X = _build_cycle_matrix(df_plc_seg)
    
    # 4. CNN ì¶”ë¡ 
    prob = _run_cnn_prediction(X)
    
    # 5. ê²°ê³¼ ì¤€ë¹„ ë° ì €ì¥ (ì›ë³¸ê³¼ ë™ì¼: df_resultì™€ probë¥¼ ê·¸ëŒ€ë¡œ ê²°í•©)
    df_final = _prepare_final_result(df_result, prob)
    
    
    _save_anomaly_results(pg, df_final)


def _load_segments_from_db(
    pg: PostgresHelper, 
    start_date: str, 
    end_date: str, 
    context: Dict[str, Any]
) -> Optional[pd.DataFrame]:
    """dbt ëª¨ë¸ì—ì„œ banbury_segments ë¡œë“œ."""
    import pandas as pd

    logging.info(f"ğŸ“¥ banbury_segments ë¡œë“œ ì¤‘: {start_date} ~ {end_date}")
    segments_sql = f"""
        SELECT 
            prod_set_id,
            collection_timestamp,
            motor_current,
            chamber_temperature,
            mixer_run,
            run_mode,
            process_stage,
            drop_door_position
        FROM {SCHEMA}.banbury_segments
        WHERE collection_timestamp >= '{start_date}'::timestamp
          AND collection_timestamp < '{end_date}'::timestamp
        ORDER BY prod_set_id, collection_timestamp
    """
    df_segments = pg.execute_query(segments_sql, task_id="get_segments", xcom_key=None, ti=context.get('ti'))
    
    if not df_segments:
        logging.warning("âš ï¸ banbury_segments ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    df_segments = pd.DataFrame(df_segments, columns=[
        'prod_set_id', 'collection_timestamp', 'motor_current', 'chamber_temperature',
        'mixer_run', 'run_mode', 'process_stage', 'drop_door_position'
    ])
    df_segments['collection_timestamp'] = pd.to_datetime(df_segments['collection_timestamp'])
    
    n_rows = len(df_segments)
    n_segments = df_segments['prod_set_id'].nunique()
    logging.info(f"âœ… banbury_segments ë¡œë“œ ì™„ë£Œ: {n_rows:,} rows, {n_segments} segments")
    return df_segments


def _calculate_cycles(df_segments):
    """ì„¸ê·¸ë¨¼íŠ¸ë³„ë¡œ ì‚¬ì´í´ ê³„ì‚° (Python algorithm ì‚¬ìš©).
    
    ì›ë³¸ run.pyì™€ ë™ì¼í•œ ë¡œì§:
    - ì„¸ê·¸ë¨¼íŠ¸ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
    - filtered_numì„ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
    
    Returns:
        DataFrame with cycles or None if no cycles could be derived
    """
    import pandas as pd
    from algorithm import cycle_features

    n_segments = df_segments['prod_set_id'].nunique()
    logging.info(f"ğŸ”„ ì‚¬ì´í´ ê³„ì‚° ì¤‘: {n_segments} segments")
    dfs: list[pd.DataFrame] = []
    
    # ì›ë³¸ run.pyì™€ ë™ì¼: ì„¸ê·¸ë¨¼íŠ¸ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬ (prod_set_id ìˆœì„œ ìœ ì§€)
    for i, (prod_set_id, segment) in enumerate(df_segments.groupby('prod_set_id', sort=False)):
        segment_df = segment[[
            'collection_timestamp', 'motor_current', 'chamber_temperature', 
            'mixer_run', 'run_mode', 'process_stage', 'drop_door_position'
        ]].copy()
        
        # ì§„ë‹¨ ì •ë³´ ìˆ˜ì§‘
        diagnostic_info = _diagnose_segment(segment_df, prod_set_id)
        
        df_cycle = cycle_features.compare_peak(segment_df)
        if df_cycle.empty:
            logging.warning(
                f"âš ï¸ Segment {prod_set_id} (index {i}) produced no cycles. "
                f"Diagnostics: {diagnostic_info}"
            )
            continue
        df_cycle['prod_set_id'] = prod_set_id
        df_cycle['filtered_num'] = i  # ì›ë³¸ê³¼ ë™ì¼: ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤
        dfs.append(df_cycle)
    
    if not dfs:
        # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ì§„ë‹¨ ì •ë³´ ìˆ˜ì§‘
        all_diagnostics = []
        for i, (prod_set_id, segment) in enumerate(df_segments.groupby('prod_set_id', sort=False)):
            segment_df = segment[[
                'collection_timestamp', 'motor_current', 'chamber_temperature', 
                'mixer_run', 'run_mode', 'process_stage', 'drop_door_position'
            ]].copy()
            diag = _diagnose_segment(segment_df, prod_set_id)
            all_diagnostics.append(f"Segment {prod_set_id}: {diag}")
        
        warning_msg = (
            "No cycles could be derived from filtered segments.\n"
            f"Total segments processed: {n_segments}\n"
            "Diagnostics:\n" + "\n".join(all_diagnostics)
        )
        logging.warning(f"âš ï¸ {warning_msg}")
        return None
    
    # ì›ë³¸ê³¼ ë™ì¼: ì •ë ¬í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ê²°í•©
    df_result = pd.concat(dfs, ignore_index=True).reset_index(drop=True)
    logging.info(f"âœ… ì‚¬ì´í´ ê³„ì‚° ì™„ë£Œ: {len(df_result)} cycles")
    return df_result


def _diagnose_segment(segment_df, prod_set_id: int) -> str:
    """ì„¸ê·¸ë¨¼íŠ¸ ì§„ë‹¨ ì •ë³´ ìˆ˜ì§‘ (ë””ë²„ê¹…ìš©)."""
    try:
        import numpy as np
        import pandas as pd
        # ê¸°ë³¸ ì •ë³´
        n_rows = len(segment_df)
        time_col = 'collection_timestamp'
        motor_col = 'motor_current'
        mixer_col = 'mixer_run'
        door_col = 'drop_door_position'
        stage_col = 'process_stage'
        
        # ì‹œê°„ ë²”ìœ„
        times = pd.to_datetime(segment_df[time_col], errors='coerce')
        time_span = (times.max() - times.min()).total_seconds() if len(times.dropna()) > 1 else 0
        
        # ê²½ê³„ íƒì§€ ì‹œë®¬ë ˆì´ì…˜
        door = segment_df[door_col].where(segment_df[door_col].isin(["close", "open"]))
        door_ffill = door.ffill()
        open_after_close = (door_ffill == "open") & (door_ffill.shift() == "close")
        cycle_marks_count = open_after_close.sum()
        
        mixer = segment_df[mixer_col].where(segment_df[mixer_col].isin(["RUN", "STOP"]))
        mixer_prev = mixer.ffill().shift()
        run_starts_count = ((mixer == "RUN") & (mixer_prev.isin(["STOP", np.nan]))).sum()
        run_stops_count = ((mixer == "STOP") & (mixer_prev == "RUN")).sum()
        
        # mix ë‹¨ê³„ í™•ì¸
        stage = segment_df[stage_col].where(segment_df[stage_col].isin(["load", "mix"])).ffill()
        has_mix = (stage == "mix").any()
        mix_count = (stage == "mix").sum()
        
        # ëª¨í„° ì „ë¥˜ í™•ì¸
        motor_values = pd.to_numeric(segment_df[motor_col], errors='coerce')
        motor_valid = motor_values.notna().sum()
        motor_mean = motor_values.mean() if motor_valid > 0 else None
        
        return (
            f"rows={n_rows}, time_span={time_span:.1f}s, "
            f"cycle_marks={cycle_marks_count}, run_starts={run_starts_count}, run_stops={run_stops_count}, "
            f"has_mix={has_mix}, mix_rows={mix_count}, motor_valid={motor_valid}, motor_mean={motor_mean}"
        )
    except Exception as e:
        return f"diagnostic_error: {str(e)}"


def _build_plc_segments(df_segments, df_result):
    """PLC ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± (Python algorithm ì‚¬ìš©)."""
    from algorithm import segments

    n_cycles = len(df_result)
    logging.info(f"ğŸ”„ PLC ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì¤‘: {n_cycles} cycles")
    df_filtered = df_segments[[
        'collection_timestamp', 'motor_current', 'chamber_temperature', 
        'mixer_run', 'run_mode', 'process_stage', 'drop_door_position'
    ]].copy()
    
    df_plc_seg = segments.build_plc_segments(
        df_filtered,
        df_result,
        current_col="motor_current",
        temp_col="chamber_temperature",
        cycle_col=CYCLE_ID_COL
    )
    n_rows = len(df_plc_seg)
    n_cycles_seg = df_plc_seg[CYCLE_ID_COL].nunique()
    logging.info(f"âœ… PLC ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì™„ë£Œ: {n_rows:,} rows, {n_cycles_seg} cycles")
    return df_plc_seg


def _build_cycle_matrix(df_plc_seg):
    """ì‚¬ì´í´ í–‰ë ¬ ìƒì„± (ì›ë³¸ê³¼ ë™ì¼)."""
    from algorithm import segments

    n_cycles = df_plc_seg[CYCLE_ID_COL].nunique()
    logging.info(f"ğŸ”„ ì‚¬ì´í´ í–‰ë ¬ ìƒì„± ì¤‘: {n_cycles} cycles")
    X = segments.build_cycle_matrix(
        df_plc_seg,
        idx_col=CYCLE_ID_COL,
        current_col="motor_current",
        temp_col="chamber_temperature",
        n_points=SEQUENCE_LENGTH
    )
    
    if len(X) == 0:
        raise ValueError("No valid cycles found for CNN inference.")
    
    n_valid = len(X)
    if n_valid < n_cycles:
        invalid_count = n_cycles - n_valid
        logging.warning(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì‚¬ì´í´: {invalid_count}ê°œ ì œì™¸ë¨")
    
    logging.info(f"âœ… ì‚¬ì´í´ í–‰ë ¬ ìƒì„± ì™„ë£Œ: {n_valid} cycles, shape={X.shape}")
    return X


def _run_cnn_prediction(X):
    """CNN ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰."""
    import numpy as np
    from tensorflow import keras
    from algorithm import inference

    n_cycles = len(X)
    logging.info(f"ğŸ¤– CNN ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ ì¤‘: {n_cycles} cycles")
    
    cnn_model = keras.models.load_model(MODEL_PATH)
    prob = inference.predict_cycles_cnn(cnn_model, X)
    
    # ëª¨ë¸ ì¶œë ¥ ê²€ì¦: NaNì´ë‚˜ ìŒìˆ˜ ê°’ í™•ì¸
    if np.isnan(prob).any():
        nan_count = np.isnan(prob).sum()
        logging.error(f"âŒ ëª¨ë¸ ì¶œë ¥ì— NaN ê°’ ë°œê²¬: {nan_count}ê°œ")
        prob = np.nan_to_num(prob, nan=0.5)
    
    if (prob < 0).any():
        neg_count = (prob < 0).sum()
        logging.warning(f"âš ï¸ ëª¨ë¸ ì¶œë ¥ì— ìŒìˆ˜ ê°’ ë°œê²¬: {neg_count}ê°œ (0ìœ¼ë¡œ í´ë¦¬í•‘)")
        prob = np.clip(prob, 0, 1)
    
    # ì›ë³¸ ë¡œì§ê³¼ ë™ì¼: ëª¨ë¸ ì¶œë ¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìµœì†Œê°’ ì„¤ì • ì—†ìŒ)
    # 0.0 ê°’ì€ ëª¨ë¸ì˜ ì‹¤ì œ ì¶œë ¥ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€
    
    n_anomalies = (prob < ANOMALY_THRESHOLD).sum()
    prob_min, prob_max, prob_mean = prob.min(), prob.max(), prob.mean()
    zero_final = np.sum(prob == 0.0)
    logging.info(f"âœ… CNN ì¶”ë¡  ì™„ë£Œ: {n_cycles} cycles, ì´ìƒ={n_anomalies}ê°œ (prob: min={prob_min:.6f}, max={prob_max:.4f}, mean={prob_mean:.4f}, 0.0={zero_final}ê°œ)")
    
    return prob


def _calculate_shift(start_time):
    """cycle_start ì‹œê°„ê³¼ ìš”ì¼ ê¸°ì¤€ìœ¼ë¡œ shift ê³„ì‚°.

    ì›”~ëª©:
        1: 06:30 ~ 14:30
        2: 14:30 ~ 22:30
        3: 22:30 ~ (ë‹¤ìŒë‚ )06:30
    ê¸ˆ:
        1: 06:30 ~ 15:00
        2: 15:00 ~ 22:30
        3: 22:30 ~ (ë‹¤ìŒë‚ )06:30
    í† :
        1: 06:30 ~ 11:30
        2: 11:30 ~ 16:30
        3: 16:30 ~ 21:30

    ì£¼ì˜: 06:30 ì´ì „ì€ ì „ë‚  22:30ë¶€í„° ì‹œì‘í•œ shift 3ì— ì†í•¨
    
    Args:
        start_time: ì‚¬ì´í´ ì‹œì‘ ì‹œê°„ (pd.Timestamp)
    
    Returns:
        shift ë²ˆí˜¸ (1, 2, 3) ë˜ëŠ” None
    """
    import pandas as pd

    if pd.isna(start_time):
        return None
    
    # tz-aware â†’ naive
    if pd.api.types.is_datetime64tz_dtype(type(start_time)):
        if start_time.tz is not None:
            start_time = start_time.tz_convert(None)
        else:
            start_time = start_time.tz_localize(None)
    
    weekday = start_time.weekday()  # 0=ì›”, 4=ê¸ˆ, 5=í† , 6=ì¼
    time_only = start_time.time()
    
    # ì‹œê°„ ë³€ìˆ˜ ë¯¸ë¦¬ ì¶”ì¶œ (ë°˜ë³µ í˜¸ì¶œ ë°©ì§€)
    t0630 = datetime.strptime("06:30", "%H:%M").time()
    t1430 = datetime.strptime("14:30", "%H:%M").time()
    t2230 = datetime.strptime("22:30", "%H:%M").time()
    t1500 = datetime.strptime("15:00", "%H:%M").time()
    t1130 = datetime.strptime("11:30", "%H:%M").time()
    t1630 = datetime.strptime("16:30", "%H:%M").time()
    t2130 = datetime.strptime("21:30", "%H:%M").time()
    
    # 06:30 ì´ì „ â†’ ì „ë‚  shift 3
    if time_only < t0630:
        prev_weekday = (weekday - 1) % 7
        if prev_weekday <= 4:  # ì „ë‚ ì´ ì›”~ê¸ˆ
            return 3
        return None
    
    # ì›”~ëª©
    if weekday <= 3:
        if time_only < t1430:
            return 1
        elif time_only < t2230:
            return 2
        else:
            return 3
    
    # ê¸ˆìš”ì¼
    if weekday == 4:
        if time_only < t1500:
            return 1
        elif time_only < t2230:
            return 2
        else:
            return 3
    
    # í† ìš”ì¼
    if weekday == 5:
        if time_only < t1130:
            return 1
        elif time_only < t1630:
            return 2
        elif time_only < t2130:
            return 3
        else:
            return None
    
    # ì¼ìš”ì¼
    return None


def _prepare_final_result(df_result, prob):
    """ìµœì¢… ê²°ê³¼ DataFrame ì¤€ë¹„.
    
    í™”ë©´ í‘œì‹œ ìˆœì„œ: no, shift, cycle_start, cycle_end, mode, mix_duration_sec, max_temp, is_3_stage, is_anomaly, anomaly_prob
    """
    # ê²°ê³¼ ê²°í•©
    import pandas as pd
    from algorithm import inference

    df_pred = pd.DataFrame({
        "anomaly_prob": prob,
        "is_anomaly": [inference.convert_prob_to_result(p, threshold=ANOMALY_THRESHOLD) for p in prob]
    })
    df_final = pd.concat([df_result.reset_index(drop=True), df_pred], axis=1)
    
    # filtered_numì€ ì´ë¯¸ _calculate_cyclesì—ì„œ ì„¤ì •ë¨ (ì›ë³¸ê³¼ ë™ì¼)
    
    # result ì¶”ê°€: is_anomalyì™€ is_3_stage ë‘˜ ë‹¤ trueì—¬ì•¼ë§Œ True, í•˜ë‚˜ë¼ë„ falseë©´ False
    df_final['result'] = df_final.apply(
        lambda row: bool(row['is_anomaly'] and row['is_3_stage']),
        axis=1
    )
    
    # shift ê³„ì‚°: cycle_start ì‹œê°„ê³¼ ìš”ì¼ ê¸°ì¤€
    df_final['shift'] = df_final['start'].apply(_calculate_shift)
    
    # no ìƒì„±: banbury03_yyyyMMdd_{ìˆœì°¨ìˆœì„œ} í˜•ì‹
    # ì›ë³¸ ìˆœì„œ ìœ ì§€: ì •ë ¬í•˜ì§€ ì•Šê³  ì›ë³¸ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬ (ì›ë³¸ run.pyì™€ ë™ì¼)
    df_final['date_str'] = df_final['start'].dt.strftime('%Y%m%d')
    df_final['seq'] = df_final.groupby('date_str').cumcount() + 1
    df_final['no'] = 'banbury03_' + df_final['date_str'] + '_' + df_final['seq'].astype(str)
    df_final = df_final.drop(columns=['date_str', 'seq'])
    
    # ì»¬ëŸ¼ëª… ë³€ê²½: í™”ë©´ í‘œì‹œ ìˆœì„œì— ë§ì¶¤
    df_final = df_final.rename(columns={
        'start': 'cycle_start',
        'end': 'cycle_end',
        'run_mode_start': 'mode'
    })
    
    # ë°ì´í„° ì •ì œ: NaNë§Œ ì²˜ë¦¬ (ì›ë³¸ ë¡œì§ê³¼ ë™ì¼, ìµœì†Œê°’ ì„¤ì • ì—†ìŒ)
    if df_final['anomaly_prob'].isna().any():
        logging.warning(f"âš ï¸ anomaly_probì— NaN ê°’ ë°œê²¬: {df_final['anomaly_prob'].isna().sum()}ê°œ")
        df_final['anomaly_prob'] = df_final['anomaly_prob'].fillna(0.5)
    
    if df_final['is_anomaly'].isna().any():
        df_final['is_anomaly'] = df_final['is_anomaly'].fillna(False)
    
    # í™”ë©´ í‘œì‹œ ìˆœì„œë¡œ ì»¬ëŸ¼ ì •ë ¬
    display_columns = [
        'no', 'shift', 'cycle_start', 'cycle_end', 'mode',
        'mix_duration_sec', 'max_temp', 'is_3_stage', 'is_anomaly', 'anomaly_prob'
    ]
    # ë‚´ë¶€ ì‚¬ìš© ì»¬ëŸ¼ë„ í¬í•¨
    internal_columns = ['filtered_num', 'peak_count', 'result']
    all_columns = display_columns + [col for col in internal_columns if col in df_final.columns]
    
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    available_columns = [col for col in all_columns if col in df_final.columns]
    df_final = df_final[available_columns]
    
    logging.info(f"âœ… CNN ì¶”ë¡  ì™„ë£Œ: {len(df_final)} cycles, ì´ìƒ: {df_final['is_anomaly'].sum()}ê°œ")
    return df_final


def _save_anomaly_results(pg: PostgresHelper, df_final) -> None:
    """ì´ìƒ ê°ì§€ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥."""
    n_rows = len(df_final)
    n_anomalies = df_final['is_anomaly'].sum()
    logging.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {n_rows} cycles (ì´ìƒ={n_anomalies}ê°œ)")
    
    insert_data, columns = _prepare_insert_data(df_final)
    
    pg.insert_data(
        schema_name="gold",
        table_name="banbury_anomaly_result",
        data=insert_data,
        columns=columns,
        conflict_columns=["no"],
        chunk_size=500
    )
    
    logging.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(insert_data):,} rows")


def _save_cycles_to_db(pg: PostgresHelper, df_result, start_date: str, end_date: str) -> None:
    """compare_peak ê²°ê³¼ë¥¼ banbury_cycles í…Œì´ë¸”ì— ì €ì¥.
    
    Python algorithm ê²°ê³¼ êµ¬ì¡°ì— ë§ê²Œ ì €ì¥:
    - ì»¬ëŸ¼: prod_set_id, cycle_id, start, end, run_mode_start, mix_duration_sec, max_temp, peak_count, is_3_stage
    
    Args:
        pg: PostgresHelper instance
        df_result: compare_peak ê²°ê³¼ DataFrame
        start_date: ì‹œì‘ ë‚ ì§œ
        end_date: ì¢…ë£Œ ë‚ ì§œ
    """
    from psycopg2.extras import execute_values
    import pandas as pd
    
    # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„± (Python algorithm ê²°ê³¼ êµ¬ì¡°ì— ë§ê²Œ)
    create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {SCHEMA}.{TABLE_BANBURY_CYCLES} (
            prod_set_id INTEGER NOT NULL,
            cycle_id INTEGER NOT NULL,
            start TIMESTAMPTZ NOT NULL,
            "end" TIMESTAMPTZ NOT NULL,
            run_mode_start VARCHAR(10),
            mix_duration_sec NUMERIC(10, 1) NOT NULL,
            max_temp NUMERIC(10, 2),
            peak_count INTEGER NOT NULL,
            is_3_stage BOOLEAN NOT NULL,
            CONSTRAINT pk_banbury_cycles PRIMARY KEY (prod_set_id, cycle_id)
        )
    """
    
    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (í•´ë‹¹ ë‚ ì§œ ë²”ìœ„)
    delete_sql = f"""
        DELETE FROM {SCHEMA}.{TABLE_BANBURY_CYCLES}
        WHERE start >= '{start_date}'::timestamp
          AND start < '{end_date}'::timestamp
    """
    
    # INSERT ë°ì´í„° ì¤€ë¹„
    # NaT (Not a Time) ê°’ ì²˜ë¦¬
    insert_data = []
    cycle_id_counter = {}  # prod_set_idë³„ cycle_id ì¹´ìš´í„°
    
    for idx, row in df_result.iterrows():
        prod_set_id = int(row['prod_set_id'])
        
        # start, end timestamp ì²˜ë¦¬: NaT ì²´í¬
        start_ts = pd.to_datetime(row['start'], errors='coerce')
        end_ts = pd.to_datetime(row['end'], errors='coerce')
        if pd.isna(start_ts) or pd.isna(end_ts):
            continue  # NaTì¸ í–‰ì€ ê±´ë„ˆë›°ê¸°
        
        # ê° prod_set_idë³„ë¡œ cycle_idë¥¼ 1ë¶€í„° ì‹œì‘
        if prod_set_id not in cycle_id_counter:
            cycle_id_counter[prod_set_id] = 0
        cycle_id_counter[prod_set_id] += 1
        cycle_id = cycle_id_counter[prod_set_id]
        
        insert_data.append((
            prod_set_id,
            cycle_id,
            start_ts.to_pydatetime(),
            end_ts.to_pydatetime(),
            row.get('run_mode_start') if pd.notna(row.get('run_mode_start')) else None,
            float(row['mix_duration_sec']),
            float(row['max_temp']) if pd.notna(row['max_temp']) else None,
            int(row['peak_count']),
            bool(row['is_3_stage'])
        ))
    
    insert_sql = f"""
        INSERT INTO {SCHEMA}.{TABLE_BANBURY_CYCLES}
        (prod_set_id, cycle_id, start, "end", run_mode_start, mix_duration_sec, max_temp, peak_count, is_3_stage)
        VALUES %s
    """
    
    try:
        with pg.hook.get_conn() as conn, conn.cursor() as cursor:
            cursor.execute(create_table_sql)
            conn.commit()
            cursor.execute(delete_sql)
            conn.commit()
            if insert_data:
                execute_values(cursor, insert_sql, insert_data)
                conn.commit()
            logging.info(f"âœ… banbury_cycles ì €ì¥ ì™„ë£Œ: {len(insert_data):,} rows")
    except Exception as e:
        logging.error(f"âŒ banbury_cycles ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        raise


def _prepare_insert_data(df_final) -> Tuple[list, list]:
    """Prepare data for database insertion.
    
    í™”ë©´ í‘œì‹œ ìˆœì„œ: no, shift, cycle_start, cycle_end, mode, mix_duration_sec, max_temp, is_3_stage, is_anomaly, anomaly_prob
    
    Args:
        df_final: Final DataFrame with predictions
    
    Returns:
        Tuple of (insert_data, columns)
    """
    insert_data = []
    
    import pandas as pd

    for _, row in df_final.iterrows():
        insert_data.append((
            str(row['no']),  # banbury03_yyyyMMdd_{ìˆœì°¨ìˆœì„œ}
            int(row['shift']) if pd.notna(row['shift']) else None,  # shift (1, 2, 3)
            pd.to_datetime(row['cycle_start']).to_pydatetime(),  # cycle_start
            pd.to_datetime(row['cycle_end']).to_pydatetime(),  # cycle_end
            row.get('mode'),  # mode (run_mode_start)
            round(float(row['mix_duration_sec']), 1),  # ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼
            float(row['max_temp']) if pd.notna(row['max_temp']) else None,
            bool(row['is_3_stage']),
            bool(row['is_anomaly']),
            float(row['anomaly_prob']),
            # ë‚´ë¶€ ì‚¬ìš© ì»¬ëŸ¼
            int(row['filtered_num']),
            int(row.get('peak_count', 0)),
            bool(row.get('result', False))
        ))
    
    columns = [
        "no", "shift", "cycle_start", "cycle_end", "mode",
        "mix_duration_sec", "max_temp", "is_3_stage", "is_anomaly", "anomaly_prob",
        "filtered_num", "peak_count", "result"
    ]
    
    return insert_data, columns


# ============================================================================
# DAG ì •ì˜
# ============================================================================

# Incremental DAG
with DAG(
    dag_id="dbt_banbury_anomaly_detection_incremental",
    default_args=DEFAULT_ARGS,
    description="Banbury ê³µì • ì´ìƒ ê°ì§€ - ì¦ë¶„ ì²˜ë¦¬",
    schedule_interval="30 7 * * *",  # ë§¤ì¼ 07:30 (06:30 ë°ì´í„° ì²˜ë¦¬)
    catchup=False,
    tags=["dbt", "banbury", "anomaly", "incremental"],
) as incremental_dag:
    
    get_date_range = PythonOperator(
        task_id="get_date_range",
        python_callable=get_incremental_date_range,
    )
    
    def prepare_dbt_vars(**context):
        """dbt ì‹¤í–‰ì— í•„ìš”í•œ ë³€ìˆ˜ ì¤€ë¹„."""
        date_range = context['ti'].xcom_pull(task_ids='get_date_range')
        if date_range:
            return {
                "start_date": date_range["start_date"],
                "end_date": date_range["end_date"]
            }
        return {}
    
    prepare_vars = PythonOperator(
        task_id="prepare_dbt_vars",
        python_callable=prepare_dbt_vars,
    )
    
    dbt_task = DbtTaskGroup(
        group_id="dbt_banbury_anomaly",
        project_config=ProjectConfig(DBT_PROJECT_DIR),
        profile_config=PROFILE_CONFIG,
        execution_config=EXECUTION_CONFIG,
        operator_args={
            "vars": "{{ ti.xcom_pull(task_ids='prepare_dbt_vars') }}",
            # banbury_anomaly_resultëŠ” ìµœì¢… ê²°ê³¼ í…Œì´ë¸”ì´ë¯€ë¡œ ì œì™¸
            # banbury_cyclesëŠ” Pythonì—ì„œ ì›ë³¸ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ dbt ì‹¤í–‰ ì œì™¸
            # banbury_plc_segmentsë„ Pythonì—ì„œ ìƒì„±í•˜ë¯€ë¡œ dbt ì‹¤í–‰ ì œì™¸
            "exclude": ["banbury_anomaly_result", "banbury_cycles", "banbury_plc_segments", "tag:result"],
        },
    )
    
    cnn_inference = PythonOperator(
        task_id="cnn_inference",
        python_callable=run_cnn_inference,
    )
    
    update_var = PythonOperator(
        task_id="update_variable",
        python_callable=update_variable_after_run,
    )
    
    get_date_range >> prepare_vars >> dbt_task >> cnn_inference >> update_var


# Backfill DAG
with DAG(
    dag_id="dbt_banbury_anomaly_detection_backfill",
    default_args=DEFAULT_ARGS,
    description="Banbury ê³µì • ì´ìƒ ê°ì§€ - ë°±í•„ ì²˜ë¦¬ (2025-01-01ë¶€í„° ì˜¤ëŠ˜-2ì¼ê¹Œì§€)",
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰
    catchup=False,
    tags=["dbt", "banbury", "anomaly", "backfill"],
) as backfill_dag:
    
    get_backfill_range = PythonOperator(
        task_id="get_backfill_date_range",
        python_callable=get_backfill_date_range,
    )
    
    def prepare_backfill_vars(**context):
        """dbt ì‹¤í–‰ì— í•„ìš”í•œ ë³€ìˆ˜ ì¤€ë¹„."""
        date_range = context['ti'].xcom_pull(task_ids='get_backfill_date_range')
        if date_range:
            return {
                "backfill_start_date": date_range["backfill_start_date"],
                "backfill_end_date": date_range["backfill_end_date"]
            }
        return {}
    
    prepare_backfill_vars_task = PythonOperator(
        task_id="prepare_backfill_vars",
        python_callable=prepare_backfill_vars,
    )
    
    dbt_backfill_task = DbtTaskGroup(
        group_id="dbt_banbury_anomaly_backfill",
        project_config=ProjectConfig(DBT_PROJECT_DIR),
        profile_config=PROFILE_CONFIG,
        execution_config=EXECUTION_CONFIG,
        operator_args={
            "vars": "{{ ti.xcom_pull(task_ids='prepare_backfill_vars') }}",
            "full_refresh": True,
            # banbury_anomaly_resultëŠ” ìµœì¢… ê²°ê³¼ í…Œì´ë¸”ì´ë¯€ë¡œ ì œì™¸
            # banbury_cyclesëŠ” Pythonì—ì„œ ì›ë³¸ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ dbt ì‹¤í–‰ ì œì™¸
            # banbury_plc_segmentsë„ Pythonì—ì„œ ìƒì„±í•˜ë¯€ë¡œ dbt ì‹¤í–‰ ì œì™¸
            "exclude": ["banbury_anomaly_result", "banbury_cycles", "banbury_plc_segments", "tag:result"],
        },
    )
    
    cnn_inference_backfill = PythonOperator(
        task_id="cnn_inference_backfill",
        python_callable=run_cnn_inference,
    )
    
    update_backfill_var = PythonOperator(
        task_id="update_backfill_variable",
        python_callable=update_backfill_variable,
    )
    
    get_backfill_range >> prepare_backfill_vars_task >> dbt_backfill_task >> cnn_inference_backfill >> update_backfill_var
