"""
Bulk Insert Test DAG
====================
PostgresHelperì˜ bulk_insert ë©”ì„œë“œ í…ŒìŠ¤íŠ¸ìš© DAG
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import PythonOperator
from plugins.hooks.postgres_hook import PostgresHelper
import logging

logger = logging.getLogger(__name__)

# Default arguments
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=1),
}

# í…ŒìŠ¤íŠ¸ìš© Connection ID (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
TEST_CONN_ID = "postgres_default"
TEST_SCHEMA = "public"
TEST_TABLE = "test_bulk_insert"


def check_bulk_insert_method(**kwargs):
    """bulk_insert ë©”ì„œë“œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
    logger.info("=" * 60)
    logger.info("1ï¸âƒ£ PostgresHelper í´ë˜ìŠ¤ í™•ì¸")
    logger.info("=" * 60)
    
    pg = PostgresHelper(conn_id=TEST_CONN_ID)
    
    # ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
    has_method = hasattr(pg, 'bulk_insert')
    logger.info(f"âœ… bulk_insert ë©”ì„œë“œ ì¡´ì¬ ì—¬ë¶€: {has_method}")
    
    if has_method:
        logger.info(f"âœ… ë©”ì„œë“œ íƒ€ì…: {type(getattr(pg, 'bulk_insert'))}")
        logger.info(f"âœ… ë©”ì„œë“œ docstring: {getattr(pg, 'bulk_insert').__doc__}")
    else:
        logger.error("âŒ bulk_insert ë©”ì„œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        raise AttributeError("bulk_insert method not found")
    
    # ëª¨ë“  ë©”ì„œë“œ ëª©ë¡ í™•ì¸
    methods = [method for method in dir(pg) if not method.startswith('_')]
    logger.info(f"ğŸ“‹ PostgresHelper ë©”ì„œë“œ ëª©ë¡: {', '.join(methods)}")
    
    return {"status": "success", "has_bulk_insert": has_method}


def create_test_table(**kwargs):
    """í…ŒìŠ¤íŠ¸ìš© í…Œì´ë¸” ìƒì„±"""
    logger.info("=" * 60)
    logger.info("2ï¸âƒ£ í…ŒìŠ¤íŠ¸ í…Œì´ë¸” ìƒì„±")
    logger.info("=" * 60)
    
    pg = PostgresHelper(conn_id=TEST_CONN_ID)
    
    create_table_sql = f"""
    CREATE TABLE IF NOT EXISTS {TEST_SCHEMA}.{TEST_TABLE} (
        id INTEGER PRIMARY KEY,
        name VARCHAR(100),
        value NUMERIC(10, 2),
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        status VARCHAR(20)
    );
    """
    
    try:
        with pg.hook.get_conn() as conn, conn.cursor() as cursor:
            # ìŠ¤í‚¤ë§ˆ ìƒì„±
            cursor.execute(f"CREATE SCHEMA IF NOT EXISTS {TEST_SCHEMA}")
            
            # í…Œì´ë¸” ìƒì„±
            cursor.execute(create_table_sql)
            conn.commit()
            logger.info(f"âœ… í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {TEST_SCHEMA}.{TEST_TABLE}")
            
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            cursor.execute(f"TRUNCATE TABLE {TEST_SCHEMA}.{TEST_TABLE}")
            conn.commit()
            logger.info(f"âœ… í…Œì´ë¸” ì´ˆê¸°í™” ì™„ë£Œ")
            
    except Exception as e:
        logger.error(f"âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
        raise
    
    return {"status": "success"}


def test_bulk_insert(**kwargs):
    """bulk_insert ë©”ì„œë“œ í…ŒìŠ¤íŠ¸"""
    logger.info("=" * 60)
    logger.info("3ï¸âƒ£ bulk_insert ë©”ì„œë“œ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)
    
    pg = PostgresHelper(conn_id=TEST_CONN_ID)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (1000ê°œ í–‰)
    test_data = []
    for i in range(1, 1001):
        test_data.append((
            i,  # id
            f"test_name_{i}",  # name
            round(i * 1.5, 2),  # value
            datetime.now(),  # created_at
            "active" if i % 2 == 0 else "inactive"  # status
        ))
    
    logger.info(f"ğŸ“¦ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: {len(test_data):,}ê°œ í–‰")
    logger.info(f"   ìƒ˜í”Œ ë°ì´í„° (ì²« 3ê°œ): {test_data[:3]}")
    
    # ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    columns = ["id", "name", "value", "created_at", "status"]
    
    try:
        # bulk_insert ì‹¤í–‰
        import time
        start_time = time.time()
        
        pg.bulk_insert(
            schema_name=TEST_SCHEMA,
            table_name=TEST_TABLE,
            data=test_data,
            columns=columns,
            chunk_size=1000  # 1000ê°œì”© ì²­í¬ ì²˜ë¦¬
        )
        
        elapsed_time = time.time() - start_time
        logger.info(f"â±ï¸ bulk_insert ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        logger.info(f"ğŸ“Š ì²˜ë¦¬ ì†ë„: {len(test_data) / elapsed_time:.0f} rows/sec")
        
        # ë°ì´í„° í™•ì¸
        with pg.hook.get_conn() as conn, conn.cursor() as cursor:
            cursor.execute(f"SELECT COUNT(*) FROM {TEST_SCHEMA}.{TEST_TABLE}")
            count = cursor.fetchone()[0]
            logger.info(f"âœ… ì‚½ì…ëœ í–‰ ìˆ˜: {count:,}ê°œ")
            
            if count != len(test_data):
                raise ValueError(f"ë°ì´í„° ê°œìˆ˜ ë¶ˆì¼ì¹˜: ì˜ˆìƒ {len(test_data)}, ì‹¤ì œ {count}")
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            cursor.execute(f"SELECT * FROM {TEST_SCHEMA}.{TEST_TABLE} ORDER BY id LIMIT 5")
            sample = cursor.fetchall()
            logger.info(f"ğŸ“‹ ìƒ˜í”Œ ë°ì´í„° (ì²« 5ê°œ): {sample}")
        
        return {
            "status": "success",
            "rows_inserted": count,
            "elapsed_time": elapsed_time
        }
        
    except Exception as e:
        logger.error(f"âŒ bulk_insert í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise


def compare_with_insert_data(**kwargs):
    """insert_dataì™€ bulk_insert ì„±ëŠ¥ ë¹„êµ"""
    logger.info("=" * 60)
    logger.info("4ï¸âƒ£ insert_data vs bulk_insert ì„±ëŠ¥ ë¹„êµ")
    logger.info("=" * 60)
    
    pg = PostgresHelper(conn_id=TEST_CONN_ID)
    
    # ë¹„êµìš© í…Œì´ë¸” ìƒì„±
    compare_table = f"{TEST_TABLE}_compare"
    create_table_sql = f"""
    CREATE TABLE IF NOT EXISTS {TEST_SCHEMA}.{compare_table} (
        id INTEGER PRIMARY KEY,
        name VARCHAR(100),
        value NUMERIC(10, 2),
        created_at TIMESTAMPTZ DEFAULT NOW(),
        status VARCHAR(20)
    );
    """
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (100ê°œ í–‰ìœ¼ë¡œ ë¹„êµ)
    test_data = []
    for i in range(1, 101):
        test_data.append((
            i,
            f"compare_name_{i}",
            round(i * 1.5, 2),
            datetime.now(),
            "active" if i % 2 == 0 else "inactive"
        ))
    
    columns = ["id", "name", "value", "created_at", "status"]
    
    try:
        with pg.hook.get_conn() as conn, conn.cursor() as cursor:
            cursor.execute(f"CREATE SCHEMA IF NOT EXISTS {TEST_SCHEMA}")
            cursor.execute(create_table_sql)
            cursor.execute(f"TRUNCATE TABLE {TEST_SCHEMA}.{compare_table}")
            conn.commit()
        
        # 1. insert_data í…ŒìŠ¤íŠ¸
        import time
        start_time = time.time()
        pg.insert_data(
            schema_name=TEST_SCHEMA,
            table_name=compare_table,
            data=test_data,
            columns=columns,
            chunk_size=100
        )
        insert_data_time = time.time() - start_time
        logger.info(f"â±ï¸ insert_data ì‹¤í–‰ ì‹œê°„: {insert_data_time:.2f}ì´ˆ")
        
        # í…Œì´ë¸” ì´ˆê¸°í™”
        with pg.hook.get_conn() as conn, conn.cursor() as cursor:
            cursor.execute(f"TRUNCATE TABLE {TEST_SCHEMA}.{compare_table}")
            conn.commit()
        
        # 2. bulk_insert í…ŒìŠ¤íŠ¸
        start_time = time.time()
        pg.bulk_insert(
            schema_name=TEST_SCHEMA,
            table_name=compare_table,
            data=test_data,
            columns=columns,
            chunk_size=100
        )
        bulk_insert_time = time.time() - start_time
        logger.info(f"â±ï¸ bulk_insert ì‹¤í–‰ ì‹œê°„: {bulk_insert_time:.2f}ì´ˆ")
        
        # ì„±ëŠ¥ ë¹„êµ
        if bulk_insert_time > 0:
            speedup = insert_data_time / bulk_insert_time
            logger.info(f"ğŸš€ ì„±ëŠ¥ í–¥ìƒ: {speedup:.2f}x ë¹ ë¦„")
        
        return {
            "status": "success",
            "insert_data_time": insert_data_time,
            "bulk_insert_time": bulk_insert_time,
            "speedup": speedup if bulk_insert_time > 0 else None
        }
        
    except Exception as e:
        logger.error(f"âŒ ì„±ëŠ¥ ë¹„êµ ì‹¤íŒ¨: {e}")
        raise


def cleanup_test_table(**kwargs):
    """í…ŒìŠ¤íŠ¸ í…Œì´ë¸” ì •ë¦¬"""
    logger.info("=" * 60)
    logger.info("5ï¸âƒ£ í…ŒìŠ¤íŠ¸ í…Œì´ë¸” ì •ë¦¬")
    logger.info("=" * 60)
    
    pg = PostgresHelper(conn_id=TEST_CONN_ID)
    
    try:
        with pg.hook.get_conn() as conn, conn.cursor() as cursor:
            # í…ŒìŠ¤íŠ¸ í…Œì´ë¸” ì‚­ì œ
            cursor.execute(f"DROP TABLE IF EXISTS {TEST_SCHEMA}.{TEST_TABLE}")
            cursor.execute(f"DROP TABLE IF EXISTS {TEST_SCHEMA}.{TEST_TABLE}_compare")
            conn.commit()
            logger.info(f"âœ… í…ŒìŠ¤íŠ¸ í…Œì´ë¸” ì‚­ì œ ì™„ë£Œ")
            
    except Exception as e:
        logger.warning(f"âš ï¸ í…Œì´ë¸” ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}")
    
    return {"status": "success"}


# DAG ì •ì˜
with DAG(
    dag_id="test_bulk_insert",
    default_args=default_args,
    description="PostgresHelper bulk_insert ë©”ì„œë“œ í…ŒìŠ¤íŠ¸",
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰
    catchup=False,
    tags=["test", "postgres", "bulk_insert"],
    max_active_runs=1,
) as dag:
    
    start = DummyOperator(task_id='start')
    end = DummyOperator(task_id='end')
    
    # Task 1: ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
    check_method = PythonOperator(
        task_id='check_bulk_insert_method',
        python_callable=check_bulk_insert_method,
    )
    
    # Task 2: í…ŒìŠ¤íŠ¸ í…Œì´ë¸” ìƒì„±
    create_table = PythonOperator(
        task_id='create_test_table',
        python_callable=create_test_table,
    )
    
    # Task 3: bulk_insert í…ŒìŠ¤íŠ¸
    test_bulk = PythonOperator(
        task_id='test_bulk_insert',
        python_callable=test_bulk_insert,
    )
    
    # Task 4: ì„±ëŠ¥ ë¹„êµ
    compare_performance = PythonOperator(
        task_id='compare_with_insert_data',
        python_callable=compare_with_insert_data,
    )
    
    # Task 5: ì •ë¦¬ (ì„ íƒì )
    cleanup = PythonOperator(
        task_id='cleanup_test_table',
        python_callable=cleanup_test_table,
        trigger_rule='all_done',  # ì„±ê³µ/ì‹¤íŒ¨ ê´€ê³„ì—†ì´ ì‹¤í–‰
    )
    
    # DAG ì˜ì¡´ì„±
    start >> check_method >> create_table >> test_bulk >> compare_performance >> cleanup >> end

