import logging
from datetime import datetime, timedelta, timezone
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from plugins.hooks.postgres_hook import PostgresHelper

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ Configuration Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_ARGS = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'sla': timedelta(hours=24)
}

# Database Configuration
SOURCE_POSTGRES_CONN_ID = "pg_jj_banb_hmi_dw"
TARGET_POSTGRES_CONN_ID = "pg_jj_telemetry_dw"
SCHEMA_NAME = "bronze"
TABLE_NAME = "os_banb_hmi_data"

# Migration Configuration
VAR_KEY = "os_banb_hmi_data_migration_last_date"
BATCH_SIZE = 50000  # í•œ ë²ˆì— ì²˜ë¦¬í•  ë°°ì¹˜ í¬ê¸° (ì†ŒìŠ¤ì—ì„œ ê°€ì ¸ì˜¬ ë°°ì¹˜ í¬ê¸°)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ Utility Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _eod(dt: datetime) -> datetime:
    """End of day: í•´ë‹¹ ì¼ì˜ ë§ˆì§€ë§‰ ì‹œê°„ 23:59:59.999999"""
    return dt.replace(hour=23, minute=59, second=59, microsecond=999999)

def get_min_max_rx_date(pg: PostgresHelper) -> tuple:
    """ì†ŒìŠ¤ í…Œì´ë¸”ì—ì„œ rx_dateì˜ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ì„ ê°€ì ¸ì˜´"""
    sql = f"""
        SELECT 
            MIN(rx_date) AS min_date,
            MAX(rx_date) AS max_date,
            COUNT(*) AS total_count
        FROM {SCHEMA_NAME}.{TABLE_NAME}
        WHERE rx_date IS NOT NULL
    """
    
    try:
        with pg.hook.get_conn() as conn, conn.cursor() as cursor:
            cursor.execute(sql)
            result = cursor.fetchone()
            
            if result and result[0] and result[1]:
                min_date = result[0]
                max_date = result[1]
                total_count = result[2] if result[2] else 0
                
                # timezoneì´ ì—†ìœ¼ë©´ UTCë¡œ ê°€ì •
                if min_date.tzinfo is None:
                    min_date = min_date.replace(tzinfo=timezone.utc)
                if max_date.tzinfo is None:
                    max_date = max_date.replace(tzinfo=timezone.utc)
                
                logging.info(f"ğŸ“Š ë°ì´í„° ë²”ìœ„: {min_date} ~ {max_date} (ì´ {total_count:,} rows)")
                return min_date, max_date, total_count
            else:
                raise Exception("ì†ŒìŠ¤ í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"âŒ ìµœì†Œ/ìµœëŒ€ ë‚ ì§œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise

def get_migration_start_date() -> datetime:
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘ ë‚ ì§œë¥¼ ê°€ì ¸ì˜´ (Variable ë˜ëŠ” None)"""
    cursor_str = Variable.get(VAR_KEY, default_var=None)
    if cursor_str:
        try:
            cursor_dt = datetime.fromisoformat(cursor_str)
            if cursor_dt.tzinfo is None:
                cursor_dt = cursor_dt.replace(tzinfo=timezone.utc)
            logging.info(f"ğŸ“Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¬ê°œ ì§€ì : {cursor_dt}")
            return cursor_dt + timedelta(seconds=1)
        except Exception as e:
            logging.warning(f"âš ï¸ Variable íŒŒì‹± ì‹¤íŒ¨: {str(e)}. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
            return None
    return None

def set_migration_date(dt: datetime) -> None:
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ì§„í–‰ ë‚ ì§œë¥¼ Variableì— ì €ì¥"""
    Variable.set(VAR_KEY, dt.isoformat())
    logging.info(f"ğŸ“Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì§„í–‰ ë‚ ì§œ ì €ì¥: {dt}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ Data Extraction & Loading
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_extract_sql(start_date: datetime, end_date: datetime) -> str:
    """ì†ŒìŠ¤ í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” SQL ì¿¼ë¦¬"""
    # TIMESTAMPTZ íƒ€ì…ì„ ìœ„í•´ íƒ€ì„ì¡´ ì •ë³´ í¬í•¨
    # PostgreSQLì—ì„œ TIMESTAMPTZëŠ” íƒ€ì„ì¡´ì„ í¬í•¨í•˜ë¯€ë¡œ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    start_str = start_date.isoformat()
    end_str = end_date.isoformat()
    
    return f"""
        SELECT 
            factory,
            equipment,
            seq_no,
            pid,
            rx_date,
            p_value,
            rxdate_year,
            rxdate_month,
            rxdate_day,
            etl_extract_time,
            etl_ingest_time
        FROM {SCHEMA_NAME}.{TABLE_NAME}
        WHERE rx_date >= '{start_str}'::timestamptz
          AND rx_date <= '{end_str}'::timestamptz
          AND rx_date IS NOT NULL
        ORDER BY rx_date, factory, equipment, seq_no
    """

def extract_and_load_daily_batch(
    source_pg: PostgresHelper,
    target_pg: PostgresHelper,
    start_date: datetime,
    end_date: datetime
) -> int:
    """1ì¼ ë‹¨ìœ„ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  íƒ€ê²Ÿì— ë¡œë“œ"""
    logging.info(f"ğŸ”„ ì¼ ë‹¨ìœ„ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {start_date} ~ {end_date}")
    
    sql = build_extract_sql(start_date, end_date)
    
    columns = [
        "factory", "equipment", "seq_no", "pid", "rx_date", "p_value",
        "rxdate_year", "rxdate_month", "rxdate_day", "etl_extract_time", "etl_ingest_time"
    ]
    conflict_columns = ["factory", "equipment", "seq_no", "rx_date"]
    
    total_rows = 0
    batch_count = 0
    
    try:
        # ì†ŒìŠ¤ì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì¶”ì¶œ
        with source_pg.hook.get_conn() as source_conn:
            with source_conn.cursor() as source_cursor:
                source_cursor.execute(sql)
                
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                logging.info(f"ğŸ“¥ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ ì‹œì‘...")
                while True:
                    batch_data = source_cursor.fetchmany(BATCH_SIZE)
                    
                    if not batch_data:
                        break
                    
                    batch_count += 1
                    batch_rows = len(batch_data)
                    
                    # íƒ€ê²Ÿì— ë°°ì¹˜ ì‚½ì…
                    try:
                        logging.info(f"ğŸ“¤ ë°°ì¹˜ {batch_count} ì‚½ì… ì‹œì‘: {batch_rows:,} rows")
                        target_pg.insert_data(
                            SCHEMA_NAME,
                            TABLE_NAME,
                            batch_data,
                            columns,
                            conflict_columns,
                            chunk_size=1000
                        )
                        total_rows += batch_rows
                        
                        # ë§¤ ë°°ì¹˜ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥ (ì§„í–‰ ìƒí™© ì¶”ì )
                        logging.info(f"âœ… ë°°ì¹˜ {batch_count} ì‚½ì… ì™„ë£Œ: {batch_rows:,} rows (ëˆ„ì : {total_rows:,})")
                        
                        # ë©”ëª¨ë¦¬ ì •ë¦¬ íŒíŠ¸
                        del batch_data
                        
                    except Exception as e:
                        logging.error(f"âŒ ë°°ì¹˜ {batch_count} ì‚½ì… ì‹¤íŒ¨: {str(e)}")
                        raise
        
        logging.info(f"âœ… ì¼ ë‹¨ìœ„ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {start_date} ~ {end_date} (ì´ {batch_count} ë°°ì¹˜, {total_rows:,} rows)")
        return total_rows
        
    except Exception as e:
        logging.error(f"âŒ ì¼ ë‹¨ìœ„ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {start_date} ~ {end_date} - {str(e)}")
        raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4ï¸âƒ£ Main Migration Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def migrate_data(**context) -> dict:
    """ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë©”ì¸ í•¨ìˆ˜"""
    source_pg = PostgresHelper(conn_id=SOURCE_POSTGRES_CONN_ID)
    target_pg = PostgresHelper(conn_id=TARGET_POSTGRES_CONN_ID)
    
    try:
        # ì†ŒìŠ¤ í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        if not source_pg.check_table(SCHEMA_NAME, TABLE_NAME):
            raise Exception(f"ì†ŒìŠ¤ í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SCHEMA_NAME}.{TABLE_NAME}")
        
        # íƒ€ê²Ÿ í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        if not target_pg.check_table(SCHEMA_NAME, TABLE_NAME):
            raise Exception(f"íƒ€ê²Ÿ í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SCHEMA_NAME}.{TABLE_NAME}")
        
        # ë°ì´í„° ë²”ìœ„ í™•ì¸
        min_date, max_date, total_count = get_min_max_rx_date(source_pg)
        logging.info(f"ğŸ“Š ì´ ë°ì´í„° ê°œìˆ˜: {total_count:,} rows")
        
        # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘ ë‚ ì§œ ê²°ì •
        migration_start = get_migration_start_date()
        if migration_start is None:
            migration_start = min_date.replace(hour=0, minute=0, second=0, microsecond=0)
        else:
            # timezone í†µì¼
            if migration_start.tzinfo != min_date.tzinfo:
                migration_start = migration_start.astimezone(min_date.tzinfo)
        
        # ë§ˆì´ê·¸ë ˆì´ì…˜ ì¢…ë£Œ ë‚ ì§œ
        migration_end = max_date
        
        # ì˜ˆìƒ ì¼ìˆ˜ ê³„ì‚°
        estimated_days = (migration_end.date() - migration_start.date()).days + 1
        logging.info(f"ğŸš€ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘: {migration_start} ~ {migration_end} (ì˜ˆìƒ {estimated_days}ì¼)")
        
        # ì¼ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        current_start = migration_start
        total_migrated = 0
        day_count = 0
        
        while current_start <= migration_end:
            # í˜„ì¬ ì¼ì˜ ë ë‚ ì§œ ê³„ì‚°
            current_end = _eod(current_start)
            if current_end > migration_end:
                current_end = migration_end
            
            day_count += 1
            day_start_str = current_start.strftime("%Y-%m-%d")
            day_end_str = current_end.strftime("%Y-%m-%d")
            logging.info(f"ğŸ“… ì¼ {day_count} ì²˜ë¦¬ ì‹œì‘: {day_start_str} ~ {day_end_str}")
            
            try:
                # ì¼ ë‹¨ìœ„ ë°ì´í„° ì¶”ì¶œ ë° ë¡œë“œ
                day_rows = extract_and_load_daily_batch(
                    source_pg,
                    target_pg,
                    current_start,
                    current_end
                )
                total_migrated += day_rows
                
                # ì§„í–‰ ìƒí™© ì €ì¥ (ì„±ê³µí•œ ê²½ìš°ì—ë§Œ)
                set_migration_date(current_end)
                
                progress_pct = (total_migrated / total_count * 100) if total_count > 0 else 0
                logging.info(f"âœ… ì¼ {day_count} ì™„ë£Œ: {day_rows:,} rows (ëˆ„ì : {total_migrated:,} rows, ì§„í–‰ë¥ : {progress_pct:.2f}%)")
                
            except Exception as e:
                logging.error(f"âŒ ì¼ {day_count} ì²˜ë¦¬ ì‹¤íŒ¨ ({day_start_str} ~ {day_end_str}): {str(e)}")
                # ì‹¤íŒ¨í•œ ì¼ì€ ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ë‹¤ì‹œ ì‹œë„í•  ìˆ˜ ìˆë„ë¡ Variable ì—…ë°ì´íŠ¸ ì•ˆ í•¨
                raise
            
            # ë‹¤ìŒ ì¼ ì‹œì‘ (ë‹¤ìŒ ë‚  ìì •)
            next_day = (current_start + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
            current_start = next_day
            if current_start > migration_end:
                break
        
        # ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ ì‹œ Variable ì‚­ì œ
        if total_migrated > 0:
            try:
                Variable.delete(VAR_KEY)
                logging.info(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ. Variable ì‚­ì œ: {VAR_KEY}")
            except Exception:
                pass
        
        logging.info(f"ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: ì´ {total_migrated:,} rows ì²˜ë¦¬ ({day_count}ì¼)")
        return {
            "status": "success",
            "total_migrated": total_migrated,
            "day_count": day_count,
            "start_date": migration_start.isoformat(),
            "end_date": migration_end.isoformat()
        }
        
    except Exception as e:
        logging.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "failed",
            "error": str(e)
        }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5ï¸âƒ£ DAG Definition
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with DAG(
    dag_id="os_banb_hmi_data_migration",
    default_args=DEFAULT_ARGS,
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["JJ", "OS", "Banbury", "HMI", "data", "migration", "bronze layer"],
) as dag:
    
    migrate_task = PythonOperator(
        task_id="migrate_os_banb_hmi_data",
        python_callable=migrate_data,
        provide_context=True,
    )

