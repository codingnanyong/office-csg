"""
CTM Mold Temperature Raw Common Functions
=========================================
ê³µí†µ í•¨ìˆ˜ ë° ì„¤ì •ì„ ëª¨ì•„ë‘” ëª¨ë“ˆ
"""

import logging
from datetime import datetime, timedelta, timezone
from airflow.exceptions import AirflowSkipException
from airflow.models import Variable
from plugins.hooks.mssql_hook import MSSQLHelper
from plugins.hooks.postgres_hook import PostgresHelper

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Database Configuration
INCREMENT_KEY = "last_extract_time_ctm_mold_temperature_raw"
TABLE_NAME = "ctm_mold_temperature_raw"
SCHEMA_NAME = "bronze"

# Connection IDs
SOURCE_POSTGRES_CONN_ID = "ms_ctm_edge"
TARGET_POSTGRES_CONN_ID = "pg_jj_telemetry_dw"

# Date Configuration
INDO_TZ = timezone(timedelta(hours=7))
INITIAL_START_DATE = datetime(2020, 1, 1, 0, 0, 0)
DAYS_OFFSET_FOR_INCREMENTAL = 2


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utility Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_datetime(dt_str: str) -> datetime:
    """Parse datetime string with microsecond support"""
    try:
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S.%f")
    except ValueError:
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S")


def get_month_end_date(start_date: datetime) -> datetime:
    """Get the last day of the month for a given date"""
    next_month = start_date.replace(day=1) + timedelta(days=32)
    month_end = next_month.replace(day=1) - timedelta(days=1)
    # 23:59:59ë¡œ ì„¤ì •
    return month_end.replace(hour=23, minute=59, second=59, microsecond=999999)


def calculate_expected_monthly_loops(start_date: datetime, end_date: datetime) -> int:
    """Calculate expected number of monthly loops"""
    current_date = start_date
    month_count = 0
    
    while current_date < end_date:
        month_end = get_month_end_date(current_date)
        if month_end > end_date:
            month_end = end_date
        current_date = month_end + timedelta(days=1)
        month_count += 1
    
    return month_count


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Extraction
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_extract_sql(start_date: str, end_date: str) -> str:
    """Build SQL query for CTM mold temperature data extraction"""
    # rst_ymdê°€ ë¬¸ìì—´ í˜•íƒœì´ë¯€ë¡œ ì§ì ‘ ë¹„êµ
    return f'''
        SELECT 
            mc_cd,
            hot_1_1, hot_1_2, hot_1_3, hot_1_4, hot_1_5,
            hot_2_1, hot_2_2, hot_2_3, hot_2_4, hot_2_5,
            hot_3_1, hot_3_2, hot_3_3, hot_3_4, hot_3_5,
            hot_4_1, hot_4_2, hot_4_3, hot_4_4, hot_4_5,
            hot_5_1, hot_5_2, hot_5_3, hot_5_4, hot_5_5,
            cool_1, cool_2, cool_3, cool_4, cool_5,
            rst_ymd
        FROM dbo.mold_temperature
        WHERE rst_ymd >= '{start_date}' 
          AND rst_ymd <= '{end_date}'
        ORDER BY rst_ymd
    '''


def extract_data(mssql: MSSQLHelper, start_date: str, end_date: str) -> tuple:
    """Extract data from CTM SQL Server database"""
    sql = build_extract_sql(start_date, end_date)
    logging.info(f"ì‹¤í–‰ ì¿¼ë¦¬: {sql}")
    
    data = mssql.execute_query(sql, task_id="extract_data_task", xcom_key=None)
    
    # Calculate row count
    if data and isinstance(data, list):
        row_count = len(data)
    else:
        row_count = 0
    
    logging.info(f"{start_date} ~ {end_date} ì¶”ì¶œ row ìˆ˜: {row_count}")
    return data, row_count


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Loading
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prepare_insert_data(data: list, extract_time: datetime) -> list:
    """Prepare data for PostgreSQL insertion"""
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ì§€ í™•ì¸í•˜ê³  ì²˜ë¦¬
    if data and isinstance(data[0], dict):
        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš° (PostgreSQL ê²°ê³¼)
        return [
            (
                row['mc_cd'],
                row['hot_1_1'], row['hot_1_2'], row['hot_1_3'], row['hot_1_4'], row['hot_1_5'],
                row['hot_2_1'], row['hot_2_2'], row['hot_2_3'], row['hot_2_4'], row['hot_2_5'],
                row['hot_3_1'], row['hot_3_2'], row['hot_3_3'], row['hot_3_4'], row['hot_3_5'],
                row['hot_4_1'], row['hot_4_2'], row['hot_4_3'], row['hot_4_4'], row['hot_4_5'],
                row['hot_5_1'], row['hot_5_2'], row['hot_5_3'], row['hot_5_4'], row['hot_5_5'],
                row['cool_1'], row['cool_2'], row['cool_3'], row['cool_4'], row['cool_5'],
                row['rst_ymd'],
                extract_time  # etl_extract_timeë§Œ ì „ë‹¬, etl_ingest_timeì€ PostgreSQL DEFAULT now() ì‚¬ìš©
            ) for row in data
        ]
    else:
        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš° (ê¸°ì¡´ ì½”ë“œ)
        return [
            (
                row[0],  # mc_cd
                row[1], row[2], row[3], row[4], row[5],  # hot_1_1~hot_1_5
                row[6], row[7], row[8], row[9], row[10],  # hot_2_1~hot_2_5
                row[11], row[12], row[13], row[14], row[15],  # hot_3_1~hot_3_5
                row[16], row[17], row[18], row[19], row[20],  # hot_4_1~hot_4_5
                row[21], row[22], row[23], row[24], row[25],  # hot_5_1~hot_5_5
                row[26], row[27], row[28], row[29], row[30],  # cool_1~cool_5
                row[31],  # rst_ymd
                extract_time  # etl_extract_timeë§Œ ì „ë‹¬, etl_ingest_timeì€ PostgreSQL DEFAULT now() ì‚¬ìš©
            ) for row in data
        ]


def get_column_names() -> list:
    """Get column names for PostgreSQL table"""
    return [
        "mc_cd",
        "hot_1_1", "hot_1_2", "hot_1_3", "hot_1_4", "hot_1_5",
        "hot_2_1", "hot_2_2", "hot_2_3", "hot_2_4", "hot_2_5",
        "hot_3_1", "hot_3_2", "hot_3_3", "hot_3_4", "hot_3_5",
        "hot_4_1", "hot_4_2", "hot_4_3", "hot_4_4", "hot_4_5",
        "hot_5_1", "hot_5_2", "hot_5_3", "hot_5_4", "hot_5_5",
        "cool_1", "cool_2", "cool_3", "cool_4", "cool_5",
        "rst_ymd", "etl_extract_time", "etl_ingest_time"
    ]


def load_data(pg: PostgresHelper, data: list, extract_time: datetime) -> None:
    """Load data into PostgreSQL database"""
    insert_data = prepare_insert_data(data, extract_time)
    columns = get_column_names()
    conflict_columns = ["mc_cd", "rst_ymd"]
    
    pg.insert_data(SCHEMA_NAME, TABLE_NAME, insert_data, columns, conflict_columns)
    logging.info(f"âœ… {len(data)} rows inserted (duplicates ignored).")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Variable Management
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def update_variable(end_extract_time: str) -> None:
    """Update Airflow variable with last extract time (always 23:59:59)"""
    # end_extract_timeì„ datetimeìœ¼ë¡œ íŒŒì‹±í•˜ì—¬ 23:59:59ë¡œ ì„¤ì •
    parsed_time = parse_datetime(end_extract_time)
    formatted_time = parsed_time.replace(hour=23, minute=59, second=59, microsecond=999999).strftime("%Y-%m-%d %H:%M:%S")
    
    Variable.set(INCREMENT_KEY, formatted_time)
    logging.info(f"ğŸ“Œ Variable `{INCREMENT_KEY}` Update: {formatted_time}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Incremental Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def daily_incremental_collection_task(**kwargs) -> dict:
    """ë§¤ì¼ ìµœì‹  ë°ì´í„°ë§Œ ìˆ˜ì§‘í•˜ëŠ” íƒœìŠ¤í¬"""
    # ë§¤ì¼ ì „ì¼ ë°ì´í„°ë§Œ ìˆ˜ì§‘
    last_extract_time_str = Variable.get(INCREMENT_KEY, default_var=None)
    
    if last_extract_time_str:
        # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì„ íŒŒì‹±
        last_extract_time = parse_datetime(last_extract_time_str)
        
        # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì˜ ë‹¤ìŒ ë‚  00:00:00ë¶€í„° 23:59:59ê¹Œì§€
        start_date = last_extract_time.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
        end_date = start_date.replace(hour=23, minute=59, second=59, microsecond=999999)
        
        logging.info(f"ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„: {last_extract_time_str}")
        logging.info(f"ë‹¤ìŒ ë‚  ë°ì´í„° ìˆ˜ì§‘: {start_date.strftime('%Y-%m-%d')}")
    else:
        # Variableì´ ì—†ìœ¼ë©´ ì–´ì œ ë°ì´í„° ìˆ˜ì§‘
        yesterday = datetime.now(INDO_TZ) - timedelta(days=1)
        start_date = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = yesterday.replace(hour=23, minute=59, second=59, microsecond=999999)
        logging.info(f"Variableì´ ì—†ì–´ì„œ ì–´ì œ ë°ì´í„° ìˆ˜ì§‘: {yesterday.strftime('%Y-%m-%d')}")
    
    start_str = start_date.strftime("%Y-%m-%d %H:%M:%S")
    end_str = end_date.strftime("%Y-%m-%d %H:%M:%S")
    
    logging.info(f"ğŸ“… ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {start_str} ~ {end_str}")
    logging.info(f"ğŸ“Š ì²˜ë¦¬ ë‚ ì§œ: {start_date.strftime('%Y-%m-%d')}")
    
    try:
        source_mssql = MSSQLHelper(conn_id=SOURCE_POSTGRES_CONN_ID)
        target_pg = PostgresHelper(conn_id=TARGET_POSTGRES_CONN_ID)
        
        # ë°ì´í„° ì¶”ì¶œ ë° ì ì¬
        data, row_count = extract_data(source_mssql, start_str, end_str)
        
        if row_count > 0:
            extract_time = datetime.utcnow()
            load_data(target_pg, data, extract_time)
            logging.info(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {row_count} rows")
            
            # Variable ì—…ë°ì´íŠ¸
            update_variable(end_str)
            
            return {
                "status": "daily_incremental_completed",
                "date": start_date.strftime("%Y-%m-%d"),
                "rows_processed": row_count,
                "start_time": start_str,
                "end_time": end_str,
                "extract_time": extract_time.isoformat()
            }
        else:
            logging.info(f"âš ï¸ ìˆ˜ì§‘í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {start_str} ~ {end_str}")
            
            # Variable ì—…ë°ì´íŠ¸ (ë°ì´í„°ê°€ ì—†ì–´ë„ ì‹œê°„ì€ ì—…ë°ì´íŠ¸)
            update_variable(end_str)
            
            return {
                "status": "daily_incremental_completed_no_data",
                "date": start_date.strftime("%Y-%m-%d"),
                "rows_processed": 0,
                "start_time": start_str,
                "end_time": end_str,
                "message": "ìˆ˜ì§‘í•  ë°ì´í„°ê°€ ì—†ìŒ"
            }
    except Exception as e:
        # ì—°ê²° ì‹¤íŒ¨ ë“± ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ì¸ ê²½ìš° Skip ì²˜ë¦¬
        error_str = str(e)
        error_lower = error_str.lower()
        is_connection_error = (
            "connection" in error_lower or
            "timeout" in error_lower or
            "timed out" in error_lower or
            "connection reset" in error_lower or
            "reset by peer" in error_lower or
            "errno" in error_lower or
            "network" in error_lower or
            "could not connect" in error_lower or
            "unable to connect" in error_lower or
            "login failed" in error_lower or
            "server is not found" in error_lower
        )
        
        if is_connection_error:
            logging.warning(f"âš ï¸ ì—°ê²° ì‹¤íŒ¨: {error_str} - íƒœìŠ¤í¬ Skip")
            # Skip ì „ì— Variable ì—…ë°ì´íŠ¸ (ì—°ê²° ì‹¤íŒ¨í•´ë„ ì‹œê°„ì€ ì—…ë°ì´íŠ¸í•˜ì—¬ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì˜¬ë°”ë¥¸ ì‹œì ë¶€í„° ì¬ì‹œë„)
            try:
                update_variable(end_str)
                logging.info(f"âœ… Variable '{INCREMENT_KEY}' ì—…ë°ì´íŠ¸ (ì—°ê²° ì‹¤íŒ¨ë¡œ Skip): {end_str}")
            except Exception as var_err:
                logging.warning(f"âš ï¸ Variable ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ë¬´ì‹œ): {var_err}")
            
            skip_msg = (
                f"â­ï¸ CTM Mold Temperature ETL ì¤‘ ì—°ê²° ë¶ˆê°€ - íƒœìŠ¤í¬ Skip\n"
                f"ì›ì¸: {error_str}\n"
                f"ì„¤ëª…: ì†ŒìŠ¤ ë˜ëŠ” íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\n"
                f"      Variableì€ ì—…ë°ì´íŠ¸ë˜ì—ˆìœ¼ë¯€ë¡œ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì¬ì‹œë„ë©ë‹ˆë‹¤."
            )
            logging.warning(skip_msg)
            raise AirflowSkipException(skip_msg) from e
        
        # ê·¸ ì™¸ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ raise
        logging.error(f"âŒ CTM Mold Temperature ETL ì‹¤íŒ¨: {e}")
        raise


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Backfill Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_monthly_batch(
    source_mssql: MSSQLHelper, 
    target_pg: PostgresHelper, 
    start_date: datetime, 
    end_date: datetime,
    loop_count: int,
    expected_loops: int
) -> dict:
    """Process a single monthly batch"""
    logging.info(f"ğŸ”„ ë£¨í”„ {loop_count}/{expected_loops} ì‹œì‘")
    
    start_str = start_date.strftime("%Y-%m-%d %H:%M:%S")
    end_str = end_date.strftime("%Y-%m-%d %H:%M:%S")
    
    logging.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {start_str} ~ {end_str}")
    
    data, row_count = extract_data(source_mssql, start_str, end_str)
    
    if row_count > 0:
        extract_time = datetime.utcnow()
        load_data(target_pg, data, extract_time)
        logging.info(f"âœ… ë°°ì¹˜ ì™„ë£Œ: {start_str} ~ {end_str} ({row_count} rows)")
    else:
        logging.info(f"ë°°ì¹˜ì— ë°ì´í„° ì—†ìŒ: {start_str} ~ {end_str}")
    
    update_variable(end_str)
    
    return {
        "loop": loop_count,
        "start": start_str,
        "end": end_str,
        "row_count": row_count,
        "batch_size_days": (end_date - start_date).days,
        "month": start_date.strftime("%Y-%m")
    }


def backfill_monthly_batch_task(**kwargs) -> dict:
    """Main backfill task for monthly batch processing"""
    source_mssql = MSSQLHelper(conn_id=SOURCE_POSTGRES_CONN_ID)
    target_pg = PostgresHelper(conn_id=TARGET_POSTGRES_CONN_ID)
    
    # Get start date from variable or use initial date
    last_extract_time = Variable.get(INCREMENT_KEY, default_var=None)
    if not last_extract_time:
        start_date = INITIAL_START_DATE
        logging.info(f"ì´ˆê¸° ì‹œì‘ ë‚ ì§œ ì‚¬ìš©: {start_date}")
    else:
        start_date = parse_datetime(last_extract_time)
        logging.info(f"ì´ì „ ì§„í–‰ ì§€ì  ì‚¬ìš©: {start_date}")
    
    # Set timezone and calculate end date
    if start_date.tzinfo is None:
        start_date = start_date.replace(tzinfo=INDO_TZ)
    
    # Backfillë„ incrementalê³¼ ë™ì¼í•˜ê²Œ í˜„ì¬ ì‹œê°„ì—ì„œ 2ì¼ ì „ê¹Œì§€ë§Œ ì²˜ë¦¬
    end_date = datetime.now(INDO_TZ).replace(
        minute=0, second=0, microsecond=0
    ) - timedelta(days=DAYS_OFFSET_FOR_INCREMENTAL)
    
    # Calculate expected loops
    expected_loops = calculate_expected_monthly_loops(start_date, end_date)
    
    # Log backfill information
    logging.info(f"Backfill ì‹œì‘: {start_date} ~ {end_date}")
    logging.info(f"ë°°ì¹˜ í¬ê¸°: ì›”ë³„ (ê° ì›”ì˜ ì‹¤ì œ ì¼ìˆ˜ì— ë§ì¶¤)")
    logging.info(f"ì˜ˆìƒ ë£¨í”„ íšŸìˆ˜: {expected_loops}íšŒ (ì›”ë³„)")
    logging.info(f"âš ï¸ í˜„ì¬ ì‹œê°„ì—ì„œ {DAYS_OFFSET_FOR_INCREMENTAL}ì¼ ì „ìœ¼ë¡œ ì„¤ì • (incremental DAG ì‹œì‘ì )")
    
    # Process monthly batches
    results = []
    total_processed = 0
    loop_count = 0
    current_date = start_date
    
    while current_date < end_date:
        loop_count += 1
        
        # ì›” ì‹œì‘ì¼ì„ 00:00:00ìœ¼ë¡œ ì„¤ì •
        month_start = current_date.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        
        # Calculate month end date
        month_end = get_month_end_date(current_date)
        if month_end > end_date:
            month_end = end_date
        
        # Process batch
        batch_result = process_monthly_batch(
            source_mssql, target_pg, month_start, month_end, loop_count, expected_loops
        )
        
        results.append(batch_result)
        total_processed += batch_result["row_count"]
        
        # Move to next month
        current_date = month_end + timedelta(days=1)
    
    # Log completion
    logging.info(f"ğŸ‰ Backfill ì™„ë£Œ! ì´ {loop_count}íšŒ ë£¨í”„, {total_processed}ê°œ rows ìˆ˜ì§‘")
    if results:
        logging.info(f"ì²˜ë¦¬ ê¸°ê°„: {results[0]['start']} ~ {results[-1]['end']}")
    
    return {
        "status": "backfill_completed",
        "total_loops": loop_count,
        "total_batches": len(results),
        "total_rows": total_processed,
        "results": results
    }

