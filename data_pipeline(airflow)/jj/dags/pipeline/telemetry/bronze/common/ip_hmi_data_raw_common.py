"""
IP HMI Data Raw Common Functions
=================================
ê³µí†µ í•¨ìˆ˜ ë° ì„¤ì •ì„ ëª¨ì•„ë‘” ëª¨ë“ˆ
"""

import logging
import json
import time
from datetime import datetime, timedelta, timezone
from airflow.models import Variable
from plugins.hooks.mysql_hook import MySQLHelper
from plugins.hooks.postgres_hook import PostgresHelper

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Database Configuration - ë¨¸ì‹ ë³„ ë…ë¦½ì ì¸ Variable ì‚¬ìš©
TABLE_NAME = "ip_hmi_data_raw"
SCHEMA_NAME = "bronze"

# Connection IDs (ì„¤ë¹„ ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬)
IP_CONN_DB = ["maria_ip_04", "maria_ip_12", "maria_ip_20", "maria_ip_34", "maria_ip_37"]
POSTGRES_CONN_ID = "pg_jj_telemetry_dw"  # TimescaleDB ëŒ€ìƒ

# Date Configuration
INDO_TZ = timezone(timedelta(hours=7))
HOURS_OFFSET_FOR_INCREMENTAL = 1  # 1ì‹œê°„ ì „ ë°ì´í„°ê¹Œì§€ (ì•ˆì „ ë§ˆì§„)

# ì„¼ì„œ ì„¤ì • (ì„¤ë¹„ ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬)
IP_MACHINE_NO = ["04", "12", "20", "34", "37"]  # ì„¼ì„œ ì¡´ ë²ˆí˜¸

# Backfill Configuration
INITIAL_START_DATE = datetime(2024, 8, 21, 0, 0, 0)

# Query Timeout Configuration
QUERY_TIMEOUT_SECONDS = 600  # ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ: 10ë¶„ (600ì´ˆ)
IP04_CHUNK_MINUTES = 10  # IP04 ì¿¼ë¦¬ ì§€ì—° ëŒ€ì‘: 10ë¶„ ë‹¨ìœ„ ë¶„í• 


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utility Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_increment_key(machine_no: str) -> str:
    """Get increment key for specific machine"""
    return f"last_extract_time_ip_hmi_data_raw_{machine_no}"


def parse_datetime(dt_str: str) -> datetime:
    """Parse datetime string with microsecond support"""
    try:
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S.%f")
    except ValueError:
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S")


def get_hour_end_date(start_date: datetime) -> datetime:
    """Get the end of the hour for a given date"""
    # ì‹œê°„ì„ ì •ê·œí™”í•´ì„œ í•´ë‹¹ ì‹œê°„ì˜ ë§ˆì§€ë§‰ ì´ˆë¡œ ì„¤ì • (ì˜ˆ: 21:00:00 -> 21:59:59)
    normalized_hour = start_date.replace(minute=0, second=0, microsecond=0)
    return normalized_hour.replace(minute=59, second=59, microsecond=999999)


def calculate_expected_hourly_loops(start_date: datetime, end_date: datetime) -> int:
    """Calculate expected number of hourly loops"""
    current_date = start_date
    hour_count = 0
    
    while current_date < end_date:
        hour_end = get_hour_end_date(current_date)
        if hour_end > end_date:
            hour_end = end_date
        current_date = hour_end + timedelta(hours=1)
        hour_count += 1
    
    return hour_count


def _iter_time_windows(start_dt: datetime, end_dt: datetime, chunk_minutes: int):
    """Iterate time windows between start/end with inclusive boundaries."""
    cur = start_dt
    while cur <= end_dt:
        window_end = cur + timedelta(minutes=chunk_minutes) - timedelta(seconds=1)
        if window_end > end_dt:
            window_end = end_dt
        yield cur, window_end
        cur = window_end + timedelta(seconds=1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Extraction
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_extract_sql(start_date: str, end_date: str) -> str:
    """Build SQL query for sensor data extraction"""
    return f'''
        SELECT
            SeqNo, PID, RxDate, Pvalue
        FROM rtf_data
        WHERE RxDate BETWEEN '{start_date}' AND '{end_date}'
        ORDER BY RxDate
    '''


def extract_data(mysql: MySQLHelper, start_date: str, end_date: str) -> tuple:
    """Extract sensor data from MySQL database
    
    ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ì´ 10ë¶„ì„ ì´ˆê³¼í•˜ë©´ TimeoutErrorë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
    """
    sql = build_extract_sql(start_date, end_date)
    logging.info(f"ì‹¤í–‰ ì¿¼ë¦¬: {sql}")
    
    # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    query_start_time = time.time()
    
    try:
        data = mysql.execute_query(sql, task_id="extract_data_task", xcom_key=None)
    except Exception as e:
        # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ í™•ì¸ (10ë¶„ ì´ˆê³¼ ì‹œ ì¶”ê°€ ì •ë³´ ì œê³µ)
        elapsed_time = time.time() - query_start_time
        if elapsed_time > QUERY_TIMEOUT_SECONDS:
            timeout_minutes = QUERY_TIMEOUT_SECONDS / 60
            error_msg = f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ì´ {timeout_minutes}ë¶„({QUERY_TIMEOUT_SECONDS}ì´ˆ)ì„ ì´ˆê³¼í•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ (ê²½ê³¼ ì‹œê°„: {elapsed_time:.1f}ì´ˆ)"
            logging.error(f"â±ï¸ {error_msg}")
            raise TimeoutError(error_msg) from e
        raise
    
    # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ í™•ì¸ (10ë¶„ ì´ˆê³¼ ì‹œ ì˜ˆì™¸ ë°œìƒ)
    elapsed_time = time.time() - query_start_time
    if elapsed_time > QUERY_TIMEOUT_SECONDS:
        timeout_minutes = QUERY_TIMEOUT_SECONDS / 60
        error_msg = f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ì´ {timeout_minutes}ë¶„({QUERY_TIMEOUT_SECONDS}ì´ˆ)ì„ ì´ˆê³¼í•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ (ê²½ê³¼ ì‹œê°„: {elapsed_time:.1f}ì´ˆ)"
        logging.error(f"â±ï¸ {error_msg}")
        raise TimeoutError(error_msg)
    
    # Calculate row count from MySQL result
    if data and (isinstance(data, list) or isinstance(data, tuple)) and len(data) > 0:
        row_count = len(data)
        logging.info(f"{start_date} ~ {end_date} ì¶”ì¶œ row ìˆ˜: {row_count} (ì‹¤í–‰ ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")
        logging.info("ìƒ˜í”Œ row: %s", data[0])
    else:
        row_count = 0
        logging.info(f"{start_date} ~ {end_date} ì¶”ì¶œ row ìˆ˜: {row_count} (ë°ì´í„° ì—†ìŒ, ì‹¤í–‰ ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")
    
    return data, row_count


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Loading
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prepare_insert_data(data: list, extract_time: datetime, machine_no: str) -> list:
    """Prepare sensor data for PostgreSQL insertion"""
    machine_no_with_prefix = f"MCA{machine_no}"
    
    if data and isinstance(data[0], dict):
        return [
            (
                machine_no_with_prefix,
                row['SeqNo'],
                row['PID'],
                row['RxDate'],
                row['Pvalue'],
                extract_time
            ) for row in data
        ]
    else:
        return [
            (
                machine_no_with_prefix,
                row[0],
                row[1],
                row[2],
                row[3],
                extract_time
            ) for row in data
        ]


def get_column_names() -> list:
    """Get column names for PostgreSQL table"""
    return [
        "machine_no",
        "seqno",
        "pid", 
        "rxdate",
        "pvalue",
        "etl_extract_time"
    ]


def load_data(pg: PostgresHelper, data: list, extract_time: datetime, machine_no: str) -> None:
    """Load sensor data into PostgreSQL TimescaleDB"""
    insert_data = prepare_insert_data(data, extract_time, machine_no)
    columns = get_column_names()
    conflict_columns = ["machine_no", "seqno", "rxdate"]
    
    pg.insert_data(SCHEMA_NAME, TABLE_NAME, insert_data, columns, conflict_columns, chunk_size=10000)
    logging.info(f"âœ… {len(data):,} rows inserted (duplicates ignored) for machine {machine_no}.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Variable Management
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_machine_variables(machine_no: str) -> dict:
    """Get machine-specific extract times from Variable"""
    try:
        increment_key = get_increment_key(machine_no)
        variables_str = Variable.get(increment_key, default_var="{}")
        return json.loads(variables_str)
    except Exception as e:
        logging.warning(f"Variable íŒŒì‹± ì‹¤íŒ¨, ë¹ˆ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©: {e}")
        return {}


def update_machine_variable(machine_no: str, end_extract_time: str) -> None:
    """Update machine-specific extract time in JSON Variable"""
    try:
        # Get current variables for this machine
        variables = get_machine_variables(machine_no)
        
        # Update specific machine
        mca_machine_no = f"MCA{machine_no}"
        variables[mca_machine_no] = end_extract_time
        
        # Save back to machine-specific Variable
        variable_key = get_increment_key(machine_no)
            
        try:
            Variable.set(variable_key, json.dumps(variables))
        except Exception as e:
            # ê¸°ì¡´ ê°’ì´ ìˆëŠ” ê²½ìš° ì—…ë°ì´íŠ¸
            if "already exists" in str(e):
                # Variableì„ ì‚­ì œí•˜ê³  ë‹¤ì‹œ ìƒì„±
                try:
                    Variable.delete(variable_key)
                except:
                    pass  # ì‚­ì œ ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
                Variable.set(variable_key, json.dumps(variables))
            else:
                raise e
        
        logging.info(f"ğŸ“Œ Machine {machine_no} Variable Update: {end_extract_time}")
        logging.info(f"ğŸ“Œ Machine {machine_no} Variable ìƒíƒœ: {variables}")
    except Exception as e:
        logging.error(f"âŒ Variable ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


def get_machine_last_extract_time(machine_no: str) -> str:
    """Get last extract time for specific machine"""
    variables = get_machine_variables(machine_no)
    mca_machine_no = f"MCA{machine_no}"
    return variables.get(mca_machine_no, None)


def get_machine_start_date(machine_no: str) -> datetime:
    """Get start date for specific machine"""
    variables = get_machine_variables(machine_no)
    
    # Check for MCA prefix in variables
    mca_machine_no = f"MCA{machine_no}"
    if mca_machine_no in variables:
        # Variableì— ì €ì¥ëœ ì‹œê°„ì€ KST ì‹œê°„ (MySQL ì‹œê°„)
        last_time_str = variables[mca_machine_no]
        last_time = parse_datetime(last_time_str)
        # KST (UTC+9)ë¡œ ì„¤ì •
        if last_time.tzinfo is None:
            kst_tz = timezone(timedelta(hours=9))
            last_time = last_time.replace(tzinfo=kst_tz)
        # INDO_TZ (UTC+7)ë¡œ ë³€í™˜
        last_time_indo = last_time.astimezone(INDO_TZ)
        logging.info(f"Machine {machine_no} ì´ì „ ì§„í–‰ ì§€ì  ì‚¬ìš© (KST -> INDO_TZ): {last_time} -> {last_time_indo}")
        return last_time_indo
    else:
        logging.info(f"Machine {machine_no} ì´ˆê¸° ì‹œì‘ ë‚ ì§œ ì‚¬ìš©: {INITIAL_START_DATE}")
        return INITIAL_START_DATE


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Hourly Incremental Collection
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_hourly_incremental_for_machine(
    mysql: MySQLHelper, 
    pg: PostgresHelper, 
    start_date: datetime, 
    end_date: datetime,
    machine_no: str
) -> dict:
    """Process hourly incremental collection for specific machine"""
    # start_dateì™€ end_dateëŠ” ì´ë¯¸ KST ì‹œê°„ëŒ€ë¡œ ì „ë‹¬ë¨
    start_str = start_date.strftime("%Y-%m-%d %H:%M:%S")
    end_str = end_date.strftime("%Y-%m-%d %H:%M:%S")

    logging.info(f"ğŸ“… ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {start_str} ~ {end_str} (Machine: {machine_no})")

    chunk_minutes = IP04_CHUNK_MINUTES if machine_no == "04" else 60
    total_rows = 0
    last_extract_time = None

    for window_start, window_end in _iter_time_windows(start_date, end_date, chunk_minutes):
        w_start_str = window_start.strftime("%Y-%m-%d %H:%M:%S")
        w_end_str = window_end.strftime("%Y-%m-%d %H:%M:%S")
        logging.info(
            f"ğŸ§© ë¶„í•  ìˆ˜ì§‘({chunk_minutes}ë¶„, Machine {machine_no}): {w_start_str} ~ {w_end_str}"
        )

        try:
            data, row_count = extract_data(mysql, w_start_str, w_end_str)
        except Exception as e:
            error_msg = str(e)
            logging.warning(f"âš ï¸ MySQL ì—°ê²°/ì¿¼ë¦¬ ì‹¤íŒ¨ (Machine {machine_no}): {error_msg} - ìŠ¤í‚µí•©ë‹ˆë‹¤")
            return {
                "machine_no": machine_no,
                "rows_processed": 0,
                "start_time": start_str,
                "end_time": end_str,
                "status": "failed",
                "error": error_msg
            }

        if row_count > 0:
            extract_time = datetime.utcnow()
            load_data(pg, data, extract_time, machine_no)
            total_rows += row_count
            last_extract_time = extract_time
            logging.info(f"âœ… ë¶„í•  ìˆ˜ì§‘ ì™„ë£Œ: {row_count} rows (Machine: {machine_no})")
        else:
            logging.info(f"âš ï¸ ë¶„í•  ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ: {w_start_str} ~ {w_end_str} (Machine: {machine_no})")

    # Update machine-specific variable (ëª¨ë“  ë¶„í•  ì²˜ë¦¬ ì™„ë£Œ í›„)
    update_machine_variable(machine_no, end_str)

    if total_rows > 0:
        return {
            "machine_no": machine_no,
            "rows_processed": total_rows,
            "start_time": start_str,
            "end_time": end_str,
            "extract_time": last_extract_time.isoformat() if last_extract_time else None,
            "status": "completed"
        }

    return {
        "machine_no": machine_no,
        "rows_processed": 0,
        "start_time": start_str,
        "end_time": end_str,
        "status": "no_data"
    }


def process_machine_incremental(machine_no: str, machine_index: int) -> dict:
    """Process hourly incremental collection for specific machine"""
    logging.info(f"ğŸ”„ Machine {machine_no} ì²˜ë¦¬ ì‹œì‘")
    
    # Get machine-specific last extract time
    last_extract_time_str = get_machine_last_extract_time(machine_no)
    
    # ìµœëŒ€ í—ˆìš© ì‹œê°„: í˜„ì¬ ì‹œê°„ - 1ì‹œê°„ (ì•ˆì „ ë§ˆì§„)
    max_allowed_time = datetime.now(INDO_TZ) - timedelta(hours=1)
    max_allowed_time = max_allowed_time.replace(minute=0, second=0, microsecond=0)
    
    if last_extract_time_str:
        # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì„ íŒŒì‹±
        # Variableì— ì €ì¥ëœ ì‹œê°„ì€ KST ì‹œê°„ (MySQL ì‹œê°„)
        last_extract_time = parse_datetime(last_extract_time_str)
        # KST (UTC+9)ë¡œ ì„¤ì •
        if last_extract_time.tzinfo is None:
            kst_tz = timezone(timedelta(hours=9))
            last_extract_time = last_extract_time.replace(tzinfo=kst_tz)
        # INDO_TZ (UTC+7)ë¡œ ë³€í™˜
        last_extract_time_indo = last_extract_time.astimezone(INDO_TZ)
        
        # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì˜ ë‹¤ìŒ ì‹œê°„ë¶€í„° 1ì‹œê°„ ë™ì•ˆ
        start_date = last_extract_time_indo.replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)
        end_date = start_date.replace(minute=59, second=59, microsecond=999999)
        
        # ğŸ”’ ì•ˆì „ ì œì•½: ìµœëŒ€ í—ˆìš© ì‹œê°„ì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì œí•œ
        if start_date > max_allowed_time:
            logging.warning(f"âš ï¸ Machine {machine_no} ìš”ì²­ ì‹œê°„ì´ ì•ˆì „ ë§ˆì§„ì„ ì´ˆê³¼í•©ë‹ˆë‹¤!")
            logging.warning(f"   ìš”ì²­ ì‹œê°„: {start_date.strftime('%Y-%m-%d %H:00')}")
            logging.warning(f"   ìµœëŒ€ í—ˆìš©: {max_allowed_time.strftime('%Y-%m-%d %H:00')}")
            logging.warning(f"   Machine {machine_no} ë°ì´í„° ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {
                "machine_no": machine_no,
                "rows_processed": 0,
                "status": "skipped_safety_margin",
                "reason": f"Requested time exceeds safety margin (max: {max_allowed_time.strftime('%Y-%m-%d %H:00')})"
            }
        
        logging.info(f"Machine {machine_no} ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„: {last_extract_time_str}")
        logging.info(f"Machine {machine_no} ë‹¤ìŒ ì‹œê°„ ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘: {start_date.strftime('%Y-%m-%d %H:00')}")
    else:
        # Variableì´ ì—†ìœ¼ë©´ 1ì‹œê°„ ì „ ë°ì´í„° ìˆ˜ì§‘ (ì•ˆì „ ë§ˆì§„ ì ìš©)
        one_hour_ago = datetime.now(INDO_TZ) - timedelta(hours=1)
        start_date = one_hour_ago.replace(minute=0, second=0, microsecond=0)
        end_date = start_date.replace(minute=59, second=59, microsecond=999999)
        logging.info(f"Machine {machine_no} Variableì´ ì—†ì–´ì„œ 1ì‹œê°„ ì „ ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘: {start_date.strftime('%Y-%m-%d %H:00')}")
    
    # ì‹œê°„ëŒ€ ë³€í™˜: INDO_TZ (UTC+7) -> KST (UTC+9) = +2ì‹œê°„
    # MySQLì€ KST ì‹œê°„ìœ¼ë¡œ ì €ì¥ë¨
    start_date_kst = start_date.astimezone(timezone(timedelta(hours=9)))  # INDO_TZ -> KST
    end_date_kst = end_date.astimezone(timezone(timedelta(hours=9)))  # INDO_TZ -> KST
    
    start_str = start_date_kst.strftime("%Y-%m-%d %H:%M:%S")
    end_str = end_date_kst.strftime("%Y-%m-%d %H:%M:%S")
    
    logging.info(f"ğŸ“… Machine {machine_no} ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {start_str} ~ {end_str}")
    logging.info(f"ğŸ” ì‹œê°„ëŒ€ ë³€í™˜: INDO_TZ {start_date.strftime('%Y-%m-%d %H:%M:%S %Z')} -> KST {start_str}")
    
    # Process this machine
    conn_id = IP_CONN_DB[machine_index]
    try:
        mysql = MySQLHelper(conn_id=conn_id)
        pg = PostgresHelper(conn_id=POSTGRES_CONN_ID)
    except Exception as e:
        logging.warning(f"âš ï¸ ì—°ê²° ì‹¤íŒ¨ (Machine {machine_no}): {str(e)} - ìŠ¤í‚µí•©ë‹ˆë‹¤")
        return {
            "machine_no": machine_no,
            "rows_processed": 0,
            "status": "skipped",
            "reason": "connection_failed",
            "error": str(e)
        }
    
    try:
        machine_result = process_hourly_incremental_for_machine(
            mysql, pg, start_date_kst, end_date_kst, machine_no
        )
        logging.info(f"âœ… Machine {machine_no} ì™„ë£Œ: {machine_result['rows_processed']} rows")
        return machine_result
    except Exception as e:
        logging.error(f"âŒ Machine {machine_no} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return {
            "machine_no": machine_no,
            "rows_processed": 0,
            "status": "failed",
            "error": str(e)
        }


def create_incremental_task(machine_no: str, machine_index: int):
    """Create incremental task for specific machine"""
    def incremental_task(**kwargs) -> dict:
        return process_machine_incremental(machine_no, machine_index)
    return incremental_task


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Backfill Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_hourly_batch_for_machine(
    mysql: MySQLHelper, 
    pg: PostgresHelper, 
    start_date: datetime, 
    end_date: datetime,
    machine_no: str,
    loop_count: int,
    expected_loops: int
) -> dict:
    """Process a single hourly batch for specific machine"""
    logging.info(f"ğŸ”„ ë£¨í”„ {loop_count}/{expected_loops} ì‹œì‘ (Machine: {machine_no})")
    
    # ì‹œê°„ëŒ€ ë³€í™˜: INDO_TZ (UTC+7) -> KST (UTC+9) = +2ì‹œê°„
    # MySQLì€ KST ì‹œê°„ìœ¼ë¡œ ì €ì¥ë¨
    start_date_kst = start_date.astimezone(timezone(timedelta(hours=9)))  # INDO_TZ -> KST
    end_date_kst = end_date.astimezone(timezone(timedelta(hours=9)))  # INDO_TZ -> KST
    
    start_str = start_date_kst.strftime("%Y-%m-%d %H:%M:%S")
    end_str = end_date_kst.strftime("%Y-%m-%d %H:%M:%S")
    
    logging.info(f"ì‹œê°„ë³„ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {start_str} ~ {end_str} (Machine: {machine_no})")
    logging.info(f"ğŸ” ì‹œê°„ëŒ€ ë³€í™˜: INDO_TZ {start_date.strftime('%Y-%m-%d %H:%M:%S %Z')} -> KST {start_str}")
    
    data, row_count = extract_data(mysql, start_str, end_str)
    
    if row_count > 0:
        extract_time = datetime.utcnow()
        load_data(pg, data, extract_time, machine_no)
        logging.info(f"âœ… ì‹œê°„ë³„ ë°°ì¹˜ ì™„ë£Œ: {start_str} ~ {end_str} ({row_count:,} rows) for machine {machine_no}")
    else:
        logging.info(f"ì‹œê°„ë³„ ë°°ì¹˜ì— ë°ì´í„° ì—†ìŒ: {start_str} ~ {end_str} (Machine: {machine_no})")
    
    # Update machine-specific variable
    update_machine_variable(machine_no, end_str)
    
    return {
        "loop": loop_count,
        "machine_no": machine_no,
        "start": start_str,
        "end": end_str,
        "row_count": row_count,
        "batch_size_hours": (end_date - start_date).total_seconds() / 3600,
        "datetime": start_date.strftime("%Y-%m-%d %H:00")
    }


def process_machine_backfill(machine_no: str, machine_index: int) -> dict:
    """Process backfill for a single machine"""
    # Calculate end date (common for all machines) - ì•ˆì „ ë§ˆì§„ ì ìš©
    end_date = datetime.now(INDO_TZ).replace(
        minute=0, second=0, microsecond=0
    ) - timedelta(hours=1)  # ìµœì†Œ 1ì‹œê°„ ì•ˆì „ ë§ˆì§„
    
    logging.info(f"ğŸ”„ Machine {machine_no} ì²˜ë¦¬ ì‹œì‘")
    
    # Get machine-specific start date
    start_date = get_machine_start_date(machine_no)
    
    # Set timezone
    if start_date.tzinfo is None:
        start_date = start_date.replace(tzinfo=INDO_TZ)
    
    # Calculate expected loops for this machine
    expected_loops = calculate_expected_hourly_loops(start_date, end_date)
    
    logging.info(f"Machine {machine_no} ì‹œì‘: {start_date} ~ {end_date}")
    logging.info(f"Machine {machine_no} ì˜ˆìƒ ë£¨í”„: {expected_loops}íšŒ (ì‹œê°„ë³„)")
    
    # Process hourly batches for this machine
    machine_results = []
    loop_count = 0
    current_date = start_date
    
    while current_date < end_date:
        loop_count += 1
        
        # Calculate hour end date
        hour_end = get_hour_end_date(current_date)
        if hour_end > end_date:
            hour_end = end_date
        
        # Process this hour for this machine
        conn_id = IP_CONN_DB[machine_index]
        
        # ì‹œê°„ì„ ì •ê·œí™” (00:00:00 í˜•íƒœë¡œ)
        normalized_start = current_date.replace(minute=0, second=0, microsecond=0)
        normalized_end = hour_end.replace(minute=59, second=59, microsecond=999999)
        
        try:
            # ì—°ê²° ì‹œë„
            mysql = MySQLHelper(conn_id=conn_id)
            pg = PostgresHelper(conn_id=POSTGRES_CONN_ID)
        except Exception as e:
            logging.warning(f"âš ï¸ ì—°ê²° ì‹¤íŒ¨ (Machine {machine_no}, ë£¨í”„ {loop_count}): {str(e)} - ìŠ¤í‚µí•©ë‹ˆë‹¤")
            # ì—°ê²° ì‹¤íŒ¨ ì‹œì—ë„ Variable ì—…ë°ì´íŠ¸ëŠ” ì§„í–‰ (ì§„í–‰ ìƒí™© ì¶”ì )
            end_str = normalized_end.astimezone(timezone(timedelta(hours=9))).strftime("%Y-%m-%d %H:%M:%S")
            update_machine_variable(machine_no, end_str)
            continue
        
        try:
            batch_result = process_hourly_batch_for_machine(
                mysql, pg, normalized_start, normalized_end, machine_no, loop_count, expected_loops
            )
            machine_results.append(batch_result)
            logging.info(f"âœ… Machine {machine_no} ë£¨í”„ {loop_count} ì™„ë£Œ: {batch_result['row_count']:,} rows")
        except Exception as e:
            error_msg = str(e)
            logging.warning(f"âš ï¸ Machine {machine_no} ë£¨í”„ {loop_count} ì‹¤íŒ¨: {error_msg} - ìŠ¤í‚µí•©ë‹ˆë‹¤")
            # ì—°ê²° ì‹¤íŒ¨ë‚˜ ì¿¼ë¦¬ ì‹¤íŒ¨ ì‹œì—ë„ Variable ì—…ë°ì´íŠ¸ëŠ” ì§„í–‰
            end_str = normalized_end.astimezone(timezone(timedelta(hours=9))).strftime("%Y-%m-%d %H:%M:%S")
            update_machine_variable(machine_no, end_str)
        
        # Move to next hour
        current_date = hour_end + timedelta(hours=1)
    
    total_rows = sum([r['row_count'] for r in machine_results])
    logging.info(f"ğŸ‰ Machine {machine_no} ì™„ë£Œ! {len(machine_results)}íšŒ ë£¨í”„, {total_rows:,}ê°œ rows")
    
    return {
        "status": "backfill_completed",
        "machine_no": machine_no,
        "total_batches": len(machine_results),
        "total_rows": sum([r['row_count'] for r in machine_results]),
        "results": machine_results
    }


def create_backfill_task(machine_no: str, machine_index: int):
    """Create backfill task for specific machine"""
    def backfill_task(**kwargs) -> dict:
        return process_machine_backfill(machine_no, machine_index)
    return backfill_task

