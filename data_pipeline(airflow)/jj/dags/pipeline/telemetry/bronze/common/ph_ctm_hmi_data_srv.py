"""
PH CTM HMI Data Common Functions
==================================
ê³µí†µ í•¨ìˆ˜ ë° ì„¤ì •ì„ ëª¨ì•„ë‘” ëª¨ë“ˆ
"""

import logging
import threading
from datetime import datetime, timedelta, timezone
from airflow.exceptions import AirflowSkipException
from airflow.models import Variable
from plugins.hooks.mysql_hook import MySQLHelper
from plugins.hooks.postgres_hook import PostgresHelper

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

INDO_TZ = timezone(timedelta(hours=7))
EQUIPMENTS = [
    {"equipment_id": "ph01", "equipment": 100, "machine_id": "PH01", "conn_id": "maria_ph_01", "var_key": "last_extract_time_ph_ctm_hmi_data_ph01"},
]
TARGET_POSTGRES_CONN_ID = "pg_jj_telemetry_dw"
SCHEMA_NAME = "bronze"
TABLE_NAME = "ph_ctm_hmi_data"
HOURS_OFFSET_FOR_INCREMENTAL = 1
DEFAULT_MARKER_HOURS_BACK = 2
INITIAL_START_DATE = datetime(2025, 9, 24, 0, 0, 0)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utility Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _test_connection_quick(mysql, timeout_seconds, result):
    """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” ë¹ ë¥¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        with mysql.hook.get_conn() as conn, conn.cursor() as cursor:
            cursor.execute("SELECT 1")
        result['success'] = True
    except Exception as e:
        result['success'] = False
        result['error'] = str(e)


def check_mysql_connection_quick(mysql, conn_id: str, timeout_seconds: int = 5) -> bool:
    """ë¹ ë¥¸ ì—°ê²° í™•ì¸ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)"""
    result = {'success': False, 'error': None}
    thread = threading.Thread(target=_test_connection_quick, args=(mysql, timeout_seconds, result))
    thread.daemon = True
    thread.start()
    thread.join(timeout=timeout_seconds)
    
    if thread.is_alive():
        logging.warning(f"âš ï¸ ì—°ê²° íƒ€ì„ì•„ì›ƒ: {conn_id} ({timeout_seconds}ì´ˆ ì´ˆê³¼)")
        return False
    
    if result['success']:
        return True
    else:
        logging.warning(f"âš ï¸ ì—°ê²° ë¶ˆê°€: {conn_id} - {result.get('error', 'Unknown error')}")
        return False


def _eod(dt: datetime) -> datetime:
    """end of day: 23:59:59.999999"""
    return dt.replace(hour=23, minute=59, second=59, microsecond=999999)


def _eoh(dt: datetime) -> datetime:
    """end of hour: hh:59:59"""
    return dt.replace(minute=59, second=59, microsecond=999999)


def _get_default_marker() -> datetime:
    """Get default marker for incremental processing"""
    base = (datetime.now(INDO_TZ) - timedelta(hours=DEFAULT_MARKER_HOURS_BACK)).astimezone(INDO_TZ)
    return _eoh(base)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Extraction
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_extract_sql(start_dt: datetime, end_dt: datetime, equipment: int, machine_id: str) -> str:
    """Build SQL query for HMI data extraction"""
    start_str = start_dt.strftime("%Y-%m-%d %H:%M:%S")
    end_str = end_dt.strftime("%Y-%m-%d %H:%M:%S")
    return f"""
        SELECT 
            0 AS factory,
            {equipment} AS equipment,
            '{machine_id}' AS machine_id,
            d.SeqNo AS seq_no,
            d.PID AS pid,
            d.RxDate AS rx_date,
            d.PValue AS p_value,
            d.RxDate_Year AS rxdate_year,
            d.RxDate_Month AS rxdate_month,
            d.RxDate_Day AS rxdate_day
        FROM rtf_data d
        WHERE d.RxDate >= '{start_str}' AND d.RxDate <= '{end_str}'
        ORDER BY d.RxDate
    """


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Loading
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prepare_rows(rows: list, extract_time: datetime) -> list:
    """Prepare rows for PostgreSQL insertion"""
    out = []
    for r in rows:
        out.append(tuple(list(r) + [extract_time, datetime.utcnow()]))
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Incremental Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_incremental(equipment_id: str, conn_id: str, var_key: str, equipment: int, machine_id: str, **context):
    """Run incremental collection for specific equipment"""
    # hard_end ê³„ì‚° (Skipëœ ê²½ìš° Variable ì—…ë°ì´íŠ¸ìš©)
    hard_cap = (datetime.now(INDO_TZ) - timedelta(hours=HOURS_OFFSET_FOR_INCREMENTAL)).astimezone(INDO_TZ)
    hard_cap = _eoh(hard_cap)
    
    try:
        mysql = MySQLHelper(conn_id=conn_id)
        pg = PostgresHelper(conn_id=TARGET_POSTGRES_CONN_ID)
        # ì‹¤ì œ ì—°ê²° í…ŒìŠ¤íŠ¸ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
        if not check_mysql_connection_quick(mysql, conn_id, timeout_seconds=5):
            logging.warning(f"âš ï¸ ì—°ê²° ë¶ˆê°€ (Eq{equipment_id}, {conn_id}) - ìŠ¤í‚µí•©ë‹ˆë‹¤")
            # Skipëœ ê²½ìš°ì—ë„ Variable ì—…ë°ì´íŠ¸ (í˜„ì¬ ì‹œê°„ - 1ì‹œê°„ê¹Œì§€ ì²˜ë¦¬í•˜ë ¤ê³  í–ˆë˜ ë§ˆì»¤)
            Variable.set(var_key, hard_cap.strftime("%Y-%m-%d %H:%M:%S"))
            logging.info(f"âœ… [{equipment_id}] Variable '{var_key}' ì—…ë°ì´íŠ¸ (Skipëœ ê²½ìš°): {hard_cap.strftime('%Y-%m-%d %H:%M:%S')}")
            skip_msg = f"â­ï¸ ì—°ê²° ë¶ˆê°€ (Eq{equipment_id}, {conn_id}) - íƒœìŠ¤í¬ Skip"
            raise AirflowSkipException(skip_msg)
    except Exception as e:
        # AirflowSkipExceptionì€ ê·¸ëŒ€ë¡œ ì „íŒŒ
        if isinstance(e, AirflowSkipException):
            raise
        logging.warning(f"âš ï¸ ì—°ê²° ì‹¤íŒ¨ (Eq{equipment_id}): {str(e)} - ìŠ¤í‚µí•©ë‹ˆë‹¤")
        # Skipëœ ê²½ìš°ì—ë„ Variable ì—…ë°ì´íŠ¸ (í˜„ì¬ ì‹œê°„ - 1ì‹œê°„ê¹Œì§€ ì²˜ë¦¬í•˜ë ¤ê³  í–ˆë˜ ë§ˆì»¤)
        Variable.set(var_key, hard_cap.strftime("%Y-%m-%d %H:%M:%S"))
        logging.info(f"âœ… [{equipment_id}] Variable '{var_key}' ì—…ë°ì´íŠ¸ (Skipëœ ê²½ìš°): {hard_cap.strftime('%Y-%m-%d %H:%M:%S')}")
        skip_msg = f"â­ï¸ ì—°ê²° ì‹¤íŒ¨ (Eq{equipment_id}): {str(e)} - íƒœìŠ¤í¬ Skip"
        raise AirflowSkipException(skip_msg) from e

    val = Variable.get(var_key, default_var="")
    if val:
        try:
            # ISO í˜•ì‹ ì‹œë„ (2025-10-31T05:59:59+07:00 ë˜ëŠ” 2025-10-31T05:59:59)
            if 'T' in val or '+' in val or val.count('-') >= 3:
                parsed = datetime.fromisoformat(val)
            else:
                # ê³µë°± í˜•ì‹ ì‹œë„ (2025-10-31 05:59:59)
                parsed = datetime.strptime(val, "%Y-%m-%d %H:%M:%S")
            if parsed.tzinfo is None:
                parsed = parsed.replace(tzinfo=INDO_TZ)
            else:
                parsed = parsed.astimezone(INDO_TZ)
            last_marker = parsed
        except Exception as e:
            logging.warning(f"âš ï¸ Variable íŒŒì‹± ì‹¤íŒ¨ ({var_key}): {val}, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            last_marker = _get_default_marker()
    else:
        last_marker = _get_default_marker()
    
    # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì˜ ë‹¤ìŒ ì‹œê°„ë¶€í„° 1ì‹œê°„ ë™ì•ˆë§Œ ì²˜ë¦¬
    cur_start = (last_marker + timedelta(seconds=1)).astimezone(INDO_TZ)
    cur_start = cur_start.replace(minute=0, second=0, microsecond=0)
    
    # ì •í™•íˆ 1ì‹œê°„ë§Œ ì²˜ë¦¬ (ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ê·¸ ë‹¤ìŒ 1ì‹œê°„ ì²˜ë¦¬)
    cur_end = _eoh(cur_start)

    # hard_capì€ í•¨ìˆ˜ ì‹œì‘ ë¶€ë¶„ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨

    logging.info(f"ğŸ” ë””ë²„ê¹… ì •ë³´ (Eq{equipment_id}): Variable={val}, last_marker={last_marker}, cur_start={cur_start}, cur_end={cur_end}, hard_cap={hard_cap}")

    # ì²˜ë¦¬í•  ì‹œê°„ì´ hard_capì„ ì´ˆê³¼í•˜ë©´ ìŠ¤í‚µ
    if cur_start > hard_cap:
        logging.info(f"â„¹ï¸ í˜„ì¬ -1ì‹œê°„ ì œí•œìœ¼ë¡œ ì²˜ë¦¬ êµ¬ê°„ì´ ìœ íš¨í•˜ì§€ ì•Šì•„ ìŠ¤í‚µí•©ë‹ˆë‹¤ (Eq{equipment_id}): cur_start={cur_start} > hard_cap={hard_cap}")
        return {"status": "success", "rows": 0, "message": "skipped by -1h cap"}

    # cur_endê°€ hard_capì„ ì´ˆê³¼í•˜ë©´ hard_capê¹Œì§€ë§Œ ì²˜ë¦¬
    if cur_end > hard_cap:
        cur_end = hard_cap
        logging.info(f"âš ï¸ ì²˜ë¦¬ êµ¬ê°„ì´ hard_capì„ ì´ˆê³¼í•˜ì—¬ ì¡°ì •: cur_end={cur_end}")

    sql = build_extract_sql(cur_start, cur_end, equipment, machine_id)
    logging.info(f"ğŸš€ Incremental ì‹¤í–‰ ì¿¼ë¦¬(1ì‹œê°„ ë‹¨ìœ„, Eq{equipment_id}): {cur_start} ~ {cur_end}\n{sql}")

    total_rows = 0
    
    try:
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ: 5000 -> 2000)
        batch_size = 2000
        batch_total = 0
        
        for batch_rows in mysql.execute_query_streaming(sql, "ph_ctm_hmi_data_incremental_extract", batch_size=batch_size):
            if batch_rows:
                insert_data = prepare_rows(batch_rows, datetime.utcnow())
                columns = [
                    "factory", "equipment", "machine_id", "seq_no", "pid", "rx_date", "p_value",
                    "rxdate_year", "rxdate_month", "rxdate_day", "etl_extract_time", "etl_ingest_time"
                ]
                conflict_columns = ["factory", "equipment", "machine_id", "seq_no", "pid", "rx_date"]
                # Insert ì²­í¬ë„ ë” ì‘ê²Œ (500 -> 300)
                pg.insert_data(SCHEMA_NAME, TABLE_NAME, insert_data, columns, conflict_columns, chunk_size=300)
                batch_total += len(batch_rows)
                total_rows += len(batch_rows)
                # ë©”ëª¨ë¦¬ ì •ë¦¬ íŒíŠ¸ (Python GCëŠ” ìë™ì´ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œ)
                del insert_data
        
        if batch_total > 0:
            logging.info(f"ğŸ“¦ ph_ctm_hmi_data ì¶”ì¶œ row ìˆ˜ (Eq{equipment_id}): {batch_total:,}")

        # ì²˜ë¦¬ ì™„ë£Œí•œ ê²½ìš° Variable ì—…ë°ì´íŠ¸ (ë°ì´í„°ê°€ ì—†ì–´ë„ ì‹œê°„ì€ ì—…ë°ì´íŠ¸í•˜ì—¬ ë‹¤ìŒ ì‹œê°„ëŒ€ë¡œ ì§„í–‰)
        Variable.set(var_key, cur_end.strftime("%Y-%m-%d %H:%M:%S"))
        if total_rows > 0:
            logging.info(f"âœ… ph_ctm_hmi_data 1ì‹œê°„ ë‹¨ìœ„ ì¦ë¶„ ì™„ë£Œ (Eq{equipment_id}), ì´ {total_rows:,} rows ì²˜ë¦¬, ë‹¤ìŒ ì²˜ë¦¬ ì‹œê°„: {cur_end + timedelta(seconds=1)}")
        else:
            logging.info(f"âœ… ph_ctm_hmi_data ì²˜ë¦¬ ì™„ë£Œ (Eq{equipment_id}), ì²˜ë¦¬ëœ row ì—†ìŒ (Variable ì—…ë°ì´íŠ¸í•˜ì—¬ ë‹¤ìŒ ì‹œê°„ëŒ€ë¡œ ì§„í–‰: {cur_end + timedelta(seconds=1)})")
        
    except Exception as e:
        error_msg = str(e)
        logging.warning(f"âš ï¸ MySQL ì—°ê²°/ì¿¼ë¦¬ ì‹¤íŒ¨ (Eq{equipment_id}): {error_msg} - ì´ë²ˆ ì‹œê°„ëŒ€ ìŠ¤í‚µ (Variable ì—…ë°ì´íŠ¸ ì•ˆ í•¨)")
        return {"status": "failed", "rows": 0, "error": error_msg}

    return {"status": "success", "rows": total_rows}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Backfill Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_backfill(equipment_id: str, conn_id: str, var_key: str, equipment: int, machine_id: str, **context):
    """Process backfill for specific equipment"""
    try:
        mysql = MySQLHelper(conn_id=conn_id)
        pg = PostgresHelper(conn_id=TARGET_POSTGRES_CONN_ID)
        # ì‹¤ì œ ì—°ê²° í…ŒìŠ¤íŠ¸ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
        if not check_mysql_connection_quick(mysql, conn_id, timeout_seconds=5):
            logging.warning(f"âš ï¸ ì—°ê²° ë¶ˆê°€ (Eq{equipment_id}, {conn_id}) - ìŠ¤í‚µí•©ë‹ˆë‹¤")
            return {"status": "skipped", "reason": "connection_unavailable"}
    except Exception as e:
        logging.warning(f"âš ï¸ ì—°ê²° ì‹¤íŒ¨ (Eq{equipment_id}): {str(e)} - ìŠ¤í‚µí•©ë‹ˆë‹¤")
        return {"status": "skipped", "reason": "connection_failed", "error": str(e)}

    # ì‹œì‘ ì§€ì  ê²°ì •: Variable â†’ INITIAL_START_DATE
    base_start = INITIAL_START_DATE.replace(tzinfo=INDO_TZ).replace(hour=0, minute=0, second=0, microsecond=0)

    cursor_str = Variable.get(var_key, default_var=None)
    if cursor_str:
        try:
            cursor_dt = datetime.fromisoformat(cursor_str)
            if cursor_dt.tzinfo is None:
                cursor_dt = cursor_dt.replace(tzinfo=INDO_TZ)
            else:
                cursor_dt = cursor_dt.astimezone(INDO_TZ)
            cur_start = (cursor_dt + timedelta(seconds=1))
        except Exception:
            cur_start = base_start
    else:
        cur_start = base_start

    # ì¢…ë£ŒëŠ” í˜„ì¬ ì‹œê° ê¸°ì¤€ HOURS_OFFSET_FOR_INCREMENTAL ì‹œê°„ ì „ (ì‹œê°„ ë‹¨ìœ„ ì²˜ë¦¬)
    hard_end = (datetime.now(INDO_TZ) - timedelta(hours=HOURS_OFFSET_FOR_INCREMENTAL)).astimezone(INDO_TZ)
    hard_end = _eoh(hard_end)

    chunk_hours = 1
    total_rows = 0

    while cur_start <= hard_end:
        # ì •í™•íˆ í•´ë‹¹ ì‹œê°ì˜ "ì‹œ ë§"ê¹Œì§€(ì˜ˆ: 10:00:00 â†’ 10:59:59), hard_endë¥¼ ë„˜ì§€ ì•Šê²Œ ì œí•œ
        cur_end = min(_eoh(cur_start), hard_end)
        sql = build_extract_sql(cur_start, cur_end, equipment, machine_id)
        logging.info(f"ğŸš€ Backfill ì¿¼ë¦¬(ì‹œê°„ë‹¨ìœ„, {machine_id}): {cur_start} ~ {cur_end}\n{sql}")

        try:
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë°°ì¹˜ í¬ê¸°: 2000)
            batch_size = 2000
            batch_total = 0
            
            for batch_rows in mysql.execute_query_streaming(sql, "ph_ctm_hmi_data_backfill_extract", batch_size=batch_size):
                if batch_rows:
                    insert_data = prepare_rows(batch_rows, datetime.utcnow())
                    columns = [
                        "factory", "equipment", "machine_id", "seq_no", "pid", "rx_date", "p_value",
                        "rxdate_year", "rxdate_month", "rxdate_day", "etl_extract_time", "etl_ingest_time"
                    ]
                    conflict_columns = ["factory", "equipment", "machine_id", "seq_no", "pid", "rx_date"]
                    # Insert ì²­í¬ë„ ë” ì‘ê²Œ (300)
                    pg.insert_data(SCHEMA_NAME, TABLE_NAME, insert_data, columns, conflict_columns, chunk_size=300)
                    batch_total += len(batch_rows)
                    total_rows += len(batch_rows)
                    # ë©”ëª¨ë¦¬ ì •ë¦¬ íŒíŠ¸
                    del insert_data
            
            if batch_total > 0:
                logging.info(f"ğŸ“¦ ph_ctm_hmi_data ì¶”ì¶œ row ìˆ˜ ({machine_id}): {batch_total:,}")

            # ì²˜ë¦¬í•œ êµ¬ê°„ì˜ ëì‹œê°„(í•­ìƒ hh:59:59)ì„ ì»¤ì„œë¡œ ì €ì¥
            Variable.set(var_key, cur_end.strftime("%Y-%m-%d %H:%M:%S"))
        except Exception as e:
            error_msg = str(e)
            logging.warning(f"âš ï¸ MySQL ì—°ê²°/ì¿¼ë¦¬ ì‹¤íŒ¨ ({machine_id}, {cur_start} ~ {cur_end}): {error_msg} - í•´ë‹¹ ì‹œê°„ëŒ€ ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰")
            # ì—°ê²° ë¬¸ì œë¡œ ì‹¤íŒ¨í•œ ê²½ìš°, ë§ˆì»¤ëŠ” ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šê³  ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°
            # Variable ì—…ë°ì´íŠ¸ë¥¼ í•˜ì§€ ì•Šì•„ì„œ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ê°™ì€ ì‹œê°„ëŒ€ë¥¼ ë‹¤ì‹œ ì‹œë„í•  ìˆ˜ ìˆìŒ

        cur_start = (cur_end + timedelta(seconds=1)).astimezone(INDO_TZ)

    logging.info(f"âœ… ph_ctm_hmi_data backfill ì™„ë£Œ ({machine_id}), ì´ {total_rows} rows")
    return {"status": "success", "rows": total_rows}
