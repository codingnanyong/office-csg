"""
OS Banbury HMI Data Common Functions
=====================================
ê³µí†µ í•¨ìˆ˜ ë° ì„¤ì •ì„ ëª¨ì•„ë‘” ëª¨ë“ˆ
"""

import gc
import logging
import threading
import time
from datetime import datetime, timedelta, timezone
from airflow.exceptions import AirflowSkipException
from airflow.models import Variable
from plugins.hooks.mysql_hook import MySQLHelper
from plugins.hooks.postgres_hook import PostgresHelper

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

INDO_TZ = timezone(timedelta(hours=7))
EQUIPMENTS = [
    {"equipment_id": "1", "equipment_value": 3001, "conn_id": "maria_jj_os_banb_1", "var_key": "last_extract_time_os_banb_hmi_data_eq1"},
    {"equipment_id": "3", "equipment_value": 3003, "conn_id": "maria_jj_os_banb_3", "var_key": "last_extract_time_os_banb_hmi_data_eq3"},
]
TARGET_POSTGRES_CONN_ID = "pg_jj_telemetry_dw"
SCHEMA_NAME = "bronze"
TABLE_NAME = "os_banb_hmi_data"
HOURS_OFFSET_FOR_INCREMENTAL = 1
DEFAULT_MARKER_HOURS_BACK = 2
INITIAL_START_DATE = datetime(2025, 10, 27, 0, 0, 0)
QUERY_TIMEOUT_SECONDS = 600  # ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ: 10ë¶„ (600ì´ˆ)
EQ1_CHUNK_MINUTES = 10  # Eq1 ì¿¼ë¦¬ ì‘ë‹µ ì§€ì—° ëŒ€ì‘: 10ë¶„ ë‹¨ìœ„ë¡œ ë¶„í• 


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utility Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _test_connection_quick(mysql, timeout_seconds, result):
    """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” ë¹ ë¥¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        with mysql.hook.get_conn() as conn, conn.cursor() as cursor:
            cursor.execute("SELECT 1")
        result['success'] = True
    except Exception as e:
        result['success'] = False
        result['error'] = str(e)


def check_mysql_connection_quick(mysql, conn_id: str, timeout_seconds: int = 5) -> bool:
    """ë¹ ë¥¸ ì—°ê²° í™•ì¸ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)"""
    result = {'success': False, 'error': None}
    thread = threading.Thread(target=_test_connection_quick, args=(mysql, timeout_seconds, result))
    thread.daemon = True
    thread.start()
    thread.join(timeout=timeout_seconds)
    
    if thread.is_alive():
        logging.warning(f"âš ï¸ ì—°ê²° íƒ€ì„ì•„ì›ƒ: {conn_id} ({timeout_seconds}ì´ˆ ì´ˆê³¼)")
        return False
    
    if result['success']:
        return True
    else:
        logging.warning(f"âš ï¸ ì—°ê²° ë¶ˆê°€: {conn_id} - {result.get('error', 'Unknown error')}")
        return False


def _eod(dt: datetime) -> datetime:
    """end of day: 23:59:59.999999"""
    return dt.replace(hour=23, minute=59, second=59, microsecond=999999)


def _eoh(dt: datetime) -> datetime:
    """end of hour: hh:59:59"""
    return dt.replace(minute=59, second=59, microsecond=999999)


def _get_default_marker() -> datetime:
    """Get default marker for incremental processing"""
    base = (datetime.now(INDO_TZ) - timedelta(hours=DEFAULT_MARKER_HOURS_BACK)).astimezone(INDO_TZ)
    return _eoh(base)


def _iter_time_windows(start_dt: datetime, end_dt: datetime, chunk_minutes: int):
    """Iterate time windows between start/end with inclusive boundaries."""
    cur = start_dt
    while cur <= end_dt:
        window_end = cur + timedelta(minutes=chunk_minutes) - timedelta(seconds=1)
        if window_end > end_dt:
            window_end = end_dt
        yield cur, window_end
        cur = window_end + timedelta(seconds=1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Extraction
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_extract_sql(start_dt: datetime, end_dt: datetime, equipment_value: int) -> str:
    """Build SQL query for HMI data extraction
    
    ì„±ëŠ¥ ìµœì í™”:
    - ORDER BY ì œê±°: PostgreSQL INSERT ì‹œ ìˆœì„œê°€ í•„ìš” ì—†ì–´ ì œê±°í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
    - INNER JOIN ëª…ì‹œ: ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ INNER JOIN ëª…ì‹œ
    """
    start_str = start_dt.strftime("%Y-%m-%d %H:%M:%S")
    end_str = end_dt.strftime("%Y-%m-%d %H:%M:%S")
    return f"""
        SELECT 
            COALESCE(s.Factory, 1) AS factory,
            COALESCE(s.Equipment, {equipment_value}) AS equipment,
            d.SeqNo AS seq_no,
            d.PID AS pid,
            d.RxDate AS rx_date,
            d.PValue AS p_value,
            d.RxDate_Year AS rxdate_year,
            d.RxDate_Month AS rxdate_month,
            d.RxDate_Day AS rxdate_day
        FROM rtf_data d
        INNER JOIN rtf_sensor s ON s.PID = d.PID
        WHERE d.RxDate >= '{start_str}' AND d.RxDate <= '{end_str}'
    """


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Loading
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prepare_rows(rows: list, extract_time: datetime) -> list:
    """Prepare rows for PostgreSQL insertion"""
    out = []
    for r in rows:
        out.append(tuple(list(r) + [extract_time, datetime.utcnow()]))
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Incremental Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_incremental(equipment_id: str, conn_id: str, var_key: str, equipment_value: int, **context):
    """Run incremental collection for specific equipment"""
    # hard_end ê³„ì‚° (Skipëœ ê²½ìš° Variable ì—…ë°ì´íŠ¸ìš©)
    hard_cap = (datetime.now(INDO_TZ) - timedelta(hours=HOURS_OFFSET_FOR_INCREMENTAL)).astimezone(INDO_TZ)
    hard_cap = _eoh(hard_cap)
    
    try:
        mysql = MySQLHelper(conn_id=conn_id)
        pg = PostgresHelper(conn_id=TARGET_POSTGRES_CONN_ID)
        # ì‹¤ì œ ì—°ê²° í…ŒìŠ¤íŠ¸ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
        if not check_mysql_connection_quick(mysql, conn_id, timeout_seconds=5):
            logging.warning(f"âš ï¸ ì—°ê²° ë¶ˆê°€ (Eq{equipment_id}, {conn_id}) - ìŠ¤í‚µí•©ë‹ˆë‹¤")
            # Skipëœ ê²½ìš°ì—ë„ Variable ì—…ë°ì´íŠ¸ (í˜„ì¬ ì‹œê°„ - 1ì‹œê°„ê¹Œì§€ ì²˜ë¦¬í•˜ë ¤ê³  í–ˆë˜ ë§ˆì»¤)
            Variable.set(var_key, hard_cap.strftime("%Y-%m-%d %H:%M:%S"))
            logging.info(f"âœ… [{equipment_id}] Variable '{var_key}' ì—…ë°ì´íŠ¸ (Skipëœ ê²½ìš°): {hard_cap.strftime('%Y-%m-%d %H:%M:%S')}")
            skip_msg = f"â­ï¸ ì—°ê²° ë¶ˆê°€ (Eq{equipment_id}, {conn_id}) - íƒœìŠ¤í¬ Skip"
            raise AirflowSkipException(skip_msg)
    except Exception as e:
        # AirflowSkipExceptionì€ ê·¸ëŒ€ë¡œ ì „íŒŒ
        if isinstance(e, AirflowSkipException):
            raise
        logging.warning(f"âš ï¸ ì—°ê²° ì‹¤íŒ¨ (Eq{equipment_id}): {str(e)} - ìŠ¤í‚µí•©ë‹ˆë‹¤")
        # Skipëœ ê²½ìš°ì—ë„ Variable ì—…ë°ì´íŠ¸ (í˜„ì¬ ì‹œê°„ - 1ì‹œê°„ê¹Œì§€ ì²˜ë¦¬í•˜ë ¤ê³  í–ˆë˜ ë§ˆì»¤)
        Variable.set(var_key, hard_cap.strftime("%Y-%m-%d %H:%M:%S"))
        logging.info(f"âœ… [{equipment_id}] Variable '{var_key}' ì—…ë°ì´íŠ¸ (Skipëœ ê²½ìš°): {hard_cap.strftime('%Y-%m-%d %H:%M:%S')}")
        skip_msg = f"â­ï¸ ì—°ê²° ì‹¤íŒ¨ (Eq{equipment_id}): {str(e)} - íƒœìŠ¤í¬ Skip"
        raise AirflowSkipException(skip_msg) from e

    val = Variable.get(var_key, default_var="")
    if val:
        try:
            # ISO í˜•ì‹ ì‹œë„ (2025-10-31T05:59:59+07:00 ë˜ëŠ” 2025-10-31T05:59:59)
            if 'T' in val or '+' in val or val.count('-') >= 3:
                parsed = datetime.fromisoformat(val)
            else:
                # ê³µë°± í˜•ì‹ ì‹œë„ (2025-10-31 05:59:59)
                parsed = datetime.strptime(val, "%Y-%m-%d %H:%M:%S")
            if parsed.tzinfo is None:
                parsed = parsed.replace(tzinfo=INDO_TZ)
            else:
                parsed = parsed.astimezone(INDO_TZ)
            last_marker = parsed
        except Exception as e:
            logging.warning(f"âš ï¸ Variable íŒŒì‹± ì‹¤íŒ¨ ({var_key}): {val}, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            last_marker = _get_default_marker()
    else:
        last_marker = _get_default_marker()
    
    # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì˜ ë‹¤ìŒ ì‹œê°„ë¶€í„° 1ì‹œê°„ ë™ì•ˆë§Œ ì²˜ë¦¬
    cur_start = (last_marker + timedelta(seconds=1)).astimezone(INDO_TZ)
    cur_start = cur_start.replace(minute=0, second=0, microsecond=0)
    
    # ì •í™•íˆ 1ì‹œê°„ë§Œ ì²˜ë¦¬ (ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ê·¸ ë‹¤ìŒ 1ì‹œê°„ ì²˜ë¦¬)
    cur_end = _eoh(cur_start)

    # hard_capì€ í•¨ìˆ˜ ì‹œì‘ ë¶€ë¶„ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨

    logging.info(f"ğŸ” ë””ë²„ê¹… ì •ë³´ (Eq{equipment_id}): Variable={val}, last_marker={last_marker}, cur_start={cur_start}, cur_end={cur_end}, hard_cap={hard_cap}")

    # ì²˜ë¦¬í•  ì‹œê°„ì´ hard_capì„ ì´ˆê³¼í•˜ë©´ ìŠ¤í‚µ
    if cur_start > hard_cap:
        logging.info(f"â„¹ï¸ í˜„ì¬ -1ì‹œê°„ ì œí•œìœ¼ë¡œ ì²˜ë¦¬ êµ¬ê°„ì´ ìœ íš¨í•˜ì§€ ì•Šì•„ ìŠ¤í‚µí•©ë‹ˆë‹¤ (Eq{equipment_id}): cur_start={cur_start} > hard_cap={hard_cap}")
        return {"status": "success", "rows": 0, "message": "skipped by -1h cap"}

    # cur_endê°€ hard_capì„ ì´ˆê³¼í•˜ë©´ hard_capê¹Œì§€ë§Œ ì²˜ë¦¬
    if cur_end > hard_cap:
        cur_end = hard_cap
        logging.info(f"âš ï¸ ì²˜ë¦¬ êµ¬ê°„ì´ hard_capì„ ì´ˆê³¼í•˜ì—¬ ì¡°ì •: cur_end={cur_end}")

    total_rows = 0
    
    try:
        # Eq1ì€ ì‘ë‹µ ì§€ì—°ì´ ì¦ì•„ 10ë¶„ ë‹¨ìœ„ë¡œ ë¶„í•  ì²˜ë¦¬
        chunk_minutes = EQ1_CHUNK_MINUTES if equipment_id == "1" else 60
        for window_start, window_end in _iter_time_windows(cur_start, cur_end, chunk_minutes):
            sql = build_extract_sql(window_start, window_end, equipment_value)
            logging.info(
                f"ğŸš€ Incremental ì‹¤í–‰ ì¿¼ë¦¬({chunk_minutes}ë¶„ ë‹¨ìœ„, Eq{equipment_id}): {window_start} ~ {window_end}\n{sql}"
            )

            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë°°ì¹˜ í¬ê¸° ë” ì¶•ì†Œ: 2000 -> 1000, ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°)
            batch_size = 1000
            batch_total = 0
            query_start_time = time.time()  # ì¿¼ë¦¬ ì‹œì‘ ì‹œê°„ ê¸°ë¡

            for batch_rows in mysql.execute_query_streaming(
                sql,
                "os_banb_hmi_data_incremental_extract",
                batch_size=batch_size,
                query_timeout_seconds=QUERY_TIMEOUT_SECONDS,
            ):
                # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ í™•ì¸ (10ë¶„ ì´ˆê³¼ ì‹œ ì¤‘ë‹¨)
                elapsed_time = time.time() - query_start_time
                if elapsed_time > QUERY_TIMEOUT_SECONDS:
                    timeout_minutes = QUERY_TIMEOUT_SECONDS / 60
                    error_msg = (
                        f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ì´ {timeout_minutes}ë¶„({QUERY_TIMEOUT_SECONDS}ì´ˆ)ì„ ì´ˆê³¼í•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ "
                        f"(ê²½ê³¼ ì‹œê°„: {elapsed_time:.1f}ì´ˆ, ì²˜ë¦¬ëœ í–‰: {batch_total:,}ê°œ, êµ¬ê°„: {window_start} ~ {window_end})"
                    )
                    logging.error(f"â±ï¸ {error_msg}")
                    raise TimeoutError(error_msg)

                if batch_rows:
                    insert_data = prepare_rows(batch_rows, datetime.utcnow())
                    columns = [
                        "factory", "equipment", "seq_no", "pid", "rx_date", "p_value",
                        "rxdate_year", "rxdate_month", "rxdate_day", "etl_extract_time", "etl_ingest_time"
                    ]
                    conflict_columns = ["factory", "equipment", "seq_no", "rx_date"]
                    # Insert ì²­í¬ë„ ë” ì‘ê²Œ (300 -> 150, ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°)
                    pg.insert_data(SCHEMA_NAME, TABLE_NAME, insert_data, columns, conflict_columns, chunk_size=150)
                    batch_total += len(batch_rows)
                    total_rows += len(batch_rows)
                    # ë©”ëª¨ë¦¬ ì •ë¦¬ (ëª…ì‹œì  ì‚­ì œ + ê°€ë¹„ì§€ ì»¬ë ‰ì…˜)
                    del insert_data
                    del batch_rows
                    # ì£¼ê¸°ì ìœ¼ë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€)
                    if batch_total % 5000 == 0:
                        gc.collect()

            if batch_total > 0:
                logging.info(f"ğŸ“¦ os_banb_hmi_data ì¶”ì¶œ row ìˆ˜ (Eq{equipment_id}): {batch_total:,} (êµ¬ê°„: {window_start} ~ {window_end})")

        # ì²˜ë¦¬ ì™„ë£Œí•œ ê²½ìš° Variable ì—…ë°ì´íŠ¸ (ë°ì´í„°ê°€ ì—†ì–´ë„ ì‹œê°„ì€ ì—…ë°ì´íŠ¸í•˜ì—¬ ë‹¤ìŒ ì‹œê°„ëŒ€ë¡œ ì§„í–‰)
        Variable.set(var_key, cur_end.strftime("%Y-%m-%d %H:%M:%S"))
        if total_rows > 0:
            logging.info(f"âœ… os_banb_hmi_data 1ì‹œê°„ ë‹¨ìœ„ ì¦ë¶„ ì™„ë£Œ (Eq{equipment_id}), ì´ {total_rows:,} rows ì²˜ë¦¬, ë‹¤ìŒ ì²˜ë¦¬ ì‹œê°„: {cur_end + timedelta(seconds=1)}")
        else:
            logging.info(f"âœ… os_banb_hmi_data ì²˜ë¦¬ ì™„ë£Œ (Eq{equipment_id}), ì²˜ë¦¬ëœ row ì—†ìŒ (Variable ì—…ë°ì´íŠ¸í•˜ì—¬ ë‹¤ìŒ ì‹œê°„ëŒ€ë¡œ ì§„í–‰: {cur_end + timedelta(seconds=1)})")
        
    except TimeoutError as e:
        error_msg = str(e)
        logging.error(f"â±ï¸ ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ (Eq{equipment_id}): {error_msg} - ì´ë²ˆ ì‹œê°„ëŒ€ ìŠ¤í‚µ")
        # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ Variable ì—…ë°ì´íŠ¸í•˜ì—¬ ë‹¤ìŒ ì‹œê°„ëŒ€ë¡œ ì§„í–‰
        Variable.set(var_key, cur_end.strftime("%Y-%m-%d %H:%M:%S"))
        logging.info(f"âœ… [{equipment_id}] Variable '{var_key}' ì—…ë°ì´íŠ¸ (íƒ€ì„ì•„ì›ƒ ë°œìƒ): {cur_end.strftime('%Y-%m-%d %H:%M:%S')}")
        skip_msg = f"â­ï¸ ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ (Eq{equipment_id}) - íƒœìŠ¤í¬ Skip"
        raise AirflowSkipException(skip_msg) from e
    except Exception as e:
        error_msg = str(e)
        logging.warning(f"âš ï¸ MySQL ì—°ê²°/ì¿¼ë¦¬ ì‹¤íŒ¨ (Eq{equipment_id}): {error_msg} - ì´ë²ˆ ì‹œê°„ëŒ€ ìŠ¤í‚µ (Variable ì—…ë°ì´íŠ¸ ì•ˆ í•¨)")
        return {"status": "failed", "rows": 0, "error": error_msg}

    return {"status": "success", "rows": total_rows}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Backfill Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_backfill(equipment_id: str, conn_id: str, var_key: str, equipment_value: int, **context):
    """Process backfill for specific equipment"""
    try:
        mysql = MySQLHelper(conn_id=conn_id)
        pg = PostgresHelper(conn_id=TARGET_POSTGRES_CONN_ID)
        # ì‹¤ì œ ì—°ê²° í…ŒìŠ¤íŠ¸ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
        if not check_mysql_connection_quick(mysql, conn_id, timeout_seconds=5):
            logging.warning(f"âš ï¸ ì—°ê²° ë¶ˆê°€ (Eq{equipment_id}, {conn_id}) - ìŠ¤í‚µí•©ë‹ˆë‹¤")
            return {"status": "skipped", "reason": "connection_unavailable"}
    except Exception as e:
        logging.warning(f"âš ï¸ ì—°ê²° ì‹¤íŒ¨ (Eq{equipment_id}): {str(e)} - ìŠ¤í‚µí•©ë‹ˆë‹¤")
        return {"status": "skipped", "reason": "connection_failed", "error": str(e)}

    # ì‹œì‘ ì§€ì  ê²°ì •: Variable â†’ INITIAL_START_DATE
    base_start = INITIAL_START_DATE.replace(tzinfo=INDO_TZ).replace(hour=0, minute=0, second=0, microsecond=0)

    cursor_str = Variable.get(var_key, default_var=None)
    if cursor_str:
        try:
            cursor_dt = datetime.fromisoformat(cursor_str)
            if cursor_dt.tzinfo is None:
                cursor_dt = cursor_dt.replace(tzinfo=INDO_TZ)
            else:
                cursor_dt = cursor_dt.astimezone(INDO_TZ)
            cur_start = (cursor_dt + timedelta(seconds=1))
        except Exception:
            cur_start = base_start
    else:
        cur_start = base_start

    # ì¢…ë£ŒëŠ” í˜„ì¬ ì‹œê° ê¸°ì¤€ HOURS_OFFSET_FOR_INCREMENTAL ì‹œê°„ ì „ (ì‹œê°„ ë‹¨ìœ„ ì²˜ë¦¬)
    hard_end = (datetime.now(INDO_TZ) - timedelta(hours=HOURS_OFFSET_FOR_INCREMENTAL)).astimezone(INDO_TZ)
    hard_end = _eoh(hard_end)

    chunk_hours = 1
    total_rows = 0

    while cur_start <= hard_end:
        # ì •í™•íˆ í•´ë‹¹ ì‹œê°ì˜ "ì‹œ ë§"ê¹Œì§€(ì˜ˆ: 10:00:00 â†’ 10:59:59), hard_endë¥¼ ë„˜ì§€ ì•Šê²Œ ì œí•œ
        cur_end = min(_eoh(cur_start), hard_end)
        try:
            # Eq1ì€ ì‘ë‹µ ì§€ì—°ì´ ì¦ì•„ 10ë¶„ ë‹¨ìœ„ë¡œ ë¶„í•  ì²˜ë¦¬
            chunk_minutes = EQ1_CHUNK_MINUTES if equipment_id == "1" else 60
            failed_window = False

            for window_start, window_end in _iter_time_windows(cur_start, cur_end, chunk_minutes):
                sql = build_extract_sql(window_start, window_end, equipment_value)
                logging.info(
                    f"ğŸš€ Backfill ì¿¼ë¦¬({chunk_minutes}ë¶„ ë‹¨ìœ„, Eq{equipment_id}): {window_start} ~ {window_end}\n{sql}"
                )

                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë°°ì¹˜ í¬ê¸°: 1000, ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°)
                batch_size = 1000
                batch_total = 0
                query_start_time = time.time()  # ì¿¼ë¦¬ ì‹œì‘ ì‹œê°„ ê¸°ë¡

                for batch_rows in mysql.execute_query_streaming(
                    sql,
                    "os_banb_hmi_data_backfill_extract",
                    batch_size=batch_size,
                    query_timeout_seconds=QUERY_TIMEOUT_SECONDS,
                ):
                    # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ í™•ì¸ (10ë¶„ ì´ˆê³¼ ì‹œ ì¤‘ë‹¨)
                    elapsed_time = time.time() - query_start_time
                    if elapsed_time > QUERY_TIMEOUT_SECONDS:
                        timeout_minutes = QUERY_TIMEOUT_SECONDS / 60
                        error_msg = (
                            f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ì´ {timeout_minutes}ë¶„({QUERY_TIMEOUT_SECONDS}ì´ˆ)ì„ ì´ˆê³¼í•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ "
                            f"(ê²½ê³¼ ì‹œê°„: {elapsed_time:.1f}ì´ˆ, ì²˜ë¦¬ëœ í–‰: {batch_total:,}ê°œ, êµ¬ê°„: {window_start} ~ {window_end})"
                        )
                        logging.error(f"â±ï¸ {error_msg}")
                        raise TimeoutError(error_msg)

                    if batch_rows:
                        insert_data = prepare_rows(batch_rows, datetime.utcnow())
                        columns = [
                            "factory", "equipment", "seq_no", "pid", "rx_date", "p_value",
                            "rxdate_year", "rxdate_month", "rxdate_day", "etl_extract_time", "etl_ingest_time"
                        ]
                        conflict_columns = ["factory", "equipment", "seq_no", "rx_date"]
                        # Insert ì²­í¬ë„ ë” ì‘ê²Œ (150, ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°)
                        pg.insert_data(SCHEMA_NAME, TABLE_NAME, insert_data, columns, conflict_columns, chunk_size=150)
                        batch_total += len(batch_rows)
                        total_rows += len(batch_rows)
                        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ëª…ì‹œì  ì‚­ì œ + ê°€ë¹„ì§€ ì»¬ë ‰ì…˜)
                        del insert_data
                        del batch_rows
                        # ì£¼ê¸°ì ìœ¼ë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€)
                        if batch_total % 5000 == 0:
                            gc.collect()

                if batch_total > 0:
                    logging.info(
                        f"ğŸ“¦ os_banb_hmi_data ì¶”ì¶œ row ìˆ˜ (Eq{equipment_id}): {batch_total:,} (êµ¬ê°„: {window_start} ~ {window_end})"
                    )
            
            if failed_window:
                raise Exception("window_failed")

            # ì²˜ë¦¬í•œ êµ¬ê°„ì˜ ëì‹œê°„(í•­ìƒ hh:59:59)ì„ ì»¤ì„œë¡œ ì €ì¥
            Variable.set(var_key, cur_end.strftime("%Y-%m-%d %H:%M:%S"))
        except Exception as e:
            error_msg = str(e)
            logging.warning(f"âš ï¸ MySQL ì—°ê²°/ì¿¼ë¦¬ ì‹¤íŒ¨ (Eq{equipment_id}, {cur_start} ~ {cur_end}): {error_msg} - í•´ë‹¹ ì‹œê°„ëŒ€ ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰")
            # ì—°ê²° ë¬¸ì œë¡œ ì‹¤íŒ¨í•œ ê²½ìš°, ë§ˆì»¤ëŠ” ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šê³  ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°
            # Variable ì—…ë°ì´íŠ¸ë¥¼ í•˜ì§€ ì•Šì•„ì„œ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ê°™ì€ ì‹œê°„ëŒ€ë¥¼ ë‹¤ì‹œ ì‹œë„í•  ìˆ˜ ìˆìŒ

        cur_start = (cur_end + timedelta(seconds=1)).astimezone(INDO_TZ)

    logging.info(f"âœ… os_banb_hmi_data backfill ì™„ë£Œ (Eq{equipment_id}), ì´ {total_rows} rows")
    return {"status": "success", "rows": total_rows}

