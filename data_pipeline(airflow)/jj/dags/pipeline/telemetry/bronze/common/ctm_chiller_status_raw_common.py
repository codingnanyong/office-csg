"""
CTM Chiller Status Raw Common Functions
=======================================
ê³µí†µ í•¨ìˆ˜ ë° ì„¤ì •ì„ ëª¨ì•„ë‘” ëª¨ë“ˆ
"""

import logging
from datetime import datetime, timedelta, timezone
from airflow.exceptions import AirflowSkipException
from airflow.models import Variable
from plugins.hooks.postgres_hook import PostgresHelper

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Database Configuration
INCREMENT_KEY = "last_extract_time_ctm_chiller_status_raw"
TABLE_NAME = "ctm_chiller_status_raw"
SCHEMA_NAME = "bronze"

# Connection IDs
SOURCE_POSTGRES_CONN_ID = "pg_ckp_chiller"
TARGET_POSTGRES_CONN_ID = "pg_jj_telemetry_dw"

# Date Configuration
INDO_TZ = timezone(timedelta(hours=7))
DAYS_OFFSET_FOR_INCREMENTAL = 2
INITIAL_START_DATE = datetime(2025, 8, 1, 0, 0, 0)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utility Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_datetime(dt_str: str) -> datetime:
    """Parse datetime string with microsecond support"""
    try:
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S.%f")
    except ValueError:
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S")


def get_week_end_date(start_date: datetime) -> datetime:
    """Get the end of the week (7 days later) for a given date"""
    week_end = start_date + timedelta(days=6)
    # 23:59:59ë¡œ ì„¤ì •
    return week_end.replace(hour=23, minute=59, second=59, microsecond=999999)


def calculate_expected_weekly_loops(start_date: datetime, end_date: datetime) -> int:
    """Calculate expected number of weekly loops"""
    current_date = start_date
    week_count = 0
    
    while current_date < end_date:
        week_end = get_week_end_date(current_date)
        if week_end > end_date:
            week_end = end_date
        current_date = week_end + timedelta(days=1)
        week_count += 1
    
    return week_count


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Extraction
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_extract_sql(start_date: str, end_date: str) -> str:
    """Build SQL query for CTM chiller status data extraction"""
    return f'''
        SELECT 
            device_id,
            water_in_temp,
            water_out_temp,
            external_temp,
            discharge_temp_1,
            discharge_temp_2,
            discharge_temp_3,
            discharge_temp_4,
            sv_temp,
            digitals,
            upd_dt
        FROM public.status
        WHERE upd_dt >= '{start_date}' 
          AND upd_dt <= '{end_date}'
        ORDER BY upd_dt
    '''


def extract_data(pg: PostgresHelper, start_date: str, end_date: str) -> tuple:
    """Extract data from CTM PostgreSQL database"""
    sql = build_extract_sql(start_date, end_date)
    logging.info(f"ì‹¤í–‰ ì¿¼ë¦¬: {sql}")
    
    data = pg.execute_query(sql, task_id="extract_data_task", xcom_key=None)
    
    # Calculate row count
    if data and isinstance(data, list):
        row_count = len(data)
    else:
        row_count = 0
    
    logging.info(f"{start_date} ~ {end_date} ì¶”ì¶œ row ìˆ˜: {row_count}")
    return data, row_count


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Loading
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prepare_insert_data(data: list, extract_time: datetime) -> list:
    """Prepare data for PostgreSQL insertion"""
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ì§€ í™•ì¸í•˜ê³  ì²˜ë¦¬
    if data and isinstance(data[0], dict):
        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš° (PostgreSQL ê²°ê³¼)
        return [
            (
                row['device_id'],
                row['water_in_temp'],
                row['water_out_temp'],
                row['external_temp'],
                row['discharge_temp_1'],
                row['discharge_temp_2'],
                row['discharge_temp_3'],
                row['discharge_temp_4'],
                row['sv_temp'],
                row['digitals'],
                row['upd_dt'],
                extract_time  # etl_extract_timeë§Œ ì „ë‹¬, etl_ingest_timeì€ PostgreSQL DEFAULT now() ì‚¬ìš©
            ) for row in data
        ]
    else:
        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš° (ê¸°ì¡´ ì½”ë“œ)
        return [
            (
                row[0],   # device_id
                row[1],   # water_in_temp
                row[2],   # water_out_temp
                row[3],   # external_temp
                row[4],   # discharge_temp_1
                row[5],   # discharge_temp_2
                row[6],   # discharge_temp_3
                row[7],   # discharge_temp_4
                row[8],   # sv_temp
                row[9],   # digitals
                row[10],  # upd_dt
                extract_time  # etl_extract_timeë§Œ ì „ë‹¬, etl_ingest_timeì€ PostgreSQL DEFAULT now() ì‚¬ìš©
            ) for row in data
        ]


def get_column_names() -> list:
    """Get column names for PostgreSQL table"""
    return [
        "device_id",
        "water_in_temp",
        "water_out_temp",
        "external_temp",
        "discharge_temp_1",
        "discharge_temp_2",
        "discharge_temp_3",
        "discharge_temp_4",
        "sv_temp",
        "digitals",
        "upd_dt",
        "etl_extract_time",
        "etl_ingest_time"
    ]


def load_data(pg: PostgresHelper, data: list, extract_time: datetime) -> None:
    """Load data into PostgreSQL database with upsert"""
    insert_data = prepare_insert_data(data, extract_time)
    columns = get_column_names()
    conflict_columns = ["device_id", "upd_dt"]  # Primary Key ê¸°ì¤€ìœ¼ë¡œ upsert
    
    pg.insert_data(SCHEMA_NAME, TABLE_NAME, insert_data, columns, conflict_columns)
    logging.info(f"âœ… {len(data)} rows upserted (device_id, upd_dt ê¸°ì¤€).")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Variable Management
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def update_variable(end_extract_time: str) -> None:
    """Update Airflow variable with last extract time (always 23:59:59)"""
    # end_extract_timeì„ datetimeìœ¼ë¡œ íŒŒì‹±í•˜ì—¬ 23:59:59ë¡œ ì„¤ì •
    parsed_time = parse_datetime(end_extract_time)
    formatted_time = parsed_time.replace(hour=23, minute=59, second=59, microsecond=999999).strftime("%Y-%m-%d %H:%M:%S")
    
    Variable.set(INCREMENT_KEY, formatted_time)
    logging.info(f"ğŸ“Œ Variable `{INCREMENT_KEY}` Update: {formatted_time}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Incremental Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def daily_incremental_collection_task(**kwargs) -> dict:
    """ë§¤ì¼ ìµœì‹  ë°ì´í„°ë§Œ ìˆ˜ì§‘í•˜ëŠ” íƒœìŠ¤í¬"""
    # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë‚  ë°ì´í„° ìˆ˜ì§‘
    last_extract_time_str = Variable.get(INCREMENT_KEY, default_var=None)
    
    if last_extract_time_str:
        # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì„ íŒŒì‹±
        last_extract_time = parse_datetime(last_extract_time_str)
        
        # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì˜ ë‹¤ìŒ ë‚  00:00:00ë¶€í„° 23:59:59ê¹Œì§€
        start_date = last_extract_time.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
        end_date = start_date.replace(hour=23, minute=59, second=59, microsecond=999999)
        
        logging.info(f"ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„: {last_extract_time_str}")
        logging.info(f"ë‹¤ìŒ ë‚  ë°ì´í„° ìˆ˜ì§‘: {start_date.strftime('%Y-%m-%d')}")
    else:
        # Variableì´ ì—†ìœ¼ë©´ ì–´ì œ ë°ì´í„° ìˆ˜ì§‘
        yesterday = datetime.now(INDO_TZ) - timedelta(days=1)
        start_date = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = yesterday.replace(hour=23, minute=59, second=59, microsecond=999999)
        logging.info(f"Variableì´ ì—†ì–´ì„œ ì–´ì œ ë°ì´í„° ìˆ˜ì§‘: {yesterday.strftime('%Y-%m-%d')}")
    
    start_str = start_date.strftime("%Y-%m-%d %H:%M:%S")
    end_str = end_date.strftime("%Y-%m-%d %H:%M:%S")
    
    logging.info(f"ğŸ“… ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {start_str} ~ {end_str}")
    logging.info(f"ğŸ“Š ì²˜ë¦¬ ë‚ ì§œ: {start_date.strftime('%Y-%m-%d')}")
    
    try:
        source_pg = PostgresHelper(conn_id=SOURCE_POSTGRES_CONN_ID)
        target_pg = PostgresHelper(conn_id=TARGET_POSTGRES_CONN_ID)
        
        # ë°ì´í„° ì¶”ì¶œ ë° ì ì¬
        data, row_count = extract_data(source_pg, start_str, end_str)
        
        if row_count > 0:
            extract_time = datetime.utcnow()
            load_data(target_pg, data, extract_time)
            logging.info(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {row_count} rows")
            
            # Variable ì—…ë°ì´íŠ¸
            update_variable(end_str)
            
            return {
                "status": "daily_incremental_completed",
                "date": start_date.strftime("%Y-%m-%d"),
                "rows_processed": row_count,
                "start_time": start_str,
                "end_time": end_str,
                "extract_time": extract_time.isoformat()
            }
        else:
            logging.info(f"âš ï¸ ìˆ˜ì§‘í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {start_str} ~ {end_str}")
            
            # Variable ì—…ë°ì´íŠ¸ (ë°ì´í„°ê°€ ì—†ì–´ë„ ì‹œê°„ì€ ì—…ë°ì´íŠ¸)
            update_variable(end_str)
            
            return {
                "status": "daily_incremental_completed_no_data",
                "date": start_date.strftime("%Y-%m-%d"),
                "rows_processed": 0,
                "start_time": start_str,
                "end_time": end_str,
                "message": "ìˆ˜ì§‘í•  ë°ì´í„°ê°€ ì—†ìŒ"
            }
    except Exception as e:
        # ì—°ê²° ì‹¤íŒ¨ ë“± ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ì¸ ê²½ìš° Skip ì²˜ë¦¬
        error_str = str(e)
        error_lower = error_str.lower()
        is_connection_error = (
            "connection" in error_lower or
            "timeout" in error_lower or
            "timed out" in error_lower or
            "connection reset" in error_lower or
            "reset by peer" in error_lower or
            "errno" in error_lower or
            "network" in error_lower or
            "could not connect" in error_lower or
            "unable to connect" in error_lower
        )
        
        if is_connection_error:
            logging.warning(f"âš ï¸ ì—°ê²° ì‹¤íŒ¨: {error_str} - íƒœìŠ¤í¬ Skip")
            # Skip ì „ì— Variable ì—…ë°ì´íŠ¸ (ì—°ê²° ì‹¤íŒ¨í•´ë„ ì‹œê°„ì€ ì—…ë°ì´íŠ¸í•˜ì—¬ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì˜¬ë°”ë¥¸ ì‹œì ë¶€í„° ì¬ì‹œë„)
            try:
                update_variable(end_str)
                logging.info(f"âœ… Variable '{INCREMENT_KEY}' ì—…ë°ì´íŠ¸ (ì—°ê²° ì‹¤íŒ¨ë¡œ Skip): {end_str}")
            except Exception as var_err:
                logging.warning(f"âš ï¸ Variable ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ë¬´ì‹œ): {var_err}")
            
            skip_msg = (
                f"â­ï¸ CTM Chiller Status ETL ì¤‘ ì—°ê²° ë¶ˆê°€ - íƒœìŠ¤í¬ Skip\n"
                f"ì›ì¸: {error_str}\n"
                f"ì„¤ëª…: ì†ŒìŠ¤ ë˜ëŠ” íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\n"
                f"      Variableì€ ì—…ë°ì´íŠ¸ë˜ì—ˆìœ¼ë¯€ë¡œ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì¬ì‹œë„ë©ë‹ˆë‹¤."
            )
            logging.warning(skip_msg)
            raise AirflowSkipException(skip_msg) from e
        
        # ê·¸ ì™¸ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ raise
        logging.error(f"âŒ CTM Chiller Status ETL ì‹¤íŒ¨: {e}")
        raise


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Backfill Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_weekly_batch(
    source_pg: PostgresHelper, 
    target_pg: PostgresHelper, 
    start_date: datetime, 
    end_date: datetime,
    loop_count: int,
    expected_loops: int
) -> dict:
    """Process a single weekly batch"""
    logging.info(f"ğŸ”„ ì£¼ë³„ ë£¨í”„ {loop_count}/{expected_loops} ì‹œì‘")
    
    week_start_str = start_date.strftime("%Y-%m-%d %H:%M:%S")
    week_end_str = end_date.strftime("%Y-%m-%d %H:%M:%S")
    
    logging.info(f"ì£¼ë³„ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {week_start_str} ~ {week_end_str}")
    
    # ì£¼ë³„ ë°ì´í„° ì²˜ë¦¬
    data, row_count = extract_data(source_pg, week_start_str, week_end_str)
    
    if row_count > 0:
        extract_time = datetime.utcnow()
        load_data(target_pg, data, extract_time)
        logging.info(f"âœ… ì£¼ë³„ ë°°ì¹˜ ì™„ë£Œ: {week_start_str} ~ {week_end_str} ({row_count} rows)")
    else:
        logging.info(f"ì£¼ë³„ ë°°ì¹˜ì— ë°ì´í„° ì—†ìŒ: {week_start_str} ~ {week_end_str}")
    
    # ì£¼ë³„ ë°°ì¹˜ ì™„ë£Œ í›„ Variable ì—…ë°ì´íŠ¸
    update_variable(week_end_str)
    
    return {
        "loop": loop_count,
        "start": week_start_str,
        "end": week_end_str,
        "row_count": row_count,
        "batch_size_days": (end_date - start_date).days,
        "week": start_date.strftime("%Y-W%U")
    }


def backfill_weekly_batch_task(**kwargs) -> dict:
    """Main backfill task for weekly batch processing"""
    source_pg = PostgresHelper(conn_id=SOURCE_POSTGRES_CONN_ID)
    target_pg = PostgresHelper(conn_id=TARGET_POSTGRES_CONN_ID)
    
    # Get start date from variable or use initial date
    last_extract_time = Variable.get(INCREMENT_KEY, default_var=None)
    if not last_extract_time:
        start_date = INITIAL_START_DATE
        logging.info(f"ì´ˆê¸° ì‹œì‘ ë‚ ì§œ ì‚¬ìš©: {start_date}")
    else:
        start_date = parse_datetime(last_extract_time)
        logging.info(f"ì´ì „ ì§„í–‰ ì§€ì  ì‚¬ìš©: {start_date}")
    
    # Set timezone and calculate end date
    if start_date.tzinfo is None:
        start_date = start_date.replace(tzinfo=INDO_TZ)
    
    # Backfillë„ incrementalê³¼ ë™ì¼í•˜ê²Œ í˜„ì¬ ì‹œê°„ì—ì„œ 2ì¼ ì „ê¹Œì§€ë§Œ ì²˜ë¦¬
    end_date = datetime.now(INDO_TZ).replace(
        hour=0, minute=0, second=0, microsecond=0
    ) - timedelta(days=DAYS_OFFSET_FOR_INCREMENTAL)
    end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=999999)
    
    # Calculate expected loops
    expected_loops = calculate_expected_weekly_loops(start_date, end_date)
    
    # Log backfill information
    logging.info(f"Backfill ì‹œì‘: {start_date} ~ {end_date}")
    logging.info(f"ë°°ì¹˜ í¬ê¸°: ì£¼ë³„ (7ì¼ ë‹¨ìœ„)")
    logging.info(f"ì˜ˆìƒ ë£¨í”„ íšŸìˆ˜: {expected_loops}íšŒ (ì£¼ë³„)")
    logging.info(f"âš ï¸ í˜„ì¬ ì‹œê°„ì—ì„œ {DAYS_OFFSET_FOR_INCREMENTAL}ì¼ ì „ìœ¼ë¡œ ì„¤ì • (incremental DAG ì‹œì‘ì )")
    
    # Process weekly batches
    results = []
    total_processed = 0
    loop_count = 0
    current_date = start_date
    
    while current_date < end_date:
        loop_count += 1
        
        # ì£¼ ì‹œì‘ì¼ì„ 00:00:00ìœ¼ë¡œ ì„¤ì •
        week_start = current_date.replace(hour=0, minute=0, second=0, microsecond=0)
        
        # Calculate week end date
        week_end = get_week_end_date(current_date)
        if week_end > end_date:
            week_end = end_date
        
        # Process batch
        batch_result = process_weekly_batch(
            source_pg, target_pg, week_start, week_end, loop_count, expected_loops
        )
        
        results.append(batch_result)
        total_processed += batch_result["row_count"]
        
        # Move to next week
        current_date = week_end + timedelta(days=1)
    
    # Log completion
    logging.info(f"ğŸ‰ Backfill ì™„ë£Œ! ì´ {loop_count}íšŒ ë£¨í”„, {total_processed}ê°œ rows ìˆ˜ì§‘")
    if results:
        logging.info(f"ì²˜ë¦¬ ê¸°ê°„: {results[0]['start']} ~ {results[-1]['end']}")
    
    return {
        "status": "backfill_completed",
        "total_loops": loop_count,
        "total_batches": len(results),
        "total_rows": total_processed,
        "results": results
    }

