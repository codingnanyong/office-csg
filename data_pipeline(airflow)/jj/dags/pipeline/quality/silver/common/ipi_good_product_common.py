"""
IP Good Product Common Functions
=================================
ê³µí†µ í•¨ìˆ˜ ë° ì„¤ì •ì„ ëª¨ì•„ë‘” ëª¨ë“ˆ
"""

import logging
import pandas as pd
from datetime import datetime, timedelta
from airflow.models import Variable
from plugins.hooks.postgres_hook import PostgresHelper

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Database Configuration
PRODUCTION_POSTGRES_CONN_ID = "pg_jj_production_dw"  # MC, RST í…Œì´ë¸”ìš©
QUALITY_POSTGRES_CONN_ID = "pg_jj_quality_dw"        # OSND í…Œì´ë¸”, Target í…Œì´ë¸”ìš©

# Source Tables
SOURCE_SCHEMA = "bronze"
MC_TABLE = "ipi_mc_output_v2_raw"      # pg_jj_production_dw
RST_TABLE = "smp_ss_ipi_rst_raw"        # pg_jj_production_dw
OSND_TABLE = "mspq_in_osnd_bt_raw"     # pg_jj_quality_dw

# Target Table
TARGET_SCHEMA = "silver"
TARGET_TABLE = "ipi_good_product"       # pg_jj_quality_dw

# Incremental Configuration
INCREMENT_KEY = "ipi_good_product_last_date"
INITIAL_START_DATE = datetime(2025, 7, 1)
DAYS_OFFSET_FOR_INCREMENTAL = 2  # -2ì¼ ì „ê¹Œì§€ (incremental DAG ì‹œì‘ì )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Extraction
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_mc_data(pg_prod: PostgresHelper, start_time: str, end_time: str) -> pd.DataFrame:
    """MC í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (pg_jj_production_dw)"""
    logging.info(f"1ï¸âƒ£ MC í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_time} ~ {end_time}")
    
    # MC í…Œì´ë¸”ì˜ rst_ymdëŠ” VARCHAR(30)ì´ê³  '2025-07-16 00:00:00.418010' í˜•ì‹
    # ì›ë˜ ì¡°ê±´: rst_ymd BETWEEN :start_time AND :end_time
    # DATE(CAST(rst_ymd AS TIMESTAMP))ë¡œ ë‚ ì§œë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ
    sql = f"""
        SELECT 
            mc_cd, st_num, st_side, act_qty, rst_ymd, upd_so_id
        FROM {SOURCE_SCHEMA}.{MC_TABLE}
        WHERE DATE(CAST(rst_ymd AS TIMESTAMP)) BETWEEN DATE('{start_time}') AND DATE('{end_time}')
    """
    
    data = pg_prod.execute_query(sql, task_id="extract_mc_data", xcom_key=None)
    
    if data:
        # tuple ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(data, columns=['mc_cd', 'st_num', 'st_side', 'act_qty', 'rst_ymd', 'upd_so_id'])
        df.columns = df.columns.str.lower()
    else:
        df = pd.DataFrame(columns=['mc_cd', 'st_num', 'st_side', 'act_qty', 'rst_ymd', 'upd_so_id'])
    
    logging.info(f"âœ… MC í…Œì´ë¸” ì¶”ì¶œ ì™„ë£Œ: {len(df):,} rows")
    return df


def extract_rst_data(pg_prod: PostgresHelper, start_time: str, end_time: str) -> pd.DataFrame:
    """ì‹¤ì  í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (pg_jj_production_dw)"""
    logging.info(f"2ï¸âƒ£ ì‹¤ì  í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_time} ~ {end_time}")
    
    # ì»¬ëŸ¼ëª… ì¡°íšŒë¥¼ ìœ„í•œ ì¿¼ë¦¬
    col_sql = f"""
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_schema = '{SOURCE_SCHEMA}' 
          AND table_name = '{RST_TABLE}'
        ORDER BY ordinal_position
    """
    columns = pg_prod.execute_query(col_sql, task_id="get_rst_columns", xcom_key=None)
    column_names = [col[0].lower() for col in columns] if columns else []
    
    # RST í…Œì´ë¸”ì˜ start_date, end_dateëŠ” TIMESTAMP íƒ€ì…
    # ì›ë˜ ì¡°ê±´: start_date BETWEEN :start_time AND :end_time OR end_date BETWEEN :start_time AND :end_time
    sql = f"""
        SELECT *
        FROM {SOURCE_SCHEMA}.{RST_TABLE}
        WHERE start_date BETWEEN TIMESTAMP '{start_time} 00:00:00' AND TIMESTAMP '{end_time} 23:59:59'
           OR end_date BETWEEN TIMESTAMP '{start_time} 00:00:00' AND TIMESTAMP '{end_time} 23:59:59'
    """
    
    data = pg_prod.execute_query(sql, task_id="extract_rst_data", xcom_key=None)
    
    if data:
        df = pd.DataFrame(data, columns=column_names)
    else:
        df = pd.DataFrame(columns=column_names)
    
    logging.info(f"âœ… ì‹¤ì  í…Œì´ë¸” ì¶”ì¶œ ì™„ë£Œ: {len(df):,} rows")
    return df


def extract_osnd_data(pg_quality: PostgresHelper, start_time: str, end_time: str) -> pd.DataFrame:
    """OSND í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (pg_jj_quality_dw)"""
    logging.info(f"3ï¸âƒ£ OSND í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_time} ~ {end_time}")
    
    # ì»¬ëŸ¼ëª… ì¡°íšŒë¥¼ ìœ„í•œ ì¿¼ë¦¬
    col_sql = f"""
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_schema = '{SOURCE_SCHEMA}' 
          AND table_name = '{OSND_TABLE}'
        ORDER BY ordinal_position
    """
    columns = pg_quality.execute_query(col_sql, task_id="get_osnd_columns", xcom_key=None)
    column_names = [col[0].lower() for col in columns] if columns else []
    
    # OSND í…Œì´ë¸”ì˜ osnd_dtëŠ” TIMESTAMP íƒ€ì…
    # ì›ë˜ ì¡°ê±´: osnd_dt BETWEEN :start_time AND :end_time
    sql = f"""
        SELECT *
        FROM {SOURCE_SCHEMA}.{OSND_TABLE}
        WHERE osnd_dt BETWEEN TIMESTAMP '{start_time} 00:00:00' AND TIMESTAMP '{end_time} 23:59:59'
    """
    
    data = pg_quality.execute_query(sql, task_id="extract_osnd_data", xcom_key=None)
    
    if data:
        df = pd.DataFrame(data, columns=column_names)
    else:
        df = pd.DataFrame(columns=column_names)
    
    logging.info(f"âœ… OSND í…Œì´ë¸” ì¶”ì¶œ ì™„ë£Œ: {len(df):,} rows")
    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Transformation
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def transform_and_join_data(mc_df: pd.DataFrame, rst_df: pd.DataFrame) -> pd.DataFrame:
    """ë°ì´í„° ë³€í™˜ ë° ì¡°ì¸"""
    logging.info("4ï¸âƒ£ ë°ì´í„° ë³€í™˜ ë° ì¡°ì¸ ì¤‘...")
    
    # ë‚ ì§œ í˜•ì‹ ë³€í™˜
    mc_df['rst_ymd'] = pd.to_datetime(mc_df['rst_ymd'], errors='coerce').dt.round('s')
    
    # ìë£Œí˜• ì •ë¦¬: ì¡°ì¸ì„ ìœ„í•œ í‚¤ê°’ ì •ìˆ˜í˜• ë³€í™˜
    mc_df['upd_so_id'] = pd.to_numeric(mc_df['upd_so_id'], errors='coerce')
    rst_df['so_id'] = pd.to_numeric(rst_df['so_id'], errors='coerce')
    
    # í•„ìš”í•œ ì—´ë§Œ ì¶”ì¶œí•˜ì—¬ ì¡°ì¸ìš© ë°ì´í„°í”„ë ˆì„ êµ¬ì„±
    good_product_join_cols = rst_df[['so_id', 'mold_id', 'mold_bar_key']].copy()
    
    # ì¡°ì¸ ìˆ˜í–‰ (left join)
    merged_df = mc_df.merge(
        good_product_join_cols, 
        how='left', 
        left_on='upd_so_id', 
        right_on='so_id'
    )
    
    # ì—´ ì´ë¦„ ë³€ê²½
    merged_df = merged_df.rename(columns={
        'mold_id': 'mold_cd',
        'mold_bar_key': 'mold_id'
    })
    
    logging.info(f"âœ… ì¡°ì¸ ì™„ë£Œ: {len(merged_df):,} rows")
    return merged_df


def remove_defective_products(
    merged_df: pd.DataFrame,
    osnd_df: pd.DataFrame,
    rst_df: pd.DataFrame
) -> tuple:
    """ë¶ˆëŸ‰ ì œí’ˆ ì œê±°"""
    logging.info("5ï¸âƒ£ ë¶ˆëŸ‰ ì œí’ˆ ì œê±° ì¤‘...")
    
    # ë‚ ì§œ í˜•ì‹ ë³€í™˜
    osnd_df['osnd_dt'] = pd.to_datetime(osnd_df['osnd_dt'], errors='coerce')
    rst_df['start_date'] = pd.to_datetime(rst_df['start_date'], errors='coerce')
    rst_df['end_date'] = pd.to_datetime(rst_df['end_date'], errors='coerce')
    
    # ë¶ˆëŸ‰ SO_ID ìˆ˜ì§‘
    bad_so_ids = []
    
    for _, row in osnd_df.iterrows():
        machine = row['machine_cd']
        mold_id = row['mold_id']
        osnd_time = row['osnd_dt']
        
        # machine_cd, mold_id ì¼ì¹˜í•˜ëŠ” rst rowë“¤
        candidates = rst_df[
            (rst_df['machine_cd'] == machine) &
            (rst_df['mold_bar_key'] == mold_id)
        ]
        
        # ì‹œê°„ ë²”ìœ„ ë‚´ì— í¬í•¨ë˜ëŠ” í–‰ ì°¾ê¸° (osnd_dt -3h ~ -1h)
        for _, rst_row in candidates.iterrows():
            for h in range(1, 4):  # 1~3 ì‹œê°„ ì°¨ì´ ìˆœíšŒ
                target_time = osnd_time - timedelta(hours=h)
                if rst_row['start_date'] <= target_time <= rst_row['end_date']:
                    bad_so_ids.append(rst_row['so_id'])
                    break  # ê°€ì¥ ì´ë¥¸ ì‹œê°„ë§Œ ê³ ë ¤
    
    # ì¤‘ë³µ ì œê±°
    bad_so_ids = set(bad_so_ids)
    logging.info(f"âœ… ë¶ˆëŸ‰ SO_ID ë°œê²¬: {len(bad_so_ids):,}ê°œ")
    
    # ì œê±° ì „ ê¸¸ì´
    original_len = len(merged_df)
    
    # mold_id ëˆ„ë½ëœ í–‰ ì œê±° ë° ë¶ˆëŸ‰ ì œí’ˆ ì œê±°
    merged_df = merged_df.dropna(subset=['mold_id'])
    clean_df = merged_df[~merged_df['upd_so_id'].isin(bad_so_ids)].copy()
    
    removed_count = original_len - len(clean_df)
    removed_ratio = removed_count / original_len * 100 if original_len > 0 else 0
    
    logging.info(f"âœ… ë¶ˆëŸ‰ ì œí’ˆ ì œê±° ì™„ë£Œ")
    logging.info(f"ğŸ“Š ì›ë³¸: {original_len:,} rows")
    logging.info(f"ğŸ“Š ì œê±°: {removed_count:,} rows ({removed_ratio:.2f}%)")
    logging.info(f"ğŸ“Š ìµœì¢…: {len(clean_df):,} rows")
    
    return clean_df, original_len, removed_count, removed_ratio


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Loading
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_to_silver(pg_quality: PostgresHelper, clean_df: pd.DataFrame) -> None:
    """Silver í…Œì´ë¸”ì— ë°ì´í„° ì ì¬"""
    logging.info("6ï¸âƒ£ Silver í…Œì´ë¸” ì ì¬ ì¤‘...")
    
    # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
    table_exists = pg_quality.check_table(TARGET_SCHEMA, TARGET_TABLE)
    
    if not table_exists:
        logging.warning(f"âš ï¸ í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {TARGET_SCHEMA}.{TARGET_TABLE}")
        logging.warning("âš ï¸ í…Œì´ë¸”ì„ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”: /home/user/apps/airflow/db/quality/silver/ip_good_product.sql")
    
    # etl_ingest_time ì¶”ê°€
    if 'etl_ingest_time' not in clean_df.columns:
        clean_df['etl_ingest_time'] = datetime.now()
    
    # ê²°ê³¼ì—ëŠ” upd_so_id ì œì™¸ (so_idë§Œ ìœ ì§€)
    if 'upd_so_id' in clean_df.columns:
        clean_df = clean_df.drop(columns=['upd_so_id'])

    # DataFrameì„ tuple ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    data_tuples = [tuple(row) for row in clean_df.values]
    columns = clean_df.columns.tolist()
    
    # ë°ì´í„° ì ì¬
    logging.info(f"ğŸ“¦ ë°ì´í„° ì ì¬ ì¤‘: {len(clean_df):,} rows")
    pg_quality.insert_data(
        schema_name=TARGET_SCHEMA,
        table_name=TARGET_TABLE,
        data=data_tuples,
        columns=columns,
        conflict_columns=['mc_cd', 'st_num', 'st_side', 'rst_ymd', 'so_id']  # Primary Key
    )
    
    logging.info(f"âœ… Silver í…Œì´ë¸” ì ì¬ ì™„ë£Œ: {TARGET_SCHEMA}.{TARGET_TABLE}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Variable Management
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def update_variable(end_date: str) -> None:
    """Update Airflow variable with last processed date"""
    Variable.set(INCREMENT_KEY, end_date)
    logging.info(f"ğŸ“Œ Variable `{INCREMENT_KEY}` Update: {end_date}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Incremental Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def incremental_ip_good_product(**context):
    """
    Incremental ì‘ì—…: ì „ì¼ ë°ì´í„° ì²˜ë¦¬
    """
    pg_prod = PostgresHelper(conn_id=PRODUCTION_POSTGRES_CONN_ID)
    pg_quality = PostgresHelper(conn_id=QUALITY_POSTGRES_CONN_ID)
    
    # ê³µìš© Variableì—ì„œ ë§ˆì§€ë§‰ ì²˜ë¦¬ì¼ì„ ì½ê³ , ê·¸ ë‹¤ìŒë‚ ì„ ì²˜ë¦¬ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •
    last_date_str = Variable.get(INCREMENT_KEY, default_var=None)
    # UTC ê¸°ì¤€ ë‚ ì§œ ê³„ì‚° (ë‚ ì§œ ë¹„êµë¥¼ ìœ„í•´ ì‹œê°„ ì œê±°)
    now_utc = datetime.utcnow()
    today_minus_1 = (now_utc - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)

    if last_date_str:
        try:
            last_date = datetime.strptime(last_date_str, '%Y-%m-%d')
            last_date = last_date.replace(hour=0, minute=0, second=0, microsecond=0)
        except Exception:
            # í˜•ì‹ ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ today-1ë¡œ ì¬ì„¤ì •
            last_date = today_minus_1 - timedelta(days=1)
        target_date = last_date + timedelta(days=1)
    else:
        # Variable ë¯¸ì„¤ì • ì‹œ today-1ì„ ì²˜ë¦¬
        target_date = today_minus_1

    # today-1ì„ ìƒí•œìœ¼ë¡œ ìº¡ (ê°™ê±°ë‚˜ ì‘ìœ¼ë©´ ì²˜ë¦¬)
    if target_date > today_minus_1:
        logging.info(f"âœ… ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. ì²˜ë¦¬í•  ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤ (Variable ê¸°ì¤€).")
        logging.info(f"   Variable last_date: {last_date_str}")
        logging.info(f"   target_date: {target_date.strftime('%Y-%m-%d')}")
        logging.info(f"   today_minus_1: {today_minus_1.strftime('%Y-%m-%d')}")
        return {"status": "up_to_date", "last_date": last_date_str or None}

    # ëŒ€ìƒ ì¼ì 00:00:00 ~ 23:59:59
    start_time = target_date.strftime('%Y-%m-%d')
    end_time = target_date.strftime('%Y-%m-%d')
    
    logging.info(f"\n{'='*60}")
    logging.info(f"ğŸš€ IP Good Product Incremental ì‹œì‘")
    logging.info(f"{'='*60}")
    logging.info(f"ğŸ“… ì²˜ë¦¬ ë‚ ì§œ: {start_time} (ì „ì¼ ë°ì´í„°)")
    
    try:
        # ë°ì´í„° ì¶”ì¶œ
        mc_df = extract_mc_data(pg_prod, start_time, end_time)
        rst_df = extract_rst_data(pg_prod, start_time, end_time)
        osnd_df = extract_osnd_data(pg_quality, start_time, end_time)
        
        # ë°ì´í„° ë³€í™˜ ë° ì¡°ì¸
        merged_df = transform_and_join_data(mc_df, rst_df)
        
        # ë¶ˆëŸ‰ ì œí’ˆ ì œê±°
        clean_df, original_len, removed_count, removed_ratio = remove_defective_products(
            merged_df, osnd_df, rst_df
        )
        
        # Silver í…Œì´ë¸” ì ì¬
        if len(clean_df) > 0:
            load_to_silver(pg_quality, clean_df)
            logging.info(f"âœ… Incremental ì™„ë£Œ: {start_time} ({len(clean_df):,} rows)")
        else:
            logging.info(f"âš ï¸ Incremental ë°ì´í„° ì—†ìŒ: {start_time}")
        
        # Variable ì—…ë°ì´íŠ¸
        update_variable(start_time)
        
        logging.info(f"\n{'='*60}")
        logging.info(f"âœ… Incremental ì™„ë£Œ")
        logging.info(f"{'='*60}")
        logging.info(f"ğŸ“… ì²˜ë¦¬ ë‚ ì§œ: {start_time}")
        logging.info(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {original_len:,} rows")
        logging.info(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {len(clean_df):,} rows")
        logging.info(f"ğŸ“Š ì œê±°ëœ ë°ì´í„°: {removed_count:,} rows ({removed_ratio:.2f}%)")
        logging.info(f"{'='*60}")
        
        return {
            "status": "success",
            "date": start_time,
            "original_rows": original_len,
            "final_rows": len(clean_df),
            "removed_rows": removed_count,
            "removed_ratio": removed_ratio
        }
        
    except Exception as e:
        logging.error(f"\n{'='*60}")
        logging.error(f"âŒ Incremental ì‹¤íŒ¨: {str(e)}")
        logging.error(f"{'='*60}")
        raise


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Backfill Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_daily_batch(
    pg_prod: PostgresHelper,
    pg_quality: PostgresHelper,
    start_date: datetime,
    loop_count: int,
    expected_days: int
) -> dict:
    """Process a single daily batch"""
    logging.info(f"ğŸ”„ ë£¨í”„ {loop_count}/{expected_days} ì‹œì‘")
    
    start_time = start_date.strftime("%Y-%m-%d")
    end_time = start_date.strftime("%Y-%m-%d")
    
    logging.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {start_time}")
    
    try:
        # ë°ì´í„° ì¶”ì¶œ
        mc_df = extract_mc_data(pg_prod, start_time, end_time)
        rst_df = extract_rst_data(pg_prod, start_time, end_time)
        osnd_df = extract_osnd_data(pg_quality, start_time, end_time)
        
        # ë°ì´í„° ë³€í™˜ ë° ì¡°ì¸
        merged_df = transform_and_join_data(mc_df, rst_df)
        
        # ë¶ˆëŸ‰ ì œí’ˆ ì œê±°
        clean_df, original_len, removed_count, removed_ratio = remove_defective_products(
            merged_df, osnd_df, rst_df
        )
        
        # Silver í…Œì´ë¸” ì ì¬
        if len(clean_df) > 0:
            load_to_silver(pg_quality, clean_df)
            logging.info(f"âœ… ë°°ì¹˜ ì™„ë£Œ: {start_time} ({len(clean_df)} rows)")
        else:
            logging.info(f"ë°°ì¹˜ì— ë°ì´í„° ì—†ìŒ: {start_time}")
        
        # Variable ì—…ë°ì´íŠ¸
        update_variable(start_time)
        
        return {
            "loop": loop_count,
            "date": start_time,
            "original_rows": original_len,
            "final_rows": len(clean_df),
            "removed_rows": removed_count,
            "status": "success"
        }
        
    except Exception as e:
        logging.error(f"âŒ ë°°ì¹˜ ì‹¤íŒ¨: {start_time} - {str(e)}")
        update_variable(start_time)  # ì‹¤íŒ¨í•´ë„ ë‚ ì§œëŠ” ì—…ë°ì´íŠ¸
        return {
            "loop": loop_count,
            "date": start_time,
            "status": "failed",
            "error": str(e)
        }


def backfill_daily_batch_task(**kwargs) -> dict:
    """Main backfill task for daily batch processing"""
    pg_prod = PostgresHelper(conn_id=PRODUCTION_POSTGRES_CONN_ID)
    pg_quality = PostgresHelper(conn_id=QUALITY_POSTGRES_CONN_ID)
    
    # Get start date from variable or use initial date
    last_date_str = Variable.get(INCREMENT_KEY, default_var=None)
    if not last_date_str:
        start_date = INITIAL_START_DATE
        logging.info(f"ì´ˆê¸° ì‹œì‘ ë‚ ì§œ ì‚¬ìš©: {start_date}")
    else:
        start_date = datetime.strptime(last_date_str, '%Y-%m-%d')
        logging.info(f"ì´ì „ ì§„í–‰ ì§€ì  ì‚¬ìš©: {start_date}")
    
    # Calculate end date (today - 2 days)
    end_date = (datetime.now() - timedelta(days=DAYS_OFFSET_FOR_INCREMENTAL)).replace(
        hour=0, minute=0, second=0, microsecond=0
    )
    
    # Calculate expected days
    expected_days = (end_date - start_date).days
    
    # Log backfill information
    logging.info(f"Backfill ì‹œì‘: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    logging.info(f"ë°°ì¹˜ í¬ê¸°: ì¼ë³„ (í•˜ë£¨ì”© ì²˜ë¦¬)")
    logging.info(f"ì˜ˆìƒ ë£¨í”„ íšŸìˆ˜: {expected_days}íšŒ (ì¼ë³„)")
    logging.info(f"âš ï¸ í˜„ì¬ ì‹œê°„ì—ì„œ {DAYS_OFFSET_FOR_INCREMENTAL}ì¼ ì „ìœ¼ë¡œ ì„¤ì • (incremental DAG ì‹œì‘ì )")
    
    # Process daily batches
    results = []
    total_original_rows = 0
    total_final_rows = 0
    total_removed_rows = 0
    loop_count = 0
    current_date = start_date
    
    while current_date < end_date:
        loop_count += 1
        
        # Process batch
        batch_result = process_daily_batch(
            pg_prod, pg_quality, current_date, loop_count, expected_days
        )
        
        results.append(batch_result)
        
        if batch_result.get("status") == "success":
            total_original_rows += batch_result.get("original_rows", 0)
            total_final_rows += batch_result.get("final_rows", 0)
            total_removed_rows += batch_result.get("removed_rows", 0)
        
        # Move to next day
        current_date += timedelta(days=1)
    
    # Log completion
    logging.info(f"ğŸ‰ Backfill ì™„ë£Œ! ì´ {loop_count}íšŒ ë£¨í”„, {total_final_rows}ê°œ rows ìˆ˜ì§‘")
    if results:
        logging.info(f"ì²˜ë¦¬ ê¸°ê°„: {results[0]['date']} ~ {results[-1]['date']}")
        logging.info(f"ğŸ“Š ì´ ì›ë³¸ ë°ì´í„°: {total_original_rows:,} rows")
        logging.info(f"ğŸ“Š ì´ ìµœì¢… ë°ì´í„°: {total_final_rows:,} rows")
        logging.info(f"ğŸ“Š ì´ ì œê±°ëœ ë°ì´í„°: {total_removed_rows:,} rows")
    
    return {
        "status": "backfill_completed",
        "total_loops": loop_count,
        "total_days": len(results),
        "total_original_rows": total_original_rows,
        "total_final_rows": total_final_rows,
        "total_removed_rows": total_removed_rows,
        "results": results
    }