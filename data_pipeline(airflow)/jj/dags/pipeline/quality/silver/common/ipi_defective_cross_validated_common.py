"""
IPI Defective Cross Validated Common Functions
===============================================
ê³µí†µ í•¨ìˆ˜ ë° ì„¤ì •ì„ ëª¨ì•„ë‘” ëª¨ë“ˆ
"""

import logging
import pandas as pd
from datetime import datetime, timedelta
from airflow.models import Variable
from plugins.hooks.postgres_hook import PostgresHelper

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Database Configuration
PRODUCTION_POSTGRES_CONN_ID = "pg_jj_production_dw"  # MMS í…Œì´ë¸”ìš©
QUALITY_POSTGRES_CONN_ID = "pg_jj_quality_dw"        # OSND, IPI_DMS, Target í…Œì´ë¸”ìš©

# Source Tables
MMS_SOURCE_SCHEMA = "silver"
MMS_SOURCE_TABLE = "ip_mold_mc_inout"  # pg_jj_production_dw

OSND_SOURCE_SCHEMA = "bronze"
OSND_SOURCE_TABLE = "mspq_in_osnd_bt_raw"  # pg_jj_quality_dw

IPI_DMS_SOURCE_SCHEMA = "silver"
IPI_DMS_SOURCE_TABLE = "ipi_defective_time_corrected"  # pg_jj_quality_dw

# Target Table
TARGET_SCHEMA = "silver"
TARGET_TABLE = "ipi_defective_cross_validated"  # pg_jj_quality_dw

# Incremental Configuration
INCREMENT_KEY = "ipi_defective_cross_validated_last_date"  # backfillê³¼ ê³µìš©
INITIAL_START_DATE = datetime(2025, 7, 1)
DAYS_OFFSET_FOR_INCREMENTAL = 2  # -2ì¼ ì „ê¹Œì§€ (incremental DAG ì‹œì‘ì )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Extraction
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_mms_data(pg_prod: PostgresHelper, start_time: str, end_time: str) -> pd.DataFrame:
    """MMS ë°ì´í„° ì¶”ì¶œ (silver.ip_mold_mc_inout)"""
    logging.info(f"1ï¸âƒ£ MMS í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_time} ~ {end_time}")
    
    # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (YYYY-MM-DD â†’ YYYYMMDD)
    start_date_formatted = start_time.replace('-', '')
    end_date_formatted = end_time.replace('-', '')
    
    # mold_input_date ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
    sql = f"""
        SELECT 
            workshop,
            machine,
            mold_id,
            mold_remove_date,
            mold_remove_time,
            mold_input_date,
            mold_input_time
        FROM {MMS_SOURCE_SCHEMA}.{MMS_SOURCE_TABLE}
        WHERE 
            workshop = 'IP'
            AND (
                (mold_input_date >= '{start_date_formatted}' AND mold_input_date <= '{end_date_formatted}')
                OR mold_remove_date IS NULL
            )
        ORDER BY mold_id, mold_input_date, mold_input_time
    """
    
    data = pg_prod.execute_query(sql, task_id="extract_mms_data", xcom_key=None)
    
    if data:
        # tuple ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(data, columns=[
            'workshop', 'machine', 'mold_id',
            'mold_remove_date', 'mold_remove_time',
            'mold_input_date', 'mold_input_time'
        ])
        df.columns = df.columns.str.lower()
    else:
        df = pd.DataFrame(columns=[
            'workshop', 'machine', 'mold_id',
            'mold_remove_date', 'mold_remove_time',
            'mold_input_date', 'mold_input_time'
        ])
    
    logging.info(f"âœ… MMS í…Œì´ë¸” ì¶”ì¶œ ì™„ë£Œ: {len(df):,} rows")
    return df


def extract_osnd_data(pg_quality: PostgresHelper, start_time: str, end_time: str) -> pd.DataFrame:
    """OSND ë°ì´í„° ì¶”ì¶œ (bronze.mspq_in_osnd_bt_raw)"""
    logging.info(f"2ï¸âƒ£ OSND í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_time} ~ {end_time}")
    
    # OSND í…Œì´ë¸”ì˜ osnd_dtëŠ” TIMESTAMP íƒ€ì…
    sql = f"""
        SELECT *
        FROM {OSND_SOURCE_SCHEMA}.{OSND_SOURCE_TABLE}
        WHERE osnd_dt BETWEEN TIMESTAMP '{start_time} 00:00:00' AND TIMESTAMP '{end_time} 23:59:59'
    """
    
    # ì»¬ëŸ¼ëª… ì¡°íšŒë¥¼ ìœ„í•œ ì¿¼ë¦¬
    col_sql = f"""
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_schema = '{OSND_SOURCE_SCHEMA}' 
          AND table_name = '{OSND_SOURCE_TABLE}'
        ORDER BY ordinal_position
    """
    columns = pg_quality.execute_query(col_sql, task_id="get_osnd_columns", xcom_key=None)
    column_names = [col[0].lower() for col in columns] if columns else []
    
    data = pg_quality.execute_query(sql, task_id="extract_osnd_data", xcom_key=None)
    
    if data:
        df = pd.DataFrame(data, columns=column_names)
    else:
        df = pd.DataFrame(columns=column_names)
    
    logging.info(f"âœ… OSND í…Œì´ë¸” ì¶”ì¶œ ì™„ë£Œ: {len(df):,} rows")
    return df


def extract_ipi_dms_data(pg_quality: PostgresHelper, start_time: str, end_time: str) -> pd.DataFrame:
    """IPI_DMS ë°ì´í„° ì¶”ì¶œ (silver.ipi_defective_time_corrected)"""
    logging.info(f"3ï¸âƒ£ IPI_DMS í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_time} ~ {end_time}")
    
    # IPI_DMS í…Œì´ë¸”ì˜ osnd_dtëŠ” TIMESTAMP íƒ€ì…
    sql = f"""
        SELECT *
        FROM {IPI_DMS_SOURCE_SCHEMA}.{IPI_DMS_SOURCE_TABLE}
        WHERE osnd_dt BETWEEN TIMESTAMP '{start_time} 00:00:00' AND TIMESTAMP '{end_time} 23:59:59'
    """
    
    # ì»¬ëŸ¼ëª… ì¡°íšŒë¥¼ ìœ„í•œ ì¿¼ë¦¬
    col_sql = f"""
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_schema = '{IPI_DMS_SOURCE_SCHEMA}' 
          AND table_name = '{IPI_DMS_SOURCE_TABLE}'
        ORDER BY ordinal_position
    """
    columns = pg_quality.execute_query(col_sql, task_id="get_ipi_dms_columns", xcom_key=None)
    column_names = [col[0].lower() for col in columns] if columns else []
    
    data = pg_quality.execute_query(sql, task_id="extract_ipi_dms_data", xcom_key=None)
    
    if data:
        df = pd.DataFrame(data, columns=column_names)
    else:
        df = pd.DataFrame(columns=column_names)
    
    logging.info(f"âœ… IPI_DMS í…Œì´ë¸” ì¶”ì¶œ ì™„ë£Œ: {len(df):,} rows")
    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MMS Data Correction Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def correct_mms_overlaps(df: pd.DataFrame) -> pd.DataFrame:
    """
    MMS ë°ì´í„° ê²¹ì¹¨ ë³´ì • ë¡œì§
    - ê°™ì€ mold_id ë‚´ì—ì„œ finish > next_startì¸ ê²½ìš° ê²¹ì¹¨ìœ¼ë¡œ íŒë‹¨
    - ê²¹ì¹¨ ì‹œ ì• êµ¬ê°„ì˜ finishë¥¼ next_startë¡œ ì¡°ì •
    - ìµœëŒ€ 5íšŒ ë°˜ë³µ
    """
    logging.info("2ï¸âƒ£ MMS ë°ì´í„° ë³´ì • ë° Station ë§¤ì¹­ ì‘ì—… ì‹œì‘")
    
    # ê³µë°± ë¬¸ìì—´ì„ None ë˜ëŠ” NaTë¡œ ë³€í™˜
    df['mold_remove_date'] = df['mold_remove_date'].replace(' ', pd.NA)
    df['mold_remove_time'] = df['mold_remove_time'].replace(' ', pd.NA)
    
    # Mold Remove Dateì™€ Mold Remove Timeì´ ë¹„ì–´ ìˆëŠ” ê²½ìš° í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ìœ¼ë¡œ ì±„ìš°ê¸°
    current_date = datetime.now().strftime('%Y%m%d')  # 'YYYYMMDD' í˜•ì‹ì˜ ë¬¸ìì—´
    current_time = datetime.now().strftime('%H:%M')   # 'HH:MM' í˜•ì‹ì˜ ë¬¸ìì—´
    
    df['mold_remove_date'] = df['mold_remove_date'].fillna(current_date).astype(str)
    df['mold_remove_time'] = df['mold_remove_time'].fillna(current_time)
    
    # ë‚ ì§œ í˜•ì‹ ì •ê·œí™” (ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ datetime ë³€í™˜)
    df['mold_remove_date'] = df['mold_remove_date'].astype(str)
    df['mold_input_date'] = df['mold_input_date'].astype(str)
    df['mold_input_time'] = df['mold_input_time'].astype(str)
    df['mold_remove_time'] = df['mold_remove_time'].astype(str)
    
    # ë‚ ì§œì™€ ì‹œê°„ì„ í•©ì³ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    df['start'] = pd.to_datetime(df['mold_input_date'] + ' ' + df['mold_input_time'], errors='coerce')
    df['finish'] = pd.to_datetime(df['mold_remove_date'] + ' ' + df['mold_remove_time'], errors='coerce')
    
    # ìƒˆë¡œ ì •ë ¬í•œ df ìƒì„±
    df_sorted = df.sort_values(by=['mold_id', 'start']).reset_index(drop=True)
    
    # ê²¹ì¹¨ ë³´ì • ë°˜ë³µ ì²˜ë¦¬
    iter_cnt = 0
    max_iters = 5
    
    while True:
        iter_cnt += 1
        overlaps = []  # ë§¤ ë°˜ë³µë§ˆë‹¤ ë°˜ë“œì‹œ ì´ˆê¸°í™”
        
        # í•œ í„´ ë³´ì •
        for i in range(len(df_sorted) - 1):
            same_mold = df_sorted.iloc[i]['mold_id'] == df_sorted.iloc[i + 1]['mold_id']
            
            if not same_mold:
                continue
            
            current_finish = df_sorted.iloc[i]['finish']
            next_start = df_sorted.iloc[i + 1]['start']
            
            # NaN ì²´í¬
            if pd.isna(current_finish) or pd.isna(next_start):
                continue
            
            # ê²¹ì¹¨ íŒë‹¨
            if current_finish > next_start:
                # ë¡œê·¸ìš© ê¸°ë¡
                overlaps.append({
                    'mold_id': df_sorted.iloc[i]['mold_id'],
                    'Overlapping Entries': f"i finish {current_finish} & i+1 start {next_start}",
                    'Overlapping Time': current_finish - next_start
                })
                
                # ë³´ì •: ì• êµ¬ê°„ì˜ Finishë¥¼ ë‹¤ìŒ Startë¡œ ì ˆë‹¨
                df_sorted.at[i, 'finish'] = next_start
        
        # ì´ë²ˆ í„´ ê²°ê³¼ ì¶œë ¥
        logging.info(f"[Iteration {iter_cnt}] overlaps found: {len(overlaps)}")
        
        # ê²¹ì¹¨ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if len(overlaps) == 0:
            break
        
        # ì•ˆì „ì¥ì¹˜
        if iter_cnt >= max_iters:
            logging.warning("âš ï¸ ë¬´í•œ ë£¨í”„, ë°ì´í„° í™•ì¸ í•„ìš”")
            break
    
    # ë§ˆì§€ë§‰ ë°˜ë³µì˜ ê²¹ì¹¨(ì—†ì–´ì•¼ ì •ìƒ) ìš”ì•½ ì¶œë ¥
    if overlaps:
        overlap_df = pd.DataFrame(overlaps)
        logging.warning(f"âš ï¸ ìµœì¢… ê²¹ì¹¨ ë°ì´í„°: {len(overlaps)}ê±´")
        logging.debug(overlap_df.head(10))
    else:
        logging.info("âœ… ë³´ì •ì‘ì—… ì™„ë£Œ!")
    
    logging.info(f"âœ… MMS ë°ì´í„° ë³´ì • ì™„ë£Œ: {len(df_sorted):,} rows")
    return df_sorted


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OSND Station Matching Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_station_from_machine(machine_str: str) -> pd.Series:
    """Machine ë¬¸ìì—´ì—ì„œ Station ì •ë³´ ì¶”ì¶œ"""
    try:
        if pd.isna(machine_str) or machine_str is None:
            return pd.Series([None, None])
        parts = str(machine_str).split('-')
        if len(parts) >= 2:
            return pd.Series([parts[-2], parts[-1]])
        else:
            return pd.Series([None, None])
    except Exception:
        return pd.Series([None, None])


def match_osnd_with_station(mms_df: pd.DataFrame, osnd_df: pd.DataFrame) -> pd.DataFrame:
    """
    OSND ë°ì´í„°ì™€ MMS ë°ì´í„°ë¥¼ ë§¤ì¹­í•˜ì—¬ Station ì •ë³´ ì¶”ê°€
    - MMS ë°ì´í„°ì—ì„œ Station ì •ë³´ ì¶”ì¶œ (machine ì»¬ëŸ¼ íŒŒì‹±)
    - OSNDì˜ mold_idì™€ osnd_dtë¥¼ ê¸°ì¤€ìœ¼ë¡œ MMSì˜ start/finish ë²”ìœ„ì™€ ë§¤ì¹­
    """
    logging.info("3ï¸âƒ£ OSNDì™€ Station ë§¤ì¹­ ì‘ì—… ì‹œì‘")
    
    if len(osnd_df) == 0:
        logging.warning("âš ï¸ OSND ë°ì´í„°ê°€ ì—†ì–´ ë§¤ì¹­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return osnd_df
    
    if len(mms_df) == 0:
        logging.warning("âš ï¸ MMS ë°ì´í„°ê°€ ì—†ì–´ ë§¤ì¹­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        # Station ì»¬ëŸ¼ì„ Noneìœ¼ë¡œ ì±„ì›Œì„œ ë°˜í™˜
        osnd_df['station'] = None
        osnd_df['station_rl'] = None
        return osnd_df
    
    # MMS ë°ì´í„°ì—ì„œ Station ì •ë³´ ì¶”ì¶œ
    mms_df[['station', 'station_rl']] = mms_df['machine'].apply(extract_station_from_machine)
    
    # MMS ë°ì´í„°ì—ì„œ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° (start, finishëŠ” ìœ ì§€)
    mms_df_clean = mms_df.drop(
        columns=['mold_remove_date', 'mold_remove_time', 'mold_input_date', 'mold_input_time'],
        errors='ignore'
    ).copy()
    
    # datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì´ë¯¸ ë˜ì–´ìˆì„ ìˆ˜ ìˆì§€ë§Œ í™•ì¸)
    mms_df_clean['start'] = pd.to_datetime(mms_df_clean['start'], errors='coerce')
    mms_df_clean['finish'] = pd.to_datetime(mms_df_clean['finish'], errors='coerce')
    osnd_df['osnd_dt'] = pd.to_datetime(osnd_df['osnd_dt'], errors='coerce')
    
    # Station ë§¤í•‘
    station_info = []
    matched_count = 0
    
    for idx, row in osnd_df.iterrows():
        mold_id = row['mold_id']
        osnd_time = row['osnd_dt']
        
        # NaN ì²´í¬
        if pd.isna(mold_id) or pd.isna(osnd_time):
            station_info.append([None, None])
            continue
        
        # MMSì—ì„œ ë§¤ì¹­: ê°™ì€ mold_idì´ê³  ì‹œê°„ ë²”ìœ„ ë‚´ì— ìˆëŠ” ê²½ìš°
        match = mms_df_clean[
            (mms_df_clean['mold_id'] == mold_id) &
            (mms_df_clean['start'] <= osnd_time) &
            (mms_df_clean['finish'] >= osnd_time)
        ]
        
        if not match.empty:
            # ì²« ë²ˆì§¸ ë§¤ì¹­ ê²°ê³¼ ì‚¬ìš©
            station_info.append(match.iloc[0][['station', 'station_rl']].values)
            matched_count += 1
        else:
            station_info.append([None, None])
    
    station_df = pd.DataFrame(station_info, columns=['station', 'station_rl'])
    
    # ë³‘í•© ì „ì— ì¤‘ë³µ ë°©ì§€: ê¸°ì¡´ Station ì»¬ëŸ¼ ì œê±°
    osnd_df = osnd_df.drop(columns=['station', 'station_rl'], errors='ignore')
    
    # ë³‘í•© ìˆ˜í–‰
    osnd_df = pd.concat([osnd_df.reset_index(drop=True), station_df], axis=1)
    osnd_df.columns = osnd_df.columns.str.lower()
    
    if len(osnd_df) > 0:
        match_ratio = matched_count / len(osnd_df) * 100
        logging.info(f"âœ… OSND Station ë§¤ì¹­ ì™„ë£Œ: {len(osnd_df):,} rows ì¤‘ {matched_count:,} rows ë§¤ì¹­ë¨ ({match_ratio:.2f}%)")
    else:
        logging.info(f"âœ… OSND Station ë§¤ì¹­ ì™„ë£Œ: 0 rows")
    return osnd_df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Cross Check Defective Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cross_check_defective(osnd_df: pd.DataFrame, ipi_dms_df: pd.DataFrame) -> pd.DataFrame:
    """
    IPI_DMSì™€ OSND ë°ì´í„°ë¥¼ Cross Checkí•˜ì—¬ ì™„ì „ ì¼ì¹˜ ë°ì´í„°ë§Œ í•„í„°ë§
    - osnd_id ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¶€ ì¡°ì¸
    - machine_cd, station, st_lr_cd ë¹„êµí•˜ì—¬ ë§¤ì¹­ ì—¬ë¶€ í™•ì¸
    - ì™„ì „ ì¼ì¹˜ (all_match & valid_comparison)ì¸ ë°ì´í„°ë§Œ í•„í„°ë§
    """
    logging.info("4ï¸âƒ£ IPI_DMSì™€ OSND Cross Check ì‘ì—… ì‹œì‘")
    
    if len(osnd_df) == 0:
        logging.warning("âš ï¸ OSND ë°ì´í„°ê°€ ì—†ì–´ Cross Checkë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return osnd_df
    
    if len(ipi_dms_df) == 0:
        logging.warning("âš ï¸ IPI_DMS ë°ì´í„°ê°€ ì—†ì–´ Cross Checkë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # osnd_id ìë£Œí˜• í†µì¼ (ë¬¸ìì—´ë¡œ ë³€í™˜)
    osnd_df['osnd_id'] = osnd_df['osnd_id'].astype(str)
    ipi_dms_df['osnd_id'] = ipi_dms_df['osnd_id'].astype(str)
    
    # osnd_id ê¸°ì¤€ ë‚´ë¶€ ì¡°ì¸
    merged_df = pd.merge(ipi_dms_df, osnd_df, on='osnd_id', suffixes=('_ipi', ''))
    
    logging.info(f"ğŸ“Š ì¡°ì¸ ê²°ê³¼: {len(merged_df):,} rows (IPI_DMS: {len(ipi_dms_df):,}, OSND: {len(osnd_df):,})")
    
    if len(merged_df) == 0:
        logging.warning("âš ï¸ ì¡°ì¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # ê° ì—´ ë¹„êµ (NaN ì œì™¸)
    merged_df['machine_cd_match'] = (
        (merged_df['machine_cd_ipi'] == merged_df['machine_cd']) &
        merged_df['machine_cd_ipi'].notna() &
        merged_df['machine_cd'].notna()
    )
    
    merged_df['station_match'] = (
        (merged_df['station_ipi'] == merged_df['station']) &
        merged_df['station_ipi'].notna() &
        merged_df['station'].notna()
    )
    
    merged_df['st_lr_cd_match'] = (
        (merged_df['st_lr_cd'] == merged_df['station_rl']) &
        merged_df['st_lr_cd'].notna() &
        merged_df['station_rl'].notna()
    )
    
    # ì„¸ í•­ëª© ëª¨ë‘ ë§¤ì¹­
    merged_df['all_match'] = (
        merged_df['machine_cd_match'] &
        merged_df['station_match'] &
        merged_df['st_lr_cd_match']
    )
    
    # NaNì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ valid_comparison = False
    merged_df['valid_comparison'] = (
        merged_df[['machine_cd_ipi', 'machine_cd', 'station_ipi', 'station',
                   'st_lr_cd', 'station_rl']].notna().all(axis=1)
    )
    
    # ë§¤ì¹­ ìš”ì•½
    valid_matches = merged_df[merged_df['valid_comparison']]['all_match'].value_counts()
    
    logging.info("âœ… IPI DMSì™€ OSND ë§¤ì¹­ì„ í†µí•œ ë°ì´í„° ê²€ì¦ ê²°ê³¼:")
    if len(valid_matches) > 0:
        matched_count = valid_matches.get(True, 0)
        unmatched_count = valid_matches.get(False, 0)
        logging.info(f"   - ìœ íš¨ ë¹„êµ ì¤‘ ë§¤ì¹­: {matched_count:,}ê±´")
        logging.info(f"   - ìœ íš¨ ë¹„êµ ì¤‘ ë¯¸ë§¤ì¹­: {unmatched_count:,}ê±´")
    else:
        logging.info("   - ìœ íš¨ ë¹„êµ ë°ì´í„° ì—†ìŒ")
    
    # ìœ íš¨í•˜ì§€ ì•Šì€ ë¹„êµ ê°œìˆ˜
    invalid_count = (~merged_df['valid_comparison']).sum()
    if invalid_count > 0:
        logging.info(f"   - NaN í¬í•¨ìœ¼ë¡œ ë¹„êµ ì œì™¸ëœ í–‰ ìˆ˜: {invalid_count:,}ê±´")
    
    # ì™„ì „ ì¼ì¹˜ + ìœ íš¨ ë¹„êµ
    matched_rows = merged_df[merged_df['all_match'] & merged_df['valid_comparison']]
    
    logging.info(f"ğŸ“Š ìµœì¢… ë§¤ì¹­ ê²°ê³¼: {len(matched_rows):,}ê±´ (ì „ì²´ ì¡°ì¸ ê²°ê³¼: {len(merged_df):,}ê±´)")
    
    if len(matched_rows) == 0:
        logging.warning("âš ï¸ ì™„ì „ ì¼ì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # ì›ë³¸ OSND_dfì—ì„œ í•´ë‹¹ osnd_idë§Œ ì¶”ì¶œ
    matched_osnd_ids = matched_rows['osnd_id'].unique()
    matched_osnd_df = osnd_df[osnd_df['osnd_id'].isin(matched_osnd_ids)].copy()
    matched_osnd_df['osnd_dt'] = matched_osnd_df['osnd_id'].map(ipi_dms_df.set_index('osnd_id')['osnd_dt'])  
    
    logging.info(f"âœ… Cross Check ì™„ë£Œ: {len(matched_osnd_df):,} rows (ì›ë³¸ OSND ê¸°ì¤€)")
    return matched_osnd_df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Loading
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_to_silver(pg_quality: PostgresHelper, df: pd.DataFrame) -> None:
    """Silver í…Œì´ë¸”ì— ë°ì´í„° ì ì¬"""
    logging.info("5ï¸âƒ£ Silver í…Œì´ë¸” ì ì¬ ì¤‘...")
    
    # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
    table_exists = pg_quality.check_table(TARGET_SCHEMA, TARGET_TABLE)
    
    if not table_exists:
        logging.warning(f"âš ï¸ í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {TARGET_SCHEMA}.{TARGET_TABLE}")
        logging.warning("âš ï¸ í…Œì´ë¸”ì„ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”: /home/user/apps/airflow/db/quality/silver/ipi_defective_cross_validated.sql")
        return
    
    # etl_ingest_time ì œê±° (DBì˜ DEFAULT ê°’ ì‚¬ìš©)
    if 'etl_ingest_time' in df.columns:
        df = df.drop(columns=['etl_ingest_time'])
    
    # etl_extract_time ì œê±° (ìµœì¢… í…Œì´ë¸”ì—ëŠ” ì—†ìŒ)
    if 'etl_extract_time' in df.columns:
        df = df.drop(columns=['etl_extract_time'])
    
    # 100% NULL ê°€ëŠ¥ì„±ì´ ë†’ì€ ì»¬ëŸ¼ ì œê±° (í…Œì´ë¸”ì— ì •ì˜ë˜ì§€ ì•Šì€ ì»¬ëŸ¼)
    excluded_columns = [
        'memo', 'rework_date', 'ss_apply_date', 'ref_caption', 'ref_value01', 'ref_value02',
        'updater', 'update_dt', 'update_pc', 'repl_qty', 'repl_dt', 'repl_user', 'repl_cfm_dt', 'repl_cfm_user'
    ]
    for col in excluded_columns:
        if col in df.columns:
            df = df.drop(columns=[col])
            logging.debug(f"   ì œì™¸ëœ ì»¬ëŸ¼: {col} (100% NULL)")
    
    # í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ìˆœì„œì— ë§ì¶° DataFrame ì»¬ëŸ¼ ì¬ì •ë ¬
    col_order_sql = f"""
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_schema = '{TARGET_SCHEMA}' 
          AND table_name = '{TARGET_TABLE}'
        ORDER BY ordinal_position
    """
    table_columns = pg_quality.execute_query(col_order_sql, task_id="get_table_columns", xcom_key=None)
    table_column_names = [col[0].lower() for col in table_columns] if table_columns else []
    
    # DataFrameì— ìˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  í…Œì´ë¸” ìˆœì„œì— ë§ì¶° ì •ë ¬
    available_columns = [col for col in table_column_names if col in df.columns]
    df = df[available_columns]
    
    # DataFrameì„ tuple ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    data_tuples = [tuple(row) for row in df.values]
    columns = df.columns.tolist()
    
    # ë°ì´í„° ì ì¬
    logging.info(f"ğŸ“¦ ë°ì´í„° ì ì¬ ì¤‘: {len(df):,} rows")
    pg_quality.insert_data(
        schema_name=TARGET_SCHEMA,
        table_name=TARGET_TABLE,
        data=data_tuples,
        columns=columns,
        conflict_columns=['plant_cd', 'osnd_id']  # Primary Key
    )
    
    logging.info(f"âœ… Silver í…Œì´ë¸” ì ì¬ ì™„ë£Œ: {TARGET_SCHEMA}.{TARGET_TABLE}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Variable Management
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def update_variable(date_str: str) -> None:
    """Variable ì—…ë°ì´íŠ¸"""
    Variable.set(INCREMENT_KEY, date_str)
    logging.info(f"ğŸ“Œ Variable `{INCREMENT_KEY}` Update: {date_str}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Incremental Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def incremental_ipi_defective_cross_validated(**context) -> dict:
    """
    Incremental ì‘ì—…: ì „ì¼ ë°ì´í„° ì²˜ë¦¬
    """
    pg_prod = PostgresHelper(conn_id=PRODUCTION_POSTGRES_CONN_ID)
    pg_quality = PostgresHelper(conn_id=QUALITY_POSTGRES_CONN_ID)
    
    # ê³µìš© Variableì—ì„œ ë§ˆì§€ë§‰ ì²˜ë¦¬ì¼ì„ ì½ê³ , ê·¸ ë‹¤ìŒë‚ ì„ ì²˜ë¦¬ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •
    last_date_str = Variable.get(INCREMENT_KEY, default_var=None)
    
    # UTC ê¸°ì¤€ ë‚ ì§œ ê³„ì‚° (ë‚ ì§œ ë¹„êµë¥¼ ìœ„í•´ ì‹œê°„ ì œê±°)
    now_utc = datetime.utcnow()
    today_minus_1 = (now_utc - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    
    if last_date_str:
        try:
            last_date = datetime.strptime(last_date_str, '%Y-%m-%d')
            last_date = last_date.replace(hour=0, minute=0, second=0, microsecond=0)
        except Exception:
            # í˜•ì‹ ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ today-1ë¡œ ì¬ì„¤ì •
            last_date = today_minus_1 - timedelta(days=1)
        target_date = last_date + timedelta(days=1)
    else:
        # Variable ë¯¸ì„¤ì • ì‹œ today-1ì„ ì²˜ë¦¬
        target_date = today_minus_1
    
    # today-1ì„ ìƒí•œìœ¼ë¡œ ìº¡ (ê°™ê±°ë‚˜ ì‘ìœ¼ë©´ ì²˜ë¦¬)
    if target_date > today_minus_1:
        logging.info(f"âœ… ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. ì²˜ë¦¬í•  ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤ (Variable ê¸°ì¤€).")
        logging.info(f"   Variable last_date: {last_date_str}")
        logging.info(f"   target_date: {target_date.strftime('%Y-%m-%d')}")
        logging.info(f"   today_minus_1: {today_minus_1.strftime('%Y-%m-%d')}")
        return {"status": "up_to_date", "last_date": last_date_str or None}
    
    # ëŒ€ìƒ ì¼ì 00:00:00 ~ 23:59:59
    start_time = target_date.strftime('%Y-%m-%d')
    end_time = target_date.strftime('%Y-%m-%d')
    
    logging.info(f"\n{'='*60}")
    logging.info(f"ğŸš€ IPI Defective Cross Validated Incremental ì‹œì‘")
    logging.info(f"{'='*60}")
    logging.info(f"ğŸ“… ì²˜ë¦¬ ë‚ ì§œ: {start_time} (ì „ì¼ ë°ì´í„°)")
    
    try:
        # 1. MMS ë°ì´í„° ì¶”ì¶œ
        mms_df = extract_mms_data(pg_prod, start_time, end_time)
        
        # 2. OSND ë°ì´í„° ì¶”ì¶œ
        osnd_df = extract_osnd_data(pg_quality, start_time, end_time)
        
        # 3. IPI_DMS ë°ì´í„° ì¶”ì¶œ
        ipi_dms_df = extract_ipi_dms_data(pg_quality, start_time, end_time)
        
        if len(mms_df) == 0 and len(osnd_df) == 0 and len(ipi_dms_df) == 0:
            logging.warning(f"âš ï¸ MMS/OSND/IPI_DMS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {start_time}")
            update_variable(start_time)
            return {
                "status": "success_no_data",
                "date": start_time,
                "rows_processed": 0
            }
        
        # 4. MMS ë°ì´í„° ë³´ì • (ê²¹ì¹¨ ì œê±°)
        corrected_mms_df = correct_mms_overlaps(mms_df) if len(mms_df) > 0 else pd.DataFrame()
        
        # 5. OSNDì™€ Station ë§¤ì¹­
        osnd_with_station_df = match_osnd_with_station(corrected_mms_df, osnd_df) if len(osnd_df) > 0 else pd.DataFrame()
        
        # 6. IPI_DMSì™€ Cross Check
        final_osnd_df = cross_check_defective(osnd_with_station_df, ipi_dms_df) if len(osnd_with_station_df) > 0 and len(ipi_dms_df) > 0 else pd.DataFrame()
        
        # 7. Silver í…Œì´ë¸” ì ì¬
        if len(final_osnd_df) > 0:
            load_to_silver(pg_quality, final_osnd_df)
            logging.info(f"âœ… Incremental ì™„ë£Œ: {start_time} ({len(final_osnd_df):,} rows)")
        else:
            logging.info(f"âš ï¸ Incremental ë°ì´í„° ì—†ìŒ: {start_time}")
        
        # Variable ì—…ë°ì´íŠ¸
        update_variable(start_time)
        
        logging.info(f"\n{'='*60}")
        logging.info(f"âœ… Incremental ì™„ë£Œ")
        logging.info(f"{'='*60}")
        logging.info(f"ğŸ“… ì²˜ë¦¬ ë‚ ì§œ: {start_time}")
        logging.info(f"ğŸ“Š MMS ì›ë³¸ ë°ì´í„°: {len(mms_df):,} rows")
        logging.info(f"ğŸ“Š MMS ë³´ì • í›„ ë°ì´í„°: {len(corrected_mms_df):,} rows")
        logging.info(f"ğŸ“Š OSND ì›ë³¸ ë°ì´í„°: {len(osnd_df):,} rows")
        logging.info(f"ğŸ“Š OSND Station ë§¤ì¹­ í›„ ë°ì´í„°: {len(osnd_with_station_df):,} rows")
        logging.info(f"ğŸ“Š IPI_DMS ë°ì´í„°: {len(ipi_dms_df):,} rows")
        logging.info(f"ğŸ“Š ìµœì¢… Cross Check ë§¤ì¹­ ë°ì´í„°: {len(final_osnd_df):,} rows")
        logging.info(f"{'='*60}")
        
        return {
            "status": "success",
            "date": start_time,
            "mms_original_rows": len(mms_df),
            "mms_corrected_rows": len(corrected_mms_df),
            "osnd_original_rows": len(osnd_df),
            "osnd_station_matched_rows": len(osnd_with_station_df),
            "ipi_dms_rows": len(ipi_dms_df),
            "final_matched_rows": len(final_osnd_df)
        }
        
    except Exception as e:
        logging.error(f"\n{'='*60}")
        logging.error(f"âŒ Incremental ì‹¤íŒ¨: {str(e)}")
        logging.error(f"{'='*60}")
        raise


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Backfill Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_daily_batch(
    pg_prod: PostgresHelper,
    pg_quality: PostgresHelper,
    start_date: datetime,
    loop_count: int,
    expected_days: int
) -> dict:
    """Process a single daily batch"""
    logging.info(f"ğŸ”„ ë£¨í”„ {loop_count}/{expected_days} ì‹œì‘")
    
    start_time = start_date.strftime("%Y-%m-%d")
    end_time = start_date.strftime("%Y-%m-%d")
    
    logging.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {start_time}")
    
    try:
        # 1. MMS ë°ì´í„° ì¶”ì¶œ
        mms_df = extract_mms_data(pg_prod, start_time, end_time)
        
        # 2. OSND ë°ì´í„° ì¶”ì¶œ
        osnd_df = extract_osnd_data(pg_quality, start_time, end_time)
        
        # 3. IPI_DMS ë°ì´í„° ì¶”ì¶œ
        ipi_dms_df = extract_ipi_dms_data(pg_quality, start_time, end_time)
        
        if len(mms_df) == 0 and len(osnd_df) == 0 and len(ipi_dms_df) == 0:
            logging.warning(f"âš ï¸ MMS/OSND/IPI_DMS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {start_time}")
            update_variable(start_time)
            return {
                "loop": loop_count,
                "date": start_time,
                "status": "success_no_data",
                "rows_processed": 0
            }
        
        # 4. MMS ë°ì´í„° ë³´ì • (ê²¹ì¹¨ ì œê±°)
        corrected_mms_df = correct_mms_overlaps(mms_df) if len(mms_df) > 0 else pd.DataFrame()
        
        # 5. OSNDì™€ Station ë§¤ì¹­
        osnd_with_station_df = match_osnd_with_station(corrected_mms_df, osnd_df) if len(osnd_df) > 0 else pd.DataFrame()
        
        # 6. IPI_DMSì™€ Cross Check
        final_osnd_df = cross_check_defective(osnd_with_station_df, ipi_dms_df) if len(osnd_with_station_df) > 0 and len(ipi_dms_df) > 0 else pd.DataFrame()
        
        # 7. Silver í…Œì´ë¸” ì ì¬
        if len(final_osnd_df) > 0:
            load_to_silver(pg_quality, final_osnd_df)
            logging.info(f"âœ… ë°°ì¹˜ ì™„ë£Œ: {start_time} ({len(final_osnd_df):,} rows)")
        else:
            logging.info(f"âš ï¸ ë°°ì¹˜ì— ë°ì´í„° ì—†ìŒ: {start_time}")
        
        # Variable ì—…ë°ì´íŠ¸
        update_variable(start_time)
        
        return {
            "loop": loop_count,
            "date": start_time,
            "status": "success",
            "mms_original_rows": len(mms_df),
            "mms_corrected_rows": len(corrected_mms_df),
            "osnd_original_rows": len(osnd_df),
            "osnd_station_matched_rows": len(osnd_with_station_df),
            "ipi_dms_rows": len(ipi_dms_df),
            "final_matched_rows": len(final_osnd_df)
        }
        
    except Exception as e:
        logging.error(f"âŒ ë°°ì¹˜ ì‹¤íŒ¨: {start_time} - {str(e)}")
        update_variable(start_time)  # ì‹¤íŒ¨í•´ë„ ë‚ ì§œëŠ” ì—…ë°ì´íŠ¸
        return {
            "loop": loop_count,
            "date": start_time,
            "status": "failed",
            "error": str(e)
        }


def backfill_daily_batch(**context) -> dict:
    """
    Backfill ì‘ì—…: ê³¼ê±° ë°ì´í„° ì²˜ë¦¬ (ì¼ë³„ ë°°ì¹˜ ë£¨í”„)
    """
    pg_prod = PostgresHelper(conn_id=PRODUCTION_POSTGRES_CONN_ID)
    pg_quality = PostgresHelper(conn_id=QUALITY_POSTGRES_CONN_ID)
    
    # Variableì—ì„œ ë§ˆì§€ë§‰ ì²˜ë¦¬ì¼ ì½ê¸°
    last_date_str = Variable.get(INCREMENT_KEY, default_var=None)
    
    if not last_date_str:
        start_date = INITIAL_START_DATE
        logging.info(f"ì´ˆê¸° ì‹œì‘ ë‚ ì§œ ì‚¬ìš©: {start_date}")
    else:
        try:
            start_date = datetime.strptime(last_date_str, '%Y-%m-%d')
            start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
            start_date = start_date + timedelta(days=1)  # ë‹¤ìŒë‚ ë¶€í„° ì‹œì‘
            logging.info(f"ì´ì „ ì§„í–‰ ì§€ì  ì‚¬ìš©: {last_date_str} â†’ ë‹¤ìŒë‚ : {start_date.strftime('%Y-%m-%d')}")
        except Exception as e:
            logging.warning(f"âš ï¸ Variable íŒŒì‹± ì˜¤ë¥˜: {e}, ì´ˆê¸° ì‹œì‘ ë‚ ì§œë¡œ ì¬ì„¤ì •")
            start_date = INITIAL_START_DATE
    
    # Calculate end date (today - 2 days)
    now_utc = datetime.utcnow()
    end_date = (now_utc - timedelta(days=DAYS_OFFSET_FOR_INCREMENTAL)).replace(
        hour=0, minute=0, second=0, microsecond=0
    )
    
    # Calculate expected days
    expected_days = (end_date - start_date).days
    
    # Log backfill information
    logging.info(f"\n{'='*60}")
    logging.info(f"ğŸš€ OSI Defective Cross Validated Backfill ì‹œì‘")
    logging.info(f"{'='*60}")
    logging.info(f"Backfill ì‹œì‘: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    logging.info(f"ë°°ì¹˜ í¬ê¸°: ì¼ë³„ (í•˜ë£¨ì”© ì²˜ë¦¬)")
    logging.info(f"ì˜ˆìƒ ë£¨í”„ íšŸìˆ˜: {expected_days}íšŒ (ì¼ë³„)")
    logging.info(f"âš ï¸ í˜„ì¬ ì‹œê°„ì—ì„œ {DAYS_OFFSET_FOR_INCREMENTAL}ì¼ ì „ìœ¼ë¡œ ì„¤ì • (incremental DAG ì‹œì‘ì )")
    logging.info(f"{'='*60}")
    
    if expected_days <= 0:
        logging.info(f"âœ… ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. ì²˜ë¦¬í•  ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤.")
        logging.info(f"   start_date: {start_date.strftime('%Y-%m-%d')}")
        logging.info(f"   end_date: {end_date.strftime('%Y-%m-%d')}")
        return {"status": "up_to_date", "last_date": last_date_str or None}
    
    # Process daily batches
    results = []
    total_mms_original_rows = 0
    total_mms_corrected_rows = 0
    total_osnd_original_rows = 0
    total_osnd_station_matched_rows = 0
    total_ipi_dms_rows = 0
    total_final_matched_rows = 0
    loop_count = 0
    current_date = start_date
    
    while current_date <= end_date:
        loop_count += 1
        
        # Process batch
        batch_result = process_daily_batch(
            pg_prod, pg_quality, current_date, loop_count, expected_days
        )
        
        results.append(batch_result)
        
        if batch_result.get("status") == "success":
            total_mms_original_rows += batch_result.get("mms_original_rows", 0)
            total_mms_corrected_rows += batch_result.get("mms_corrected_rows", 0)
            total_osnd_original_rows += batch_result.get("osnd_original_rows", 0)
            total_osnd_station_matched_rows += batch_result.get("osnd_station_matched_rows", 0)
            total_ipi_dms_rows += batch_result.get("ipi_dms_rows", 0)
            total_final_matched_rows += batch_result.get("final_matched_rows", 0)
        
        # Move to next day
        current_date += timedelta(days=1)
    
    # Log completion
    logging.info(f"\n{'='*60}")
    logging.info(f"ğŸ‰ Backfill ì™„ë£Œ! ì´ {loop_count}íšŒ ë£¨í”„")
    logging.info(f"{'='*60}")
    if results:
        logging.info(f"ì²˜ë¦¬ ê¸°ê°„: {results[0]['date']} ~ {results[-1]['date']}")
        logging.info(f"ğŸ“Š ì´ MMS ì›ë³¸ ë°ì´í„°: {total_mms_original_rows:,} rows")
        logging.info(f"ğŸ“Š ì´ MMS ë³´ì • í›„ ë°ì´í„°: {total_mms_corrected_rows:,} rows")
        logging.info(f"ğŸ“Š ì´ OSND ì›ë³¸ ë°ì´í„°: {total_osnd_original_rows:,} rows")
        logging.info(f"ğŸ“Š ì´ OSND Station ë§¤ì¹­ í›„ ë°ì´í„°: {total_osnd_station_matched_rows:,} rows")
        logging.info(f"ğŸ“Š ì´ IPI_DMS ë°ì´í„°: {total_ipi_dms_rows:,} rows")
        logging.info(f"ğŸ“Š ì´ ìµœì¢… Cross Check ë§¤ì¹­ ë°ì´í„°: {total_final_matched_rows:,} rows")
        logging.info(f"{'='*60}")
    
    return {
        "status": "backfill_completed",
        "total_loops": loop_count,
        "total_days": len(results),
        "total_mms_original_rows": total_mms_original_rows,
        "total_mms_corrected_rows": total_mms_corrected_rows,
        "total_osnd_original_rows": total_osnd_original_rows,
        "total_osnd_station_matched_rows": total_osnd_station_matched_rows,
        "total_ipi_dms_rows": total_ipi_dms_rows,
        "total_final_matched_rows": total_final_matched_rows,
        "results": results
    }

