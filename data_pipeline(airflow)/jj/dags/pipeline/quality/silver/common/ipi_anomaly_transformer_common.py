"""
IPI Anomaly Transformer Common Functions
=========================================
ê³µí†µ í•¨ìˆ˜ ë° ì„¤ì •ì„ ëª¨ì•„ë‘” ëª¨ë“ˆ
"""

import logging
import os
import re
from datetime import datetime, timedelta
from typing import Tuple

import pandas as pd
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset

from airflow.models import Variable
from plugins.hooks.postgres_hook import PostgresHelper
from plugins.models.anomaly_transformer import AnomalyTransformer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Database Configuration
POSTGRES_CONN_ID = "pg_jj_telemetry_dw"  # ì›ë³¸ ë°ì´í„° ì¶”ì¶œìš©
TARGET_POSTGRES_CONN_ID = "pg_jj_quality_dw"  # ê²°ê³¼ ì ì¬ìš©

# File Paths (Airflow Variableë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
MODEL_PATH_DEFAULT = "/opt/airflow/models/best_anomaly_transformer.pth"

# Target Database Schema and Table
TARGET_SCHEMA = "silver"
TARGET_TABLE = "ipi_anomaly_transformer_result"

# Airflow Variable Key Names
INCREMENT_KEY = "ipi_anomaly_transformer_last_date"  # ë§ˆì§€ë§‰ ì²˜ë¦¬ì¼ (YYYY-MM-DD í˜•ì‹)
VARIABLE_MODEL_PATH = "ipi_temperature_model_path"

# Machine Configuration
# ì²˜ë¦¬í•  machine_no ë¦¬ìŠ¤íŠ¸ (bronze.ip_hmi_data_raw í…Œì´ë¸”ì˜ machine_no í˜•ì‹: "MCA04", "MCA12", "MCA20", "MCA34", "MCA37")
# ì˜ˆ: MACHINE_NO_LIST = ["MCA34", "MCA20", "MCA37"]  # ì—¬ëŸ¬ ê°œ ì²˜ë¦¬
MACHINE_NO_LIST = ["MCA34"]  # í˜„ì¬ MCA34ë§Œ ì²˜ë¦¬

# Model Configuration
WINDOW_SIZE = 60  # ì¸¡ì • ì‹œê°„ê°„ê²© 20ì´ˆ / ìµœì†Œ ê³µì • ìœ ì§€ì‹œê°„ 12ë¶„ = 60 window size
INPUT_SIZE = 1
D_MODEL = 128
N_HEADS = 4
DROPOUT = 0.005036702813595816
LAMBDA_KL = 0.12210012277592575

# Processing Configuration
MIN_TEMPERATURE = 160
MAX_TEMPERATURE = 180
MIN_SENSOR_DATA_POINTS = 100
BATCH_SIZE = 64
CHUNKSIZE = 200_000
RETRY_COUNT = 3

# Backfill Configuration
INITIAL_START_DATE = datetime(2025, 1, 1)  # ì´ˆê¸° ì‹œì‘ ë‚ ì§œ: 2025-01-01
DAYS_OFFSET_FOR_INCREMENTAL = 2  # ì˜¤ëŠ˜ë¡œë¶€í„° -2ì¼ ì „ê¹Œì§€ (incremental DAG ì‹œì‘ì )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utility Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_model_path():
    """Airflow Variableì—ì„œ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        return Variable.get(VARIABLE_MODEL_PATH, default_var=MODEL_PATH_DEFAULT)
    except Exception:
        return MODEL_PATH_DEFAULT


def natural_key(sensor_name):
    """ìì—°ìŠ¤ëŸ¬ìš´ ì •ë ¬ì„ ìœ„í•œ í‚¤"""
    return [int(s) if s.isdigit() else s for s in re.split(r'(\d+)', sensor_name)]


def create_sequences(data, window_size):
    """ì‹œí€€ìŠ¤ ìƒì„±"""
    sequences, indices = [], []
    for i in range(len(data) - window_size):
        sequences.append(data[i:i+window_size])
        indices.append((i, i+window_size))
    return np.stack(sequences), indices


def preprocess_sensor_df(sensor_df: pd.DataFrame) -> pd.DataFrame:
    """ì„¼ì„œë³„ ì‹œê³„ì—´ ì „ì²˜ë¦¬"""
    sensor_df = sensor_df.copy()
    sensor_df['T'] = sensor_df['T'].interpolate(method='linear')
    return sensor_df.sort_values('Date').reset_index(drop=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Extraction
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_plate_temperature_data(start_time: str, end_time: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """PostgreSQLì—ì„œ í”Œë ˆì´íŠ¸ ì˜¨ë„ ë°ì´í„° ì¶”ì¶œ (Airflowì—ì„œ ì´ë¯¸ ìˆ˜ì§‘ëœ ë°ì´í„°)"""
    logging.info(f"ğŸ“¥ ë°ì´í„° ì¶”ì¶œ ì‹œì‘: {start_time} ~ {end_time}")
    
    # PostgreSQL ì—°ê²° ì„¤ì •
    pg = PostgresHelper(conn_id=POSTGRES_CONN_ID)
    
    # PID ë¦¬ìŠ¤íŠ¸ë¥¼ PostgreSQLì—ì„œ ì§ì ‘ ì¡°íšŒ
    logging.info("ğŸ“‹ PID ë¦¬ìŠ¤íŠ¸ë¥¼ PostgreSQLì—ì„œ ì¡°íšŒ ì¤‘...")
    pid_list_sql = """
        SELECT 
            pid,
            mc,
            prop
        FROM bronze.ip_pid_master
        WHERE mc IN ('st_1', 'st_2', 'st_3', 'st_4', 'st_5', 'st_6', 'st_7', 'st_8')
            AND prop IN (
                'Plate Temperature UR', 
                'Plate Temperature UL', 
                'Plate Temperature LR', 
                'Plate Temperature LL'
            )
    """
    
    try:
        with pg.hook.get_conn() as conn:
            pid_list = pd.read_sql_query(pid_list_sql, conn)
            logging.info(f"âœ… PID ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì™„ë£Œ: {len(pid_list)} rows")
    except Exception as e:
        logging.error(f"âŒ PID ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise
    
    if len(pid_list) == 0:
        logging.warning("âš ï¸ í•„í„°ë§ëœ PIDê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=["SeqNo", "PID", "RxDate", "Pvalue", "mc_prop", "Date", "T"]), pd.DataFrame(columns=["pid", "mc", "prop", "mc_prop"])
    
    pid_list["pid"] = pid_list["pid"].astype(str)
    pid_list_r01 = pid_list.copy()
    pid_list_r01['mc_prop'] = pid_list_r01['mc'] + '_' + pid_list_r01['prop']
    pid_tuple = tuple(pid_list_r01['pid'].dropna().unique().astype(str))
    
    if len(pid_tuple) == 0:
        logging.warning("âš ï¸ í•„í„°ë§ëœ PIDê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=["SeqNo", "PID", "RxDate", "Pvalue", "mc_prop", "Date", "T"]), pid_list_r01
    
    logging.info(f"ğŸ“Š ëŒ€ìƒ PID ê°œìˆ˜: {len(pid_tuple)}")
    
    # machine_no ë¦¬ìŠ¤íŠ¸ í™•ì¸
    if not MACHINE_NO_LIST or len(MACHINE_NO_LIST) == 0:
        logging.warning("âš ï¸ ì²˜ë¦¬í•  machine_noê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=["SeqNo", "PID", "RxDate", "Pvalue", "mc_prop", "Date", "T"]), pid_list_r01
    
    logging.info(f"ğŸ­ ì²˜ë¦¬ ëŒ€ìƒ machine_no: {', '.join(MACHINE_NO_LIST)}")
    
    # SQL ì¿¼ë¦¬ êµ¬ì„± (PostgreSQL í˜•ì‹)
    # ì—¬ëŸ¬ machine_no ì§€ì›: WHERE machine_no IN (...)
    machine_no_list_str = ','.join([f"'{mno}'" for mno in MACHINE_NO_LIST])
    pid_list_str = ','.join([f"'{pid}'" for pid in pid_tuple])
    sql = f"""
        SELECT
            seqno AS "SeqNo", 
            pid AS "PID", 
            rxdate AS "RxDate", 
            pvalue AS "Pvalue"
        FROM bronze.ip_hmi_data_raw
        WHERE machine_no IN ({machine_no_list_str})
        AND rxdate BETWEEN '{start_time}' AND '{end_time}'
        AND pid::text IN ({pid_list_str})
        ORDER BY rxdate ASC
    """
    
    # ë°ì´í„° ì¶”ì¶œ
    try:
        logging.info("ğŸ“¥ PostgreSQLì—ì„œ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        with pg.hook.get_conn() as conn:
            # chunksizeë¡œ ìŠ¤íŠ¸ë¦¬ë° ì½ê¸°
            frames = []
            chunk_iter = pd.read_sql_query(
                sql,
                conn,
                parse_dates=["RxDate"],
                dtype={"PID": "string"},
                chunksize=CHUNKSIZE,
            )
                
            for i, chunk in enumerate(chunk_iter, 1):
                frames.append(chunk)
                if i % 5 == 0:
                    logging.info(f"ğŸ“¦ {i} chunks streamed (~{i*CHUNKSIZE:,} rows)")
            
            # ëª¨ë“  ì²­í¬ í•©ì¹˜ê¸°
            df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(
                columns=["SeqNo", "PID", "RxDate", "Pvalue"]
            )
        
            logging.info(f"âœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {len(df):,} rows")
            
    except Exception as e:
        logging.error(f"âŒ DB ì½ê¸° ì‹¤íŒ¨: {e}")
        raise
    
    # ë°ì´í„° ë³€í™˜
    if len(df) > 0:
        df["Pvalue"] = pd.to_numeric(df["Pvalue"], errors="coerce")
        
        # mc_prop ë³‘í•©
        df_r01 = df.merge(pid_list_r01[['pid', 'mc_prop']], how='left', left_on='PID', right_on='pid')
        df_r01 = df_r01.drop(columns=['SeqNo', 'pid'])
        df_r01 = df_r01.dropna(subset=['mc_prop']).reset_index(drop=True)
        
        # ì»¬ëŸ¼ ì •ë¦¬
        df_r01 = df_r01.rename(columns={'RxDate': 'Date', 'Pvalue': 'T'})
        df_r01['Date'] = pd.to_datetime(df_r01['Date'], errors='coerce')
        df_r01 = df_r01.sort_values('Date')
    else:
        df_r01 = pd.DataFrame(columns=["PID", "Date", "Pvalue", "mc_prop", "T"])
    
    logging.info(f"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df_r01):,} rows")
    return df_r01, pid_list_r01


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Anomaly Detection Model Classes
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TimeSeriesDataset(Dataset):
    """ì‹œê³„ì—´ ë°ì´í„°ì…‹"""
    def __init__(self, sequences, device='cpu'):
        self.sequences = torch.tensor(sequences, dtype=torch.float32).to(device)
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return self.sequences[idx]


def load_anomaly_transformer_model(device='cpu', allow_no_model=False):
    """Anomaly Transformer ëª¨ë¸ ë¡œë“œ
    
    Args:
        device: ì‚¬ìš©í•  ì¥ì¹˜ ('cpu' ë˜ëŠ” 'cuda')
        allow_no_model: ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ë„ ì´ˆê¸°í™”ëœ ëª¨ë¸ë¡œ ì§„í–‰í• ì§€ ì—¬ë¶€
    
    Returns:
        ëª¨ë¸ ê°ì²´
    
    Raises:
        FileNotFoundError: ëª¨ë¸ íŒŒì¼ì´ ì—†ê³  allow_no_model=Falseì¸ ê²½ìš°
    """
    logging.info(f"ğŸ¤– ëª¨ë¸ ë¡œë“œ ì‹œì‘ (device: {device})")
    
    try:
        class AnomalyTransformerWrapper(nn.Module):
            def __init__(self, input_size, window_size, d_model=512, n_heads=8, 
                        dropout=0.0, lambda_kl=0.1):
                super().__init__()
                self.model = AnomalyTransformer(
                    win_size=window_size,
                    enc_in=input_size,
                    c_out=input_size,
                    d_model=d_model,
                    n_heads=n_heads,
                    e_layers=3,
                    d_ff=d_model,
                    dropout=dropout,
                    activation='gelu',
                    output_attention=True
                )
                self.lambda_kl = lambda_kl
            
            def forward(self, x):
                return self.model(x)
        
        model = AnomalyTransformerWrapper(
            input_size=INPUT_SIZE,
            window_size=WINDOW_SIZE,
            d_model=D_MODEL,
            n_heads=N_HEADS,
            dropout=DROPOUT,
            lambda_kl=LAMBDA_KL
        ).to(device)
        
        # ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        model_path_to_load = get_model_path()
        
        if os.path.exists(model_path_to_load):
            try:
                state_dict = torch.load(model_path_to_load, map_location=device)
                model.load_state_dict(state_dict)
                logging.info(f"âœ… í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path_to_load}")
            except Exception as e:
                logging.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ (ì´ˆê¸°í™”ëœ ëª¨ë¸ ì‚¬ìš©): {e}")
                if not allow_no_model:
                    raise
                logging.warning("âš ï¸ ì´ˆê¸°í™”ëœ ëª¨ë¸(í•™ìŠµë˜ì§€ ì•ŠìŒ)ë¡œ ì§„í–‰í•©ë‹ˆë‹¤. ê²°ê³¼ê°€ ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            if allow_no_model:
                logging.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path_to_load}")
                logging.warning("âš ï¸ ì´ˆê¸°í™”ëœ ëª¨ë¸(í•™ìŠµë˜ì§€ ì•ŠìŒ)ë¡œ ì§„í–‰í•©ë‹ˆë‹¤. ê²°ê³¼ê°€ ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                error_msg = (
                    f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path_to_load}\n"
                    f"ğŸ’¡ í•´ê²° ë°©ë²•:\n"
                    f"   1. ëª¨ë¸ íŒŒì¼ì„ í•´ë‹¹ ê²½ë¡œì— ì—…ë¡œë“œí•˜ê±°ë‚˜\n"
                    f"   2. Airflow Variable 'ipi_temperature_model_path'ì— ì˜¬ë°”ë¥¸ ê²½ë¡œ ì„¤ì • ë˜ëŠ”\n"
                    f"   3. ëª¨ë¸ í•™ìŠµ í›„ ì €ì¥í•˜ì„¸ìš”."
                )
                logging.error(error_msg)
                raise FileNotFoundError(error_msg)
        
        model.eval()
        return model
        
    except ImportError as e:
        error_msg = (
            f"âŒ AnomalyTransformer ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\n"
            f"ğŸ’¡ í•´ê²° ë°©ë²•: plugins/models/ ë””ë ‰í† ë¦¬ì— AnomalyTransformer ëª¨ë“ˆì„ ë³µì‚¬í•˜ì„¸ìš”.\n"
            f"   ì˜ˆ: cp -r Anomaly-Transformer/model/* plugins/models/"
        )
        logging.error(error_msg)
        raise ImportError(error_msg) from e
    except FileNotFoundError:
        raise
    except Exception as e:
        logging.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Anomaly Detection Processing
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def detect_and_remove_anomalies(df_r01: pd.DataFrame, pid_list_r01: pd.DataFrame) -> pd.DataFrame:
    """ì´ìƒì¹˜ íƒì§€ ë° ì œê±°"""
    logging.info("ğŸ” ì´ìƒì¹˜ íƒì§€ ì‹œì‘")
    
    # ì›ë³¸ ë°ì´í„° ì €ì¥
    df_r00 = df_r01.copy()
    
    # í•„í„°ë§ ì „ ì˜¨ë„ í†µê³„ ë¡œê¹…
    if len(df_r01) > 0:
        temp_stats = df_r01['T'].describe()
        logging.info(f"ğŸ“Š í•„í„°ë§ ì „ ì˜¨ë„ í†µê³„:")
        logging.info(f"   ìµœì†Œê°’: {temp_stats.get('min', 'N/A'):.2f}â„ƒ")
        logging.info(f"   ìµœëŒ€ê°’: {temp_stats.get('max', 'N/A'):.2f}â„ƒ")
        logging.info(f"   í‰ê· ê°’: {temp_stats.get('mean', 'N/A'):.2f}â„ƒ")
        logging.info(f"   ì¤‘ì•™ê°’: {temp_stats.get('50%', 'N/A'):.2f}â„ƒ")
        logging.info(f"   í•„í„°ë§ ë²”ìœ„: {MIN_TEMPERATURE}â„ƒ ~ {MAX_TEMPERATURE}â„ƒ")
    
    # 1ì°¨ í•„í„°ë§ (ì˜¨ë„ ë²”ìœ„)
    df_r01 = df_r01[(df_r01['T'] < MAX_TEMPERATURE) & (df_r01['T'] > MIN_TEMPERATURE)]
    logging.info(f"ğŸ“Š 1ì°¨ í•„í„°ë§ ì™„ë£Œ: {len(df_r01):,} rows")
    
    # í•„í„°ë§ í›„ í†µê³„ (ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
    if len(df_r01) > 0:
        temp_stats_filtered = df_r01['T'].describe()
        logging.info(f"ğŸ“Š í•„í„°ë§ í›„ ì˜¨ë„ í†µê³„:")
        logging.info(f"   ìµœì†Œê°’: {temp_stats_filtered.get('min', 'N/A'):.2f}â„ƒ")
        logging.info(f"   ìµœëŒ€ê°’: {temp_stats_filtered.get('max', 'N/A'):.2f}â„ƒ")
        logging.info(f"   í‰ê· ê°’: {temp_stats_filtered.get('mean', 'N/A'):.2f}â„ƒ")
    elif len(df_r00) > 0:
        # í•„í„°ë§ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ê°€ ì œê±°ëœ ê²½ìš°
        out_of_range_count = len(df_r00[(df_r00['T'] <= MIN_TEMPERATURE) | (df_r00['T'] >= MAX_TEMPERATURE)])
        logging.warning(f"âš ï¸ ëª¨ë“  ë°ì´í„°ê°€ ì˜¨ë„ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")
        logging.warning(f"   ë²”ìœ„ ë°– ë°ì´í„°: {out_of_range_count:,} rows")
        logging.warning(f"   ë²”ìœ„ ë‚´ ë°ì´í„°: {len(df_r00) - out_of_range_count:,} rows")
    
    # ì„¼ì„œë³„ ë°ì´í„°í”„ë ˆì„ ë¶„ë¦¬ ë° ì „ì²˜ë¦¬
    sensor_dfs = {}
    original_dfs = {}
    sensor_list = df_r01['mc_prop'].unique()
    
    for sensor in sensor_list:
        df_sensor = df_r01[df_r01['mc_prop'] == sensor].copy()
        df_sensor = preprocess_sensor_df(df_sensor)
        df_original = df_r00[df_r00['mc_prop'] == sensor].copy()
        df_original = preprocess_sensor_df(df_original)
        
        # ì‹œê°„ ì¤‘ë³µ ì œê±°
        df_sensor = df_sensor.groupby('Date', as_index=False)['T'].mean()
        df_original = df_original.groupby('Date', as_index=False)['T'].mean()
        
        # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ í™•ì¸
        if len(df_sensor) < MIN_SENSOR_DATA_POINTS:
            continue
        
        sensor_dfs[sensor] = df_sensor
        original_dfs[sensor] = df_original
    
    logging.info(f"âœ… ì„¼ì„œë³„ ë¶„ë¦¬ ì™„ë£Œ! ì´ ì„¼ì„œ ìˆ˜: {len(sensor_dfs)}")
    
    if len(sensor_dfs) == 0:
        logging.warning("âš ï¸ ì²˜ë¦¬í•  ì„¼ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=["Date", "T", "mc_prop"])
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logging.info(f"ğŸ–¥ï¸ ì‚¬ìš© ì¥ì¹˜: {device}")
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    
    # ëª¨ë¸ ë¡œë“œ (ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ë„ ì§„í–‰í•˜ë ¤ë©´ allow_no_model=Trueë¡œ ì„¤ì •)
    # ì£¼ì˜: í•™ìŠµë˜ì§€ ì•Šì€ ì´ˆê¸°í™” ëª¨ë¸ì€ ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    use_uninitialized_model = Variable.get(
        "ipi_temperature_allow_uninitialized_model", 
        default_var="false"
    ).lower() == "true"
    
    model = load_anomaly_transformer_model(device, allow_no_model=use_uninitialized_model)
    
    # ì„¼ì„œë³„ ì´ìƒì¹˜ íƒì§€ ë° ì œê±°
    filtered_sensor_dfs = {}
    sorted_sensors = sorted(sensor_dfs.keys(), key=natural_key)
    total_removed_rows = 0
    
    for sensor_name in sorted_sensors:
        sensor_df = sensor_dfs[sensor_name]
        logging.info(f"ğŸ“¡ ì„¼ì„œ ì²˜ë¦¬ ì¤‘: {sensor_name} ({len(sensor_df):,} points)")
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        if len(sensor_df) < WINDOW_SIZE:
            logging.warning(f"âš ï¸ {sensor_name}: ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤ ({len(sensor_df)} < {WINDOW_SIZE})")
            filtered_sensor_dfs[sensor_name] = sensor_df
            continue
        
        sequence_data, sequence_indices = create_sequences(sensor_df[['T']].values, WINDOW_SIZE)
        
        # ì´ìƒì¹˜ íƒì§€
        recon_error_list = []
        with torch.no_grad():
            for i in range(0, len(sequence_data), 128):
                batch = sequence_data[i:i + 128]
                batch = torch.tensor(batch, dtype=torch.float32).to(device)
                
                recon, _, _, _ = model(batch)
                error = torch.mean((batch - recon) ** 2, dim=(1, 2)).cpu().numpy()
                recon_error_list.append(error)
        
        recon_error = np.concatenate(recon_error_list)
        
        # IQR ê¸°ë°˜ ì´ìƒì¹˜ íŒë³„
        q1, q3 = np.percentile(recon_error, [25, 75])
        iqr = q3 - q1
        lower_threshold = q1 - 1.5 * iqr
        upper_threshold = q3 + 1.5 * iqr
        anomalies = (recon_error < lower_threshold) | (recon_error > upper_threshold)
        
        # ì´ìƒì¹˜ êµ¬ê°„ ì œê±°
        remove_indices = set()
        for is_anom, (start, end) in zip(anomalies, sequence_indices):
            if is_anom:
                remove_indices.update(range(start, end))
        
        df_clean = sensor_df.drop(index=sorted(remove_indices)).reset_index(drop=True)
        filtered_sensor_dfs[sensor_name] = df_clean
        
        removed_count = len(remove_indices)
        total_removed_rows += removed_count
        logging.info(f"ğŸ§¹ {sensor_name} ì´ìƒì¹˜ ì œê±° ì™„ë£Œ! ì œê±°ëœ row ìˆ˜: {removed_count:,}")
    
    logging.info(f"âœ… ì „ì²´ ì„¼ì„œ ì´ìƒì¹˜ ì œê±° ì™„ë£Œ! ì´ ì œê±°ëœ row ìˆ˜: {total_removed_rows:,}")
    
    # ì„¼ì„œ ì •ë³´ ì¶”ê°€ ë° ë³‘í•©
    for sensor_name, df in filtered_sensor_dfs.items():
        df['mc_prop'] = sensor_name
    
    cleaned_df = pd.concat(filtered_sensor_dfs.values(), ignore_index=True)
    logging.info(f"âœ… ìµœì¢… ì •ë¦¬ëœ ë°ì´í„°: {len(cleaned_df):,} rows")
    
    return cleaned_df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Data Loading
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prepare_insert_data(
    cleaned_df: pd.DataFrame, 
    start_time: str, 
    end_time: str,
    extract_time: datetime
) -> list:
    """DB ì ì¬ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
    
    Args:
        cleaned_df: ì •ì œëœ ë°ì´í„°í”„ë ˆì„ (Date, T, mc_prop ì»¬ëŸ¼ í¬í•¨)
        start_time: ì²˜ë¦¬ ì‹œì‘ ì‹œê°„ (ë¬¸ìì—´)
        end_time: ì²˜ë¦¬ ì¢…ë£Œ ì‹œê°„ (ë¬¸ìì—´)
        extract_time: ì›ë³¸ ë°ì´í„° ì¶”ì¶œ ì‹œê°„ (datetime)
    
    Returns:
        DB ì ì¬ìš© ë¦¬ìŠ¤íŠ¸ (íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
    """
    insert_data = []
    
    # datetime ë³€í™˜
    processing_start_dt = datetime.strptime(start_time, "%Y-%m-%d %H:%M:%S") if isinstance(start_time, str) else start_time
    processing_end_dt = datetime.strptime(end_time, "%Y-%m-%d %H:%M:%S") if isinstance(end_time, str) else end_time
    
    # machine_code ì„¤ì • (MACHINE_NO_LISTì˜ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©, í˜„ì¬ëŠ” í•˜ë‚˜ë§Œ ì²˜ë¦¬)
    # ì—¬ëŸ¬ machine_noë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ cleaned_dfì— machine_code ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì‚¬ìš©
    if 'machine_code' in cleaned_df.columns:
        machine_code_col = cleaned_df['machine_code']
    else:
        # MACHINE_NO_LISTì—ì„œ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš© (í˜„ì¬ëŠ” í•˜ë‚˜ë§Œ ì²˜ë¦¬)
        machine_code = MACHINE_NO_LIST[0] if MACHINE_NO_LIST and len(MACHINE_NO_LIST) > 0 else None
        if machine_code is None:
            logging.warning("âš ï¸ MACHINE_NO_LISTê°€ ë¹„ì–´ìˆì–´ machine_codeë¥¼ ì„¤ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            machine_code = "UNKNOWN"
    
    for idx, row in cleaned_df.iterrows():
        # mc_prop ë¶„ë¦¬: "st_1_Plate Temperature UR" -> mc="st_1", prop="Plate Temperature UR"
        mc_prop = row['mc_prop']
        if '_' in mc_prop:
            parts = mc_prop.split('_', 2)  # ìµœëŒ€ 2ë²ˆ ë¶„ë¦¬
            if len(parts) >= 3:
                mc = f"{parts[0]}_{parts[1]}"  # "st_1"
                prop = parts[2]  # "Plate Temperature UR"
            else:
                logging.warning(f"âš ï¸ mc_prop í˜•ì‹ ì˜¤ë¥˜: {mc_prop}")
                continue
        else:
            logging.warning(f"âš ï¸ mc_prop í˜•ì‹ ì˜¤ë¥˜: {mc_prop}")
            continue
        
        # machine_code ê²°ì • (ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
        if 'machine_code' in cleaned_df.columns:
            current_machine_code = row['machine_code']
        else:
            current_machine_code = machine_code
        
        insert_data.append((
            current_machine_code,                  # machine_code
            mc,                                    # mc
            prop,                                  # prop
            row['Date'],                          # measurement_time
            float(row['T']),                      # temperature
            processing_start_dt,                   # processing_start_time
            processing_end_dt,                     # processing_end_time
            extract_time,                          # etl_extract_time
            # etl_ingest_timeëŠ” DBì—ì„œ DEFAULT now()ë¡œ ì²˜ë¦¬
        ))
    
    return insert_data


def load_cleaned_data(
    cleaned_df: pd.DataFrame,
    start_time: str,
    end_time: str,
    extract_time: datetime
) -> int:
    """ì •ì œëœ ë°ì´í„°ë¥¼ PostgreSQLì— ì ì¬
    
    Args:
        cleaned_df: ì •ì œëœ ë°ì´í„°í”„ë ˆì„
        start_time: ì²˜ë¦¬ ì‹œì‘ ì‹œê°„
        end_time: ì²˜ë¦¬ ì¢…ë£Œ ì‹œê°„
        extract_time: ì›ë³¸ ë°ì´í„° ì¶”ì¶œ ì‹œê°„
    
    Returns:
        ì ì¬ëœ í–‰ ìˆ˜
    """
    if len(cleaned_df) == 0:
        logging.warning("âš ï¸ ì ì¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0
    
    pg = PostgresHelper(conn_id=TARGET_POSTGRES_CONN_ID)
    
    # ì ì¬ ë°ì´í„° ì¤€ë¹„
    insert_data = prepare_insert_data(cleaned_df, start_time, end_time, extract_time)
    
    if not insert_data:
        logging.warning("âš ï¸ ì ì¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ë³€í™˜ í›„).")
        return 0
    
    columns = [
        "machine_code",
        "mc",
        "prop",
        "measurement_time",
        "temperature",
        "processing_start_time",
        "processing_end_time",
        "etl_extract_time"
    ]
    
    conflict_columns = [
        "machine_code",
        "mc",
        "prop",
        "measurement_time"
    ]
    
    try:
        logging.info(f"ğŸ“¦ ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ ì‹œì‘: {len(insert_data):,} rows")
        pg.insert_data(
            schema_name=TARGET_SCHEMA,
            table_name=TARGET_TABLE,
            data=insert_data,
            columns=columns,
            conflict_columns=conflict_columns,
            chunk_size=1000
        )
        logging.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ ì™„ë£Œ: {len(insert_data):,} rows")
        return len(insert_data)
    except Exception as e:
        logging.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ ì‹¤íŒ¨: {e}")
        raise


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Variable Management
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def update_variable(date_str: str) -> None:
    """Update Airflow variable with last processed date"""
    Variable.set(INCREMENT_KEY, date_str)
    logging.info(f"ğŸ“Œ Variable `{INCREMENT_KEY}` Update: {date_str}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Incremental Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_processing_time_range(**context) -> Tuple[str, str]:
    """Airflow Variableì—ì„œ ì²˜ë¦¬ ì‹œê°„ ë²”ìœ„ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        **context: Airflow context (ds, execution_date ë“± í¬í•¨)
    
    Returns:
        (start_time, end_time) íŠœí”Œ (ë¬¸ìì—´ í˜•ì‹: "YYYY-MM-DD HH:MM:SS")
    
    ë¡œì§:
    1. ipi_anomaly_transformer_last_date Variableì´ ìˆìœ¼ë©´ ì‚¬ìš©
       - ë§ˆì§€ë§‰ ì²˜ë¦¬ì¼ì˜ ë‹¤ìŒë‚ ì„ ì²˜ë¦¬ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •
       - start_time: ë‹¤ìŒë‚  00:00:00
       - end_time: ë‹¤ìŒë‚  23:59:59
    2. ì—†ìœ¼ë©´ ì „ì¼(today-1) ì²˜ë¦¬
    """
    # Variableì—ì„œ ë§ˆì§€ë§‰ ì²˜ë¦¬ì¼ ì½ê¸°
    last_date_str = None
    try:
        last_date_str = Variable.get(INCREMENT_KEY, default_var=None)
    except Exception as e:
        logging.warning(f"âš ï¸ Variable ì½ê¸° ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
    
    # UTC ê¸°ì¤€ ë‚ ì§œ ê³„ì‚°
    now_utc = datetime.utcnow()
    today_minus_1 = (now_utc - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    
    if last_date_str:
        try:
            last_date = datetime.strptime(last_date_str, '%Y-%m-%d')
            last_date = last_date.replace(hour=0, minute=0, second=0, microsecond=0)
        except Exception:
            # í˜•ì‹ ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ today-1ë¡œ ì¬ì„¤ì •
            last_date = today_minus_1 - timedelta(days=1)
        target_date = last_date + timedelta(days=1)
    else:
        # Variable ë¯¸ì„¤ì • ì‹œ today-1ì„ ì²˜ë¦¬
        target_date = today_minus_1
    
    # today-1ì„ ìƒí•œìœ¼ë¡œ ìº¡ (ê°™ê±°ë‚˜ ì‘ìœ¼ë©´ ì²˜ë¦¬)
    if target_date > today_minus_1:
        logging.info(f"âœ… ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. ì²˜ë¦¬í•  ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤ (Variable ê¸°ì¤€).")
        logging.info(f"   Variable last_date: {last_date_str}")
        logging.info(f"   target_date: {target_date.strftime('%Y-%m-%d')}")
        logging.info(f"   today_minus_1: {today_minus_1.strftime('%Y-%m-%d')}")
        # ìµœì‹  ìƒíƒœì—¬ë„ ë‚ ì§œ ë²”ìœ„ ë°˜í™˜ (ë¹ˆ ê²°ê³¼ë¡œ ì²˜ë¦¬)
        target_date = today_minus_1
    
    # ëŒ€ìƒ ì¼ì 00:00:00 ~ 23:59:59
    date_str = target_date.strftime('%Y-%m-%d')
    start_time = f"{date_str} 00:00:00"
    end_time = f"{date_str} 23:59:59"
    
    logging.info(f"ğŸ“‹ ì²˜ë¦¬ ì‹œê°„ ë²”ìœ„: {start_time} ~ {end_time}")
    return start_time, end_time


def run_anomaly_transformer(**context) -> dict:
    """ì´ìƒì¹˜ íƒì§€ ë©”ì¸ í•¨ìˆ˜ (ì¦ë¶„ ì²˜ë¦¬)
    
    Args:
        **context: Airflow context (Variableì—ì„œ ì‹œê°„ ë²”ìœ„ ìë™ ì½ê¸°)
    
    Variable:
        - ipi_anomaly_transformer_last_date: ë§ˆì§€ë§‰ ì²˜ë¦¬ì¼ (YYYY-MM-DD í˜•ì‹)
        - ì—†ìœ¼ë©´ ì „ì¼(today-1) ì²˜ë¦¬
    """
    extract_time = datetime.utcnow()
    
    # Variableì—ì„œ ì‹œê°„ ë²”ìœ„ ê°€ì ¸ì˜¤ê¸°
    start_time, end_time = get_processing_time_range(**context)
    
    # ì²˜ë¦¬ ë‚ ì§œ ì¶”ì¶œ (YYYY-MM-DD í˜•ì‹)
    processed_date = start_time.split()[0]  # "YYYY-MM-DD HH:MM:SS"ì—ì„œ ë‚ ì§œë§Œ ì¶”ì¶œ
    
    try:
        # ë°ì´í„° ì¶”ì¶œ
        df_r01, pid_list_r01 = extract_plate_temperature_data(start_time, end_time)
        
        if len(df_r01) == 0:
            logging.warning("âš ï¸ ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            # ë°ì´í„°ê°€ ì—†ì–´ë„ Variable ì—…ë°ì´íŠ¸ (ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€)
            update_variable(processed_date)
            return {
                "status": "success",
                "rows_processed": 0,
                "rows_inserted": 0,
                "message": "No data to process",
                "processed_date": processed_date
            }
        
        # ì´ìƒì¹˜ íƒì§€ ë° ì œê±°
        cleaned_df = detect_and_remove_anomalies(df_r01, pid_list_r01)
        
        if len(cleaned_df) == 0:
            logging.warning("âš ï¸ ì •ì œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            # ë°ì´í„°ê°€ ì—†ì–´ë„ Variable ì—…ë°ì´íŠ¸ (ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€)
            update_variable(processed_date)
            return {
                "status": "success",
                "rows_processed": 0,
                "rows_inserted": 0,
                "message": "No cleaned data to insert",
                "processed_date": processed_date
            }
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì ì¬
        rows_inserted = load_cleaned_data(
            cleaned_df=cleaned_df,
            start_time=start_time,
            end_time=end_time,
            extract_time=extract_time
        )
        
        # ì²˜ë¦¬ ì™„ë£Œ í›„ Variable ì—…ë°ì´íŠ¸
        update_variable(processed_date)
        
        logging.info(f"âœ… ì´ìƒì¹˜ íƒì§€ ì™„ë£Œ: {processed_date} ({len(cleaned_df):,} rows processed, {rows_inserted:,} rows inserted)")
        
        return {
            "status": "success",
            "rows_processed": len(cleaned_df),
            "rows_inserted": rows_inserted,
            "start_time": start_time,
            "end_time": end_time,
            "processed_date": processed_date,
            "target_schema": TARGET_SCHEMA,
            "target_table": TARGET_TABLE
        }
        
    except Exception as e:
        logging.error(f"âŒ ì´ìƒì¹˜ íƒì§€ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        return {
            "status": "failed",
            "error": str(e)
        }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Backfill Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_daily_batch(start_date: datetime, loop_count: int, expected_days: int) -> dict:
    """ì¼ë³„ ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
    
    Args:
        start_date: ì²˜ë¦¬í•  ë‚ ì§œ (00:00:00)
        loop_count: í˜„ì¬ ë£¨í”„ íšŸìˆ˜
        expected_days: ì˜ˆìƒ ì´ ì¼ìˆ˜
    
    Returns:
        ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    extract_time = datetime.utcnow()
    
    # ì²˜ë¦¬ ë‚ ì§œ ë²”ìœ„ ì„¤ì •
    date_str = start_date.strftime('%Y-%m-%d')
    start_time = f"{date_str} 00:00:00"
    end_time = f"{date_str} 23:59:59"
    
    logging.info(f"\n{'='*60}")
    logging.info(f"ğŸ“… ì¼ë³„ ë°°ì¹˜ ì²˜ë¦¬: {date_str} ({loop_count}/{expected_days})")
    logging.info(f"{'='*60}")
    
    try:
        # ë°ì´í„° ì¶”ì¶œ
        df_r01, pid_list_r01 = extract_plate_temperature_data(start_time, end_time)
        
        if len(df_r01) == 0:
            logging.warning(f"âš ï¸ {date_str}: ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "status": "success",
                "date": date_str,
                "rows_processed": 0,
                "rows_inserted": 0,
                "message": "No data to process"
            }
        
        # ì´ìƒì¹˜ íƒì§€ ë° ì œê±°
        cleaned_df = detect_and_remove_anomalies(df_r01, pid_list_r01)
        
        if len(cleaned_df) == 0:
            logging.warning(f"âš ï¸ {date_str}: ì •ì œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "status": "success",
                "date": date_str,
                "rows_processed": 0,
                "rows_inserted": 0,
                "message": "No cleaned data to insert"
            }
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì ì¬
        rows_inserted = load_cleaned_data(
            cleaned_df=cleaned_df,
            start_time=start_time,
            end_time=end_time,
            extract_time=extract_time
        )
        
        logging.info(f"âœ… {date_str} ì²˜ë¦¬ ì™„ë£Œ: {len(cleaned_df):,} rows processed, {rows_inserted:,} rows inserted")
        
        return {
            "status": "success",
            "date": date_str,
            "rows_processed": len(cleaned_df),
            "rows_inserted": rows_inserted,
            "start_time": start_time,
            "end_time": end_time
        }
        
    except Exception as e:
        logging.error(f"âŒ {date_str} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        return {
            "status": "failed",
            "date": date_str,
            "error": str(e)
        }


def backfill_daily_batch_task(**context) -> dict:
    """Backfill ë©”ì¸ íƒœìŠ¤í¬: ì¼ë³„ ë°°ì¹˜ ì²˜ë¦¬ ë£¨í”„
    
    Variable ê¸°ë°˜ìœ¼ë¡œ ì‹œì‘ì  ê²°ì •:
    - Variableì´ ìˆìœ¼ë©´: ë§ˆì§€ë§‰ ì²˜ë¦¬ì¼ ë‹¤ìŒë‚ ë¶€í„° ì‹œì‘
    - Variableì´ ì—†ìœ¼ë©´: INITIAL_START_DATEë¶€í„° ì‹œì‘
    
    ì¢…ë£Œì : today - DAYS_OFFSET_FOR_INCREMENTALì¼ (incremental DAG ì‹œì‘ì )
    """
    # Variableì—ì„œ ë§ˆì§€ë§‰ ì²˜ë¦¬ì¼ ì½ê¸°
    last_date_str = Variable.get(INCREMENT_KEY, default_var=None)
    
    if not last_date_str:
        start_date = INITIAL_START_DATE
        logging.info(f"ì´ˆê¸° ì‹œì‘ ë‚ ì§œ ì‚¬ìš©: {start_date.strftime('%Y-%m-%d')}")
    else:
        try:
            start_date = datetime.strptime(last_date_str, '%Y-%m-%d')
            start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
            start_date = start_date + timedelta(days=1)  # ë‹¤ìŒë‚ ë¶€í„° ì‹œì‘
            logging.info(f"ì´ì „ ì§„í–‰ ì§€ì  ì‚¬ìš©: {last_date_str} â†’ ë‹¤ìŒë‚ : {start_date.strftime('%Y-%m-%d')}")
        except Exception as e:
            logging.warning(f"âš ï¸ Variable íŒŒì‹± ì˜¤ë¥˜: {e}, ì´ˆê¸° ì‹œì‘ ë‚ ì§œë¡œ ì¬ì„¤ì •")
            start_date = INITIAL_START_DATE
    
    # ì¢…ë£Œ ë‚ ì§œ ê³„ì‚° (today - DAYS_OFFSET_FOR_INCREMENTALì¼)
    now_utc = datetime.utcnow()
    end_date = (now_utc - timedelta(days=DAYS_OFFSET_FOR_INCREMENTAL)).replace(
        hour=0, minute=0, second=0, microsecond=0
    )
    
    # ì˜ˆìƒ ì¼ìˆ˜ ê³„ì‚°
    expected_days = (end_date - start_date).days
    
    # Backfill ì •ë³´ ë¡œê·¸
    logging.info(f"\n{'='*60}")
    logging.info(f"ğŸš€ IPI Anomaly Transformer Backfill ì‹œì‘")
    logging.info(f"{'='*60}")
    logging.info(f"Backfill ì‹œì‘: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    logging.info(f"ë°°ì¹˜ í¬ê¸°: ì¼ë³„ (í•˜ë£¨ì”© ì²˜ë¦¬)")
    logging.info(f"ì˜ˆìƒ ë£¨í”„ íšŸìˆ˜: {expected_days}íšŒ (ì¼ë³„)")
    logging.info(f"âš ï¸ í˜„ì¬ ì‹œê°„ì—ì„œ {DAYS_OFFSET_FOR_INCREMENTAL}ì¼ ì „ìœ¼ë¡œ ì„¤ì • (incremental DAG ì‹œì‘ì )")
    logging.info(f"ğŸ­ ì²˜ë¦¬ ëŒ€ìƒ machine_no: {', '.join(MACHINE_NO_LIST)}")
    logging.info(f"{'='*60}")
    
    if expected_days <= 0:
        logging.info(f"âœ… Backfill ì™„ë£Œ: ì²˜ë¦¬í•  ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {
            "status": "success",
            "message": "No dates to process",
            "start_date": start_date.strftime('%Y-%m-%d'),
            "end_date": end_date.strftime('%Y-%m-%d'),
            "total_processed": 0
        }
    
    # ì¼ë³„ ë°°ì¹˜ ì²˜ë¦¬ ë£¨í”„
    results = []
    total_processed = 0
    total_inserted = 0
    loop_count = 0
    current_date = start_date
    
    while current_date < end_date:
        loop_count += 1
        result = process_daily_batch(current_date, loop_count, expected_days)
        results.append(result)
        
        if result["status"] == "success":
            total_processed += result.get("rows_processed", 0)
            total_inserted += result.get("rows_inserted", 0)
            
            # Variable ì—…ë°ì´íŠ¸ (ë§¤ì¼ ì²˜ë¦¬ ì™„ë£Œ í›„)
            update_variable(result["date"])
        
        # ë‹¤ìŒë‚ ë¡œ ì´ë™
        current_date = current_date + timedelta(days=1)
        
        # ì§„í–‰ ìƒí™© ë¡œê·¸ (10ì¼ë§ˆë‹¤)
        if loop_count % 10 == 0:
            logging.info(f"\nğŸ“Š ì§„í–‰ ìƒí™©: {loop_count}/{expected_days}ì¼ ì²˜ë¦¬ ì™„ë£Œ ({total_processed:,} rows processed, {total_inserted:,} rows inserted)")
    
    # ìµœì¢… ê²°ê³¼ ë¡œê·¸
    failed_count = sum(1 for r in results if r["status"] == "failed")
    
    logging.info(f"\n{'='*60}")
    logging.info(f"ğŸ‰ Backfill ì™„ë£Œ!")
    logging.info(f"{'='*60}")
    logging.info(f"ì´ ì²˜ë¦¬ ì¼ìˆ˜: {loop_count}ì¼")
    logging.info(f"ì„±ê³µ: {loop_count - failed_count}ì¼, ì‹¤íŒ¨: {failed_count}ì¼")
    logging.info(f"ì´ ì²˜ë¦¬ row ìˆ˜: {total_processed:,}")
    logging.info(f"ì´ ì ì¬ row ìˆ˜: {total_inserted:,}")
    logging.info(f"ë§ˆì§€ë§‰ ì²˜ë¦¬ì¼: {results[-1]['date'] if results else 'N/A'}")
    logging.info(f"{'='*60}")
    
    return {
        "status": "success",
        "total_days": loop_count,
        "success_days": loop_count - failed_count,
        "failed_days": failed_count,
        "total_processed": total_processed,
        "total_inserted": total_inserted,
        "start_date": start_date.strftime('%Y-%m-%d'),
        "end_date": end_date.strftime('%Y-%m-%d'),
        "last_processed_date": results[-1]['date'] if results else None,
        "results": results
    }

