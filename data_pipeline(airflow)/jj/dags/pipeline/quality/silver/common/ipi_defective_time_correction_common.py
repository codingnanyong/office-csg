"""ê³µí†µ í•¨ìˆ˜ ëª¨ë“ˆ - IPI Defective Time Correction Silver"""
import logging
import pandas as pd
from datetime import datetime, timedelta
from plugins.hooks.postgres_hook import PostgresHelper
from airflow.models import Variable


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1ï¸âƒ£ Configuration Constants
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Default Configuration
PRODUCTION_POSTGRES_CONN_ID = "pg_jj_production_dw"  # MC í…Œì´ë¸”ìš©
QUALITY_POSTGRES_CONN_ID = "pg_jj_quality_dw"        # IPI í…Œì´ë¸”, Target í…Œì´ë¸”ìš©

# Source Tables
SOURCE_SCHEMA = "bronze"
MC_TABLE = "ipi_mc_output_v2_raw"                    # pg_jj_production_dw
IPI_TABLE = "mspq_in_osnd_bt_ipi_raw"                # pg_jj_quality_dw

# Target Table
TARGET_SCHEMA = "silver"
TARGET_TABLE = "ipi_defective_time_corrected"                  # pg_jj_quality_dw

# Configuration
DELTA_SEC_THRESHOLD = 600  # 600ì´ˆ ì´í•˜ë§Œ í•„í„°ë§
INITIAL_START_DATE = datetime(2025, 7, 1)
DAYS_OFFSET_FOR_INCREMENTAL = 2  # -2ì¼ ì „ê¹Œì§€ (incremental DAG ì‹œì‘ì )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2ï¸âƒ£ Data Extraction
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_mc_data(pg_prod: PostgresHelper, start_time: str, end_time: str) -> pd.DataFrame:
    """MC í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (pg_jj_production_dw)"""
    logging.info(f"1ï¸âƒ£ MC í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_time} ~ {end_time}")
    
    # MC í…Œì´ë¸”ì˜ rst_ymdëŠ” VARCHAR(30)ì´ê³  '2025-07-16 00:00:00.418010' í˜•ì‹
    sql = f"""
        SELECT 
            mc_cd, st_num, st_side, act_qty, rst_ymd, upd_so_id
        FROM {SOURCE_SCHEMA}.{MC_TABLE}
        WHERE DATE(CAST(rst_ymd AS TIMESTAMP)) BETWEEN DATE('{start_time}') AND DATE('{end_time}')
    """
    
    logging.info(f"ğŸ” MC ì¿¼ë¦¬ ì‹¤í–‰: {sql}")
    
    data = pg_prod.execute_query(sql, task_id="extract_mc_data", xcom_key=None)
    
    if data:
        # tuple ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(data, columns=['mc_cd', 'st_num', 'st_side', 'act_qty', 'rst_ymd', 'upd_so_id'])
        df.columns = df.columns.str.lower()
    else:
        df = pd.DataFrame(columns=['mc_cd', 'st_num', 'st_side', 'act_qty', 'rst_ymd', 'upd_so_id'])
    
    logging.info(f"âœ… MC í…Œì´ë¸” ì¶”ì¶œ ì™„ë£Œ: {len(df):,} rows")
    return df


def extract_ipi_data(pg_quality: PostgresHelper, start_time: str, end_time: str) -> pd.DataFrame:
    """IPI í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (pg_jj_quality_dw)"""
    logging.info(f"2ï¸âƒ£ IPI í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_time} ~ {end_time}")
    
    # IPI í…Œì´ë¸”ì˜ osnd_dtëŠ” TIMESTAMP íƒ€ì…
    sql = f"""
        SELECT *
        FROM {SOURCE_SCHEMA}.{IPI_TABLE}
        WHERE osnd_dt BETWEEN TIMESTAMP '{start_time} 00:00:00' AND TIMESTAMP '{end_time} 23:59:59'
    """
    
    logging.info(f"ğŸ” IPI ì¿¼ë¦¬ ì‹¤í–‰: {sql}")
    
    # ì»¬ëŸ¼ëª… ì¡°íšŒë¥¼ ìœ„í•œ ì¿¼ë¦¬
    col_sql = f"""
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_schema = '{SOURCE_SCHEMA}' 
          AND table_name = '{IPI_TABLE}'
        ORDER BY ordinal_position
    """
    columns = pg_quality.execute_query(col_sql, task_id="get_ipi_columns", xcom_key=None)
    column_names = [col[0].lower() for col in columns] if columns else []
    
    data = pg_quality.execute_query(sql, task_id="extract_ipi_data", xcom_key=None)
    
    if data:
        df = pd.DataFrame(data, columns=column_names)
    else:
        df = pd.DataFrame(columns=column_names)
    
    logging.info(f"âœ… IPI í…Œì´ë¸” ì¶”ì¶œ ì™„ë£Œ: {len(df):,} rows")
    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3ï¸âƒ£ Data Transformation & Time Matching
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize_strings(mc_df: pd.DataFrame, ipi_df: pd.DataFrame) -> tuple:
    """ë¬¸ìì—´ ì •ê·œí™”"""
    logging.info("3ï¸âƒ£ ë¬¸ìì—´ ì •ê·œí™” ì¤‘...")
    
    # MC ë°ì´í„° ì •ê·œí™”
    mc_df['mc_cd'] = mc_df['mc_cd'].str.strip()
    mc_df['st_num'] = mc_df['st_num'].str.zfill(2)
    mc_df['st_side'] = mc_df['st_side'].str.strip().str.upper()
    
    # IPI ë°ì´í„° ì •ê·œí™”
    ipi_df['machine_cd'] = ipi_df['machine_cd'].str.strip()
    ipi_df['station'] = ipi_df['station'].str.zfill(2)
    ipi_df['st_lr_cd'] = ipi_df['st_lr_cd'].str.strip().str.upper()
    
    logging.info("âœ… ë¬¸ìì—´ ì •ê·œí™” ì™„ë£Œ")
    return mc_df, ipi_df


def parse_datetimes(mc_df: pd.DataFrame, ipi_df: pd.DataFrame) -> tuple:
    """ì‹œê°„ íŒŒì‹±"""
    logging.info("4ï¸âƒ£ ì‹œê°„ íŒŒì‹± ì¤‘...")
    
    # MC ë°ì´í„° ì‹œê°„ íŒŒì‹±
    mc_df['rst_ymd'] = pd.to_datetime(mc_df['rst_ymd'], errors='coerce')
    
    # IPI ë°ì´í„° ì‹œê°„ íŒŒì‹±
    ipi_df['osnd_dt'] = pd.to_datetime(ipi_df['osnd_dt'], errors='coerce')
    
    logging.info("âœ… ì‹œê°„ íŒŒì‹± ì™„ë£Œ")
    return mc_df, ipi_df


def perform_time_matching(mc_df: pd.DataFrame, ipi_df: pd.DataFrame) -> pd.DataFrame:
    """ì‹œê°„ ë§¤ì¹­ ë£¨í”„ ìˆ˜í–‰"""
    logging.info("5ï¸âƒ£ ì‹œê°„ ë§¤ì¹­ ë£¨í”„ ìˆ˜í–‰ ì¤‘...")
    
    # ì›ë³¸ ì‹œê°„ ë³µì‚¬ ë° delta ì´ˆê¸°í™”
    ipi_df['origin_dt'] = ipi_df['osnd_dt']
    ipi_df['delta_sec'] = 0.0
    
    matched_count = 0
    total_count = len(ipi_df)
    
    # ë§¤ì¹­ ë£¨í”„ ìˆ˜í–‰
    for idx, row in ipi_df.iterrows():
        key_mc = row['machine_cd']
        key_st = row['station']
        key_side = row['st_lr_cd']
        tgt_time = row['osnd_dt']
        
        if pd.isnull(tgt_time):
            continue
        
        # MC ë°ì´í„°ì—ì„œ ë§¤ì¹­ ì¡°ê±´: ê°™ì€ mc_cd, st_num, st_sideì´ê³  rst_ymdê°€ osnd_dt ì´ì „
        matched = mc_df[
            (mc_df['mc_cd'] == key_mc) &
            (mc_df['st_num'] == key_st) &
            (mc_df['st_side'] == key_side) &
            (mc_df['rst_ymd'] < tgt_time)
        ]
        
        if not matched.empty:
            matched = matched.copy()
            matched['time_diff'] = (tgt_time - matched['rst_ymd']).dt.total_seconds()
            best_match = matched.loc[matched['time_diff'].idxmin()]
            
            ipi_df.at[idx, 'osnd_dt'] = best_match['rst_ymd']
            ipi_df.at[idx, 'delta_sec'] = best_match['time_diff']
            matched_count += 1
        
        # ì§„í–‰ ìƒí™© ë¡œê¹… (1000ê±´ë§ˆë‹¤)
        if (idx + 1) % 1000 == 0:
            logging.info(f"   ì§„í–‰ ì¤‘: {idx + 1:,}/{total_count:,} ({matched_count:,}ê±´ ë§¤ì¹­)")
    
    logging.info(f"âœ… ì‹œê°„ ë§¤ì¹­ ì™„ë£Œ: {matched_count:,}/{total_count:,}ê±´ ë§¤ì¹­")
    return ipi_df


def filter_by_delta_threshold(ipi_df: pd.DataFrame, threshold: float = DELTA_SEC_THRESHOLD) -> tuple:
    """Delta ì´ˆê³¼ ë²”ìœ„ í•„í„°ë§"""
    logging.info(f"6ï¸âƒ£ Delta ì´ˆê³¼ ë²”ìœ„ í•„í„°ë§ ì¤‘ (threshold: {threshold}ì´ˆ)...")
    
    # í•„í„°ë§ëœ ê²°ê³¼ ë¶„ë¦¬
    df_normal = ipi_df[ipi_df['delta_sec'] <= threshold].copy()
    df_exceed = ipi_df[ipi_df['delta_sec'] > threshold].copy()
    
    # í†µê³„ ì¶œë ¥
    logging.info(f"ğŸ“Š ì •ìƒ ë²”ìœ„ (â‰¤{threshold}ì´ˆ): {len(df_normal):,}ê±´")
    logging.info(f"ğŸ“Š ì´ˆê³¼ ë²”ìœ„ (>{threshold}ì´ˆ): {len(df_exceed):,}ê±´")
    
    if len(df_normal) > 0:
        logging.info(f"ğŸ“Š Delta í†µê³„ (ì •ìƒ ë²”ìœ„):")
        logging.info(f"   - í‰ê· : {df_normal['delta_sec'].mean():.2f}ì´ˆ")
        logging.info(f"   - ì¤‘ì•™ê°’: {df_normal['delta_sec'].median():.2f}ì´ˆ")
        logging.info(f"   - ìµœì†Œ: {df_normal['delta_sec'].min():.2f}ì´ˆ")
        logging.info(f"   - ìµœëŒ€: {df_normal['delta_sec'].max():.2f}ì´ˆ")
    
    return df_normal, df_exceed


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4ï¸âƒ£ Data Loading
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_to_silver(
    pg_quality: PostgresHelper, 
    df: pd.DataFrame,
    target_schema: str = TARGET_SCHEMA,
    target_table: str = TARGET_TABLE
) -> None:
    """Silver í…Œì´ë¸”ì— ë°ì´í„° ì ì¬"""
    logging.info("7ï¸âƒ£ Silver í…Œì´ë¸” ì ì¬ ì¤‘...")
    
    # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
    table_exists = pg_quality.check_table(target_schema, target_table)
    
    if not table_exists:
        logging.warning(f"âš ï¸ í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {target_schema}.{target_table}")
        logging.warning("âš ï¸ í…Œì´ë¸”ì„ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”: /home/user/apps/airflow/db/quality/silver/ipi_defective_time_corrected.sql")
    
    # etl_ingest_time ì œê±° (DBì˜ DEFAULT ê°’ ì‚¬ìš©)
    if 'etl_ingest_time' in df.columns:
        df = df.drop(columns=['etl_ingest_time'])
    
    # 100% NULL ì»¬ëŸ¼ ì œê±° (í…Œì´ë¸”ì— ì •ì˜ë˜ì§€ ì•Šì€ ì»¬ëŸ¼)
    # ì œì™¸í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸: memo, rework_date, ss_apply_date, ref_caption, ref_value01, ref_value02,
    #                   updater, update_dt, update_pc, repl_dt, repl_user, repl_cfm_dt, repl_cfm_user,
    #                   extra1_fld, extra2_fld, extra3_fld, extra4_fld, extra5_fld
    excluded_columns = [
        'memo', 'rework_date', 'ss_apply_date', 'ref_caption', 'ref_value01', 'ref_value02',
        'updater', 'update_dt', 'update_pc', 'repl_dt', 'repl_user', 'repl_cfm_dt', 'repl_cfm_user',
        'extra1_fld', 'extra2_fld', 'extra3_fld', 'extra4_fld', 'extra5_fld'
    ]
    for col in excluded_columns:
        if col in df.columns:
            df = df.drop(columns=[col])
            logging.debug(f"   ì œì™¸ëœ ì»¬ëŸ¼: {col} (100% NULL)")
    
    # í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ìˆœì„œì— ë§ì¶° DataFrame ì»¬ëŸ¼ ì¬ì •ë ¬
    # í…Œì´ë¸” ì»¬ëŸ¼ ìˆœì„œ ì¡°íšŒ
    col_order_sql = f"""
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_schema = '{target_schema}' 
          AND table_name = '{target_table}'
        ORDER BY ordinal_position
    """
    table_columns = pg_quality.execute_query(col_order_sql, task_id="get_table_columns", xcom_key=None)
    table_column_names = [col[0].lower() for col in table_columns] if table_columns else []
    
    # DataFrameì— ìˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  í…Œì´ë¸” ìˆœì„œì— ë§ì¶° ì •ë ¬
    available_columns = [col for col in table_column_names if col in df.columns]
    df = df[available_columns]
    
    # DataFrameì„ tuple ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    data_tuples = [tuple(row) for row in df.values]
    columns = df.columns.tolist()
    
    # ë°ì´í„° ì ì¬
    logging.info(f"ğŸ“¦ ë°ì´í„° ì ì¬ ì¤‘: {len(df):,} rows")
    pg_quality.insert_data(
        schema_name=target_schema,
        table_name=target_table,
        data=data_tuples,
        columns=columns,
        conflict_columns=['plant_cd', 'osnd_id', 'shift_cd', 'osnd_date', 'resource_cd', 'mc_cd', 'station', 'st_lr_cd']  # Primary Key
    )
    
    logging.info(f"âœ… Silver í…Œì´ë¸” ì ì¬ ì™„ë£Œ: {target_schema}.{target_table}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5ï¸âƒ£ Variable Management
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def update_variable(increment_key: str, end_date: str) -> None:
    """Update Airflow variable with last processed date"""
    Variable.set(increment_key, end_date)
    logging.info(f"ğŸ“Œ Variable `{increment_key}` Update: {end_date}")

