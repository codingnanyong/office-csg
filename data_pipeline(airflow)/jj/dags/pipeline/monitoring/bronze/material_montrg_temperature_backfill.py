import logging
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from plugins.hooks.postgres_hook import PostgresHelper
from dags.pipeline.monitoring.bronze.common.material_montrg_temperature_common import (
    get_company_start_date,
    get_day_end_date,
    calculate_expected_daily_loops,
    extract_data,
    load_data,
    update_variable,
    INDO_TZ,
    TARGET_POSTGRES_CONN_ID,
    HOURS_OFFSET_FOR_INCREMENTAL,
    COMPANIES
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ Configuration Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_ARGS = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'sla': timedelta(hours=2)
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ Main Backfill Logic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_daily_batch_for_company(
    source_pg: PostgresHelper, 
    target_pg: PostgresHelper, 
    start_date: datetime, 
    end_date: datetime,
    company_cd: str,
    loop_count: int,
    expected_loops: int
) -> dict:
    """Process a single daily batch for specific company"""
    logging.info(f"ğŸ”„ ë£¨í”„ {loop_count}/{expected_loops} ì‹œì‘ (Company: {company_cd})")
    
    start_str = start_date.strftime("%Y-%m-%d %H:%M:%S")
    end_str = end_date.strftime("%Y-%m-%d %H:%M:%S")
    
    logging.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {start_str} ~ {end_str} (Company: {company_cd})")
    
    data, row_count = extract_data(source_pg, start_str, end_str, company_cd)
    
    if row_count > 0:
        extract_time = datetime.utcnow()
        loaded_rows = load_data(target_pg, data, extract_time, company_cd)
        logging.info(f"âœ… ë°°ì¹˜ ì™„ë£Œ: {start_str} ~ {end_str} ({loaded_rows} rows) for {company_cd}")
    else:
        logging.info(f"ë°°ì¹˜ì— ë°ì´í„° ì—†ìŒ: {start_str} ~ {end_str} (Company: {company_cd})")
    
    return {
        "loop": loop_count,
        "company_cd": company_cd,
        "start": start_str,
        "end": end_str,
        "row_count": row_count,
        "date": start_date.strftime("%Y-%m-%d")
    }

def backfill_company_task(company_config: dict) -> dict:
    """Backfill task for a company"""
    return process_company_backfill(
        company_config['code'],
        company_config['increment_key'],
        company_config['source_conn_id']
    )

def process_company_backfill(company_cd: str, increment_key: str, source_conn_id: str) -> dict:
    """Process backfill for a single company"""
    source_pg = PostgresHelper(conn_id=source_conn_id)
    target_pg = PostgresHelper(conn_id=TARGET_POSTGRES_CONN_ID)
    
    # Calculate end date (ì¸ë„ë„¤ì‹œì•„ ì‹œê°„ ê¸°ì¤€, 2ì‹œê°„ ì „ê¹Œì§€)
    # ì˜ˆ: KST 16:11 -> INDO 14:11 -> 2ì‹œê°„ ì „ = 12:11 -> 11:59:59ê¹Œì§€ ìˆ˜ì§‘
    current_time_indo = datetime.now(INDO_TZ)
    safe_time = current_time_indo - timedelta(hours=HOURS_OFFSET_FOR_INCREMENTAL)
    # ì´ì „ ì‹œê°„ì˜ ë§ˆì§€ë§‰ ì´ˆë¡œ ì„¤ì • (ì˜ˆ: 12:11 -> 11:59:59)
    end_date = safe_time.replace(minute=0, second=0, microsecond=0) - timedelta(seconds=1)
    
    logging.info(f"ğŸ”„ {company_cd} ì²˜ë¦¬ ì‹œì‘")
    logging.info(f"â° í˜„ì¬ ì¸ë„ë„¤ì‹œì•„ ì‹œê°„: {current_time_indo.strftime('%Y-%m-%d %H:%M:%S')}")
    logging.info(f"â° ì•ˆì „ ë§ˆì§„ ì ìš© í›„: {safe_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logging.info(f"â° ìˆ˜ì§‘ ì¢…ë£Œ ì‹œì : {end_date.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Get company-specific start date (backfill mode - no initial_date)
    start_date = get_company_start_date(increment_key, company_cd)
    
    # Set timezone if not set
    if start_date.tzinfo is None:
        start_date = start_date.replace(tzinfo=INDO_TZ)
    
    # Calculate expected loops for this company
    expected_loops = calculate_expected_daily_loops(start_date, end_date)
    
    logging.info(f"{company_cd} ì‹œì‘: {start_date} ~ {end_date}")
    logging.info(f"{company_cd} ì˜ˆìƒ ë£¨í”„: {expected_loops}íšŒ (ì¼ë³„)")
    logging.info(f"âš ï¸ ì¸ë„ë„¤ì‹œì•„ ì‹œê°„ ê¸°ì¤€ {HOURS_OFFSET_FOR_INCREMENTAL}ì‹œê°„ ì „ê¹Œì§€ ìˆ˜ì§‘")
    
    # Process daily batches for this company
    company_results = []
    loop_count = 0
    current_date = start_date
    
    while current_date <= end_date:
        loop_count += 1
        
        # Calculate day end date
        day_end = get_day_end_date(current_date)
        if day_end > end_date:
            day_end = end_date
        
        try:
            batch_result = process_daily_batch_for_company(
                source_pg, target_pg, current_date, day_end, company_cd, loop_count, expected_loops
            )
            company_results.append(batch_result)
            logging.info(f"âœ… {company_cd} ë£¨í”„ {loop_count} ì™„ë£Œ: {batch_result['row_count']:,} rows")
            
            # Update variable after each successful batch
            update_variable(increment_key, batch_result['end'])
            
        except Exception as e:
            logging.error(f"âŒ {company_cd} ë£¨í”„ {loop_count} ì‹¤íŒ¨: {str(e)}")
        
        # Move to next day
        current_date = current_date + timedelta(days=1)
    
    total_rows = sum([r['row_count'] for r in company_results])
    logging.info(f"ğŸ‰ {company_cd} ì™„ë£Œ! {len(company_results)}íšŒ ë£¨í”„, {total_rows:,}ê°œ rows")
    
    return {
        "status": "backfill_completed",
        "company_cd": company_cd,
        "total_batches": len(company_results),
        "total_rows": total_rows,
        "results": company_results
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ DAG Definition
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with DAG(
    dag_id="material_warehouse_temperature_backfill",
    default_args=DEFAULT_ARGS,
    schedule_interval="@once",
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["JJ", "Material", "Warehouse" "Temperature", "Telemetry", "Backfill"]
) as dag:
    
    # Start task
    start = PythonOperator(
        task_id="start",
        python_callable=lambda: logging.info("ğŸš€ Temperature ë°ì´í„° Backfill ì‹œì‘"),
    )
    
    # Company-specific tasks (parallel execution) - ë™ì  ìƒì„±
    backfill_tasks = []
    for company in COMPANIES:
        task = PythonOperator(
            task_id=f"backfill_{company['code'].lower()}",
            python_callable=lambda comp=company, **kwargs: process_company_backfill(
                comp['code'], comp['increment_key'], comp['source_conn_id']
            ),
            provide_context=True,
        )
        backfill_tasks.append(task)
    
    # End task
    end = PythonOperator(
        task_id="end",
        python_callable=lambda: logging.info("ğŸ‰ Temperature ë°ì´í„° Backfill ì™„ë£Œ"),
    )
    
    # Task dependencies - All companies run in parallel
    start >> backfill_tasks >> end
