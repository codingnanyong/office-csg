"""ê³µí†µ í•¨ìˆ˜ ëª¨ë“ˆ - Material Monitoring Temperature Raw"""
import logging
from datetime import datetime, timedelta, timezone
from plugins.hooks.postgres_hook import PostgresHelper
from airflow.models import Variable


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1ï¸âƒ£ Configuration Constants
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Default Configuration
TABLE_NAME = "temperature_raw"
SCHEMA_NAME = "bronze"
TARGET_POSTGRES_CONN_ID = "pg_jj_monitoring_dw"
INDO_TZ = timezone(timedelta(hours=7))  # ì¸ë„ë„¤ì‹œì•„ ì‹œê°„ (UTC+7)
INITIAL_START_DATE = datetime(2025, 8, 1, 0, 0, 0)
HOURS_OFFSET_FOR_INCREMENTAL = 2  # 2ì‹œê°„ ì „ ë°ì´í„°ê¹Œì§€ ìˆ˜ì§‘

# Company Configuration - ë°°ì—´ë¡œ ê´€ë¦¬
COMPANIES = [
    {
        "code": "JJ",
        "increment_key": "last_extract_time_temperature_raw_jj",
        "source_conn_id": "pg_material_jj"
    },
    {
        "code": "JJ2",
        "increment_key": "last_extract_time_temperature_raw_jj2",
        "source_conn_id": "pg_material_jj2"
    }
]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2ï¸âƒ£ Utility Functions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_datetime(dt_str: str) -> datetime:
    """Parse datetime string with microsecond support"""
    try:
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S.%f")
    except ValueError:
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S")


def get_day_end_date(start_date: datetime) -> datetime:
    """Get the end of the day for a given date"""
    return start_date.replace(hour=23, minute=59, second=59, microsecond=999999)


def calculate_expected_daily_loops(start_date: datetime, end_date: datetime) -> int:
    """Calculate expected number of daily loops"""
    return (end_date.date() - start_date.date()).days + 1


def get_company_start_date(increment_key: str, company_name: str, initial_date: datetime = None) -> datetime:
    """Get start date for specific company
    
    Args:
        increment_key: Variable key for last extract time
        company_name: Company code (e.g., 'JJ', 'JJ2')
        initial_date: Initial start date if variable doesn't exist (for incremental mode)
    """
    last_time_str = Variable.get(increment_key, default_var=None)
    if last_time_str:
        last_time = parse_datetime(last_time_str)
        # Set timezone if not set
        if last_time.tzinfo is None:
            last_time = last_time.replace(tzinfo=INDO_TZ)
        # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„ì˜ ë‹¤ìŒ ì´ˆë¶€í„° ì‹œìž‘
        start_time = last_time + timedelta(seconds=1)
        logging.info(f"{company_name} - ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„: {last_time}")
        logging.info(f"{company_name} - ë‹¤ìŒ ìˆ˜ì§‘ ì‹œìž‘ ì‹œê°„: {start_time}")
        return start_time
    else:
        if initial_date:
            # Incremental mode: use 1 hour ago if no variable
            now_indo = datetime.now(INDO_TZ)
            start_time = (now_indo - timedelta(hours=1)).replace(minute=0, second=0, microsecond=0)
            logging.info(f"{company_name} - Variableì´ ì—†ì–´ ì§ì „ 1ì‹œê°„ êµ¬ê°„({start_time.strftime('%Y-%m-%d %H:00:00')})ë¶€í„° ìˆ˜ì§‘")
            return start_time
        else:
            # Backfill mode: use INITIAL_START_DATE
            logging.info(f"{company_name} - ì´ˆê¸° ì‹œìž‘ ë‚ ì§œ ì‚¬ìš©: {INITIAL_START_DATE}")
            return INITIAL_START_DATE


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3ï¸âƒ£ Data Extraction
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_extract_sql(start_date: str, end_date: str) -> str:
    """Build SQL query for data extraction"""
    return f'''
        SELECT 
            ymd,
            hmsf,
            sensor_id,
            device_id,
            capture_dt,
            t1,
            t2,
            t3,
            t4,
            t5,
            t6,
            upload_yn,
            upload_dt
        FROM public.temperature
        WHERE capture_dt >= '{start_date}'::timestamp 
          AND capture_dt <= '{end_date}'::timestamp
        ORDER BY capture_dt
    '''


def extract_data(pg: PostgresHelper, start_date: str, end_date: str, source_name: str) -> tuple:
    """Extract data from PostgreSQL database"""
    sql = build_extract_sql(start_date, end_date)
    logging.info(f"{source_name} - ì‹¤í–‰ ì¿¼ë¦¬: {sql}")
    
    data = pg.execute_query(sql, task_id=f"extract_data_{source_name}", xcom_key=None)
    
    # Calculate row count
    if data and isinstance(data, list):
        row_count = len(data)
    else:
        row_count = 0
    
    logging.info(f"{source_name} - {start_date} ~ {end_date} ì¶”ì¶œ row ìˆ˜: {row_count}")
    return data, row_count


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4ï¸âƒ£ Data Loading
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def prepare_insert_data(data: list, company_cd: str, extract_time: datetime) -> list:
    """Prepare data for PostgreSQL insertion"""
    if not data:
        return []
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ì§€ í™•ì¸í•˜ê³  ì²˜ë¦¬
    if isinstance(data[0], dict):
        return [
            (
                company_cd,
                row['sensor_id'], row['device_id'], row['ymd'], row['hmsf'], row['capture_dt'],
                row['t1'], row['t2'], row['t3'], row['t4'], row['t5'], row['t6'],
                row['upload_yn'], row['upload_dt'],
                extract_time
            ) for row in data
        ]
    else:
        # ë¦¬ìŠ¤íŠ¸/íŠœí”Œ í˜•íƒœì¸ ê²½ìš°
        # ì›ë³¸: ymd, hmsf, sensor_id, device_id, capture_dt, t1, t2, t3, t4, t5, t6, upload_yn, upload_dt
        # íƒ€ê²Ÿ: company_cd, sensor_id, device_id, ymd, hmsf, capture_dt, temperature, humidity, t3, find_dust_1.0, finde_dust2.5, fine_dust10, upload_yn, upload_dt, etl_extract_time
        return [
            (
                company_cd,
                row[2], row[3], row[0], row[1], row[4],  # sensor_id, device_id, ymd, hmsf, capture_dt
                row[5], row[6], row[7], row[8], row[9], row[10],  # t1(temperature)~t6(fine_dust10)
                row[11], row[12],  # upload_yn, upload_dt
                extract_time
            ) for row in data
        ]


def get_column_names() -> list:
    """Get column names for PostgreSQL table"""
    return [
        "company_cd", "sensor_id", "device_id", "ymd", "hmsf", "capture_dt",
        "temperature", "humidity", "t3", '"find_dust_1.0"', '"finde_dust2.5"', "fine_dust10",
        "upload_yn", "upload_dt", "etl_extract_time"
    ]


def load_data(
    pg: PostgresHelper, 
    data: list, 
    extract_time: datetime, 
    company_cd: str,
    schema_name: str = SCHEMA_NAME,
    table_name: str = TABLE_NAME
) -> int:
    """Load data into PostgreSQL database"""
    if not data:
        logging.info(f"{company_cd} - No data to load")
        return 0
    
    prepared_data = prepare_insert_data(data, company_cd, extract_time)
    columns = get_column_names()
    conflict_columns = ["company_cd", "sensor_id", "capture_dt"]
    
    pg.insert_data(schema_name, table_name, prepared_data, columns, conflict_columns)
    logging.info(f"âœ… {company_cd} - {len(prepared_data)} rows inserted (duplicates ignored).")
    
    return len(prepared_data)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5ï¸âƒ£ Variable Management
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def update_variable(key: str, end_extract_time: str) -> None:
    """Update Airflow variable with last extract time"""
    Variable.set(key, end_extract_time)
    logging.info(f"ðŸ“Œ Variable `{key}` Update: {end_extract_time}")

