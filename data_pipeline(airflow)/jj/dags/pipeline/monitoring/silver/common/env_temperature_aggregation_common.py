"""ê³µí†µ í•¨ìˆ˜ ëª¨ë“ˆ - Temperature Aggregation (10ë¶„ ë‹¨ìœ„ ì§‘ê³„)"""
import logging
from datetime import datetime, timedelta, timezone
from plugins.hooks.postgres_hook import PostgresHelper
from plugins.hooks.mysql_hook import MySQLHelper
from airflow.models import Variable


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1ï¸âƒ£ Configuration Constants
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Source Configuration (ì¶”ì¶œ)
SOURCE_TABLE = "temperature"
SOURCE_SCHEMA = "public"
SOURCE_POSTGRES_CONN_ID = "pg_jj_env"  # ì¶”ì¶œìš© PostgreSQL ì—°ê²°

# Target Configuration (ì ì¬)
TARGET_TABLE = "env_temperature"
TARGET_SCHEMA = "ccs_rtf"

# ì ì¬ ëŒ€ìƒ MySQL ì—°ê²° ëª©ë¡
# maria_jj_os_banb_1ì€ ë‚˜ì¤‘ì— ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì¤€ë¹„ (í˜„ì¬ëŠ” ë¹„í™œì„±í™”)
TARGET_MYSQL_CONNECTIONS = [
    # {"conn_id": "maria_jj_os_banb_1", "enabled": False},  # ë‚˜ì¤‘ì— í™œì„±í™” ì˜ˆì •
    {"conn_id": "maria_jj_os_banb_3", "enabled": True},  # í˜„ì¬ í™œì„±í™”
]

INDO_TZ = timezone(timedelta(hours=7))  # ì¸ë„ë„¤ì‹œì•„ ì‹œê°„ (UTC+7)
INITIAL_START_DATE = datetime(2025, 1, 1, 0, 0, 0, tzinfo=INDO_TZ)
INCREMENT_KEY = "last_extract_time_env_temperature_aggregated"
MAX_INCREMENTAL_RANGE_HOURS = 1  # ì¦ë¶„ ì²˜ë¦¬ ì‹œ í•œ ë²ˆì— ìµœëŒ€ ì²˜ë¦¬í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2ï¸âƒ£ Utility Functions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_datetime(dt_str: str) -> datetime:
    """Parse datetime string with timezone support"""
    try:
        dt = datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S.%f")
    except ValueError:
        dt = datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S")
    
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=INDO_TZ)
    return dt


def get_incremental_date_range() -> dict | None:
    """ì¦ë¶„ ì²˜ë¦¬ìš© ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (10ë¶„ ë‹¨ìœ„)
    
    í•œ ë²ˆì— ìµœëŒ€ MAX_INCREMENTAL_RANGE_HOURS ì‹œê°„ë§Œ ì²˜ë¦¬ (ë¹ˆ ì‹œê°„ì´ ê¸¸ ë•Œ í•œ ë²ˆì— ë„ˆë¬´ ë§ì´ ì²˜ë¦¬í•˜ëŠ” ê²ƒì„ ë°©ì§€)
    ìµœëŒ€ ì‹œê°„: í˜„ì¬ ì¸ë„ë„¤ì‹œì•„ ì‹œê°„ì„ 10ë¶„ ë‹¨ìœ„ë¡œ ì •ê·œí™” (í˜„ì¬ ì‹œì ê¹Œì§€ ê°€ëŠ¥í•œ ìµœëŒ€ ë²”ìœ„)
    
    Returns:
        Dictionary with 'start_date' and 'end_date' strings, or None if no data to process
    """
    last_extract_time = Variable.get(INCREMENT_KEY, default_var=None)
    
    if not last_extract_time:
        # Variableì´ ì—†ìœ¼ë©´ ìµœê·¼ 1ì¼ ì „ë¶€í„° ì‹œì‘
        now_indo = datetime.now(INDO_TZ)
        start_date = (now_indo - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
        logging.info(f"Variableì´ ì—†ì–´ ìµœê·¼ 1ì¼ ì „({start_date.strftime('%Y-%m-%d %H:%M:%S')})ë¶€í„° ìˆ˜ì§‘")
        # 10ë¶„ ë‹¨ìœ„ë¡œ ì •ê·œí™”
        start_date = _normalize_to_10min(start_date)
    else:
        last_time = parse_datetime(last_extract_time)
        # last_extract_timeì€ ì´ë¯¸ 10ë¶„ ë‹¨ìœ„ë¡œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ë‹¤ìŒ 10ë¶„ êµ¬ê°„ë¶€í„° ì‹œì‘
        last_time_normalized = _normalize_to_10min(last_time)
        start_date = last_time_normalized + timedelta(minutes=10)
        logging.info(f"ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„: {last_extract_time} â†’ ë‹¤ìŒ ìˆ˜ì§‘ ì‹œì‘ ì‹œê°„: {start_date}")
    
    # í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ ì¢…ë£Œ ì‹œê°„: ì‹œì‘ ì‹œê°„ + MAX_INCREMENTAL_RANGE_HOURS ì‹œê°„
    max_batch_end_date = start_date + timedelta(hours=MAX_INCREMENTAL_RANGE_HOURS)
    
    # ì‹¤ì œ ìµœëŒ€ ì¢…ë£Œ ì‹œê°„: í˜„ì¬ ì¸ë„ë„¤ì‹œì•„ ì‹œê°„ì„ 10ë¶„ ë‹¨ìœ„ë¡œ ì •ê·œí™”
    # (í˜„ì¬ ì‹œê°„ì´ 14:10ì´ë©´ 14:10ìœ¼ë¡œ ì •ê·œí™”ë˜ì–´ ìµœëŒ€ 14:10ê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥)
    now_indo = datetime.now(INDO_TZ)
    actual_max_end_date = _normalize_to_10min(now_indo)
    
    # ë‘˜ ì¤‘ ì‘ì€ ê°’ì„ ì‚¬ìš© (í•œ ë²ˆì— ì²˜ë¦¬í•  ì–‘ ì œí•œ)
    end_date = min(max_batch_end_date, actual_max_end_date)
    
    if start_date >= end_date:
        logging.info(f"âš ï¸ ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ: {start_date} >= {end_date}")
        return None
    
    logging.info(f"ğŸ“… ì¦ë¶„ ì²˜ë¦¬ ë²”ìœ„: {start_date} ~ {end_date} (ìµœëŒ€ {MAX_INCREMENTAL_RANGE_HOURS}ì‹œê°„ ì œí•œ ì ìš©, í˜„ì¬ ì‹œê°„: {now_indo.strftime('%Y-%m-%d %H:%M:%S')})")
    
    return {
        "start_date": start_date.strftime("%Y-%m-%d %H:%M:%S"),
        "end_date": end_date.strftime("%Y-%m-%d %H:%M:%S")
    }


def get_backfill_date_range() -> dict | None:
    """ë°±í•„ ì²˜ë¦¬ìš© ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
    
    ì¢…ë£Œ ì‹œê°„: í˜„ì¬ ì¸ë„ë„¤ì‹œì•„ ì‹œê°„ -1ì‹œê°„ (10ë¶„ ë‹¨ìœ„ë¡œ ì •ê·œí™”)
    
    Returns:
        Dictionary with 'backfill_start_date' and 'backfill_end_date' strings, or None if no data to process
    """
    last_extract_time = Variable.get(INCREMENT_KEY, default_var=None)
    
    if not last_extract_time:
        start_date = INITIAL_START_DATE
        logging.info(f"ì´ˆê¸° ì‹œì‘ ë‚ ì§œ ì‚¬ìš©: {start_date}")
        # 10ë¶„ ë‹¨ìœ„ë¡œ ì •ê·œí™”
        start_date = _normalize_to_10min(start_date)
    else:
        last_time = parse_datetime(last_extract_time)
        # last_extract_timeì€ ì´ë¯¸ 10ë¶„ ë‹¨ìœ„ë¡œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ë‹¤ìŒ 10ë¶„ êµ¬ê°„ë¶€í„° ì‹œì‘
        last_time_normalized = _normalize_to_10min(last_time)
        start_date = last_time_normalized + timedelta(minutes=10)
        logging.info(f"ì´ì „ ì§„í–‰ ì§€ì  ì‚¬ìš©: {last_extract_time} â†’ ë‹¤ìŒ ìˆ˜ì§‘ ì‹œì‘ ì‹œê°„: {start_date}")
    
    # ì¢…ë£Œ ì‹œê°„: í˜„ì¬ ì¸ë„ë„¤ì‹œì•„ ì‹œê°„ -1ì‹œê°„ (10ë¶„ ë‹¨ìœ„ë¡œ ì •ê·œí™”)
    now_indo = datetime.now(INDO_TZ)
    end_date = now_indo - timedelta(hours=1)
    end_date = _normalize_to_10min(end_date)
    
    if start_date >= end_date:
        logging.info(f"âš ï¸ ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ: {start_date} >= {end_date}")
        return None
    
    logging.info(f"ğŸ“… ë°±í•„ ì²˜ë¦¬ ë²”ìœ„: {start_date} ~ {end_date}")
    
    return {
        "backfill_start_date": start_date.strftime("%Y-%m-%d %H:%M:%S"),
        "backfill_end_date": end_date.strftime("%Y-%m-%d %H:%M:%S")
    }


def _normalize_to_10min(dt: datetime) -> datetime:
    """10ë¶„ ë‹¨ìœ„ë¡œ ì •ê·œí™” (ì˜ˆ: 14:23:45 -> 14:20:00)
    
    Args:
        dt: Datetime to normalize
    
    Returns:
        Datetime normalized to 10-minute boundary
    """
    # ë¶„ì„ 10ìœ¼ë¡œ ë‚˜ëˆˆ ëª«ì— 10ì„ ê³±í•´ì„œ 10ë¶„ ë‹¨ìœ„ë¡œ ë§Œë“¤ê¸°
    normalized_minute = (dt.minute // 10) * 10
    return dt.replace(minute=normalized_minute, second=0, microsecond=0)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3ï¸âƒ£ Data Processing
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_extraction_sql(start_date: str, end_date: str) -> str:
    """PostgreSQLì—ì„œ ë°ì´í„° ì¶”ì¶œ ì¿¼ë¦¬ ìƒì„±
    
    Args:
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD HH:MM:SS)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD HH:MM:SS)
    
    Returns:
        SQL query string (PostgreSQL)
    """
    return f"""
        SELECT
            'OSR_ENV' AS sensor_id,
            (
                to_timestamp(
                    floor(
                        extract(
                            epoch FROM (
                                capture_dt AT TIME ZONE 'Asia/Seoul'
                            )
                        ) / 600
                    ) * 600
                )
                AT TIME ZONE 'Asia/Jakarta'
            ) AS "time",
            ROUND(AVG(t1)::numeric, 2) AS temp,
            ROUND(AVG(t2)::numeric, 2) AS humidity,
            ROUND(AVG(t3)::numeric, 2) AS particle
        FROM {SOURCE_SCHEMA}.{SOURCE_TABLE}
        WHERE capture_dt AT TIME ZONE 'Asia/Seoul' AT TIME ZONE 'Asia/Jakarta' >= '{start_date}'::timestamp
          AND capture_dt AT TIME ZONE 'Asia/Seoul' AT TIME ZONE 'Asia/Jakarta' < '{end_date}'::timestamp
          AND sensor_id = 'TEMPIOT-A207'
        GROUP BY 1, 2
        ORDER BY 2 DESC
    """


def build_insert_sql_mysql() -> str:
    """MySQL/MariaDB ì ì¬ìš© INSERT ... ON DUPLICATE KEY UPDATE ì¿¼ë¦¬ ìƒì„±
    
    Returns:
        SQL query string (MySQL/MariaDB)
    """
    return f"""
        INSERT INTO {TARGET_SCHEMA}.{TARGET_TABLE} (
            sensor_id,
            `time`,
            temp,
            humidity,
            particle
        ) VALUES (%s, %s, %s, %s, %s)
        ON DUPLICATE KEY UPDATE
            temp = VALUES(temp),
            humidity = VALUES(humidity),
            particle = VALUES(particle)
    """


def create_target_table_if_not_exists(mysql: MySQLHelper, conn_id: str) -> None:
    """íƒ€ê²Ÿ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„± (MySQL/MariaDB)
    
    Args:
        mysql: MySQLHelper instance
        conn_id: Connection ID (ë¡œê¹…ìš©)
    """
    create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {TARGET_SCHEMA}.{TARGET_TABLE} (
            sensor_id VARCHAR(50) NOT NULL,
            `time` DATETIME NOT NULL,
            temp DECIMAL(10, 2),
            humidity DECIMAL(10, 2),
            particle DECIMAL(10, 2),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            PRIMARY KEY (sensor_id, `time`),
            INDEX idx_bucket_time (`time`)
        )
    """
    
    try:
        with mysql.hook.get_conn() as conn, conn.cursor() as cursor:
            cursor.execute(f"CREATE SCHEMA IF NOT EXISTS {TARGET_SCHEMA}")
            conn.commit()
            cursor.execute(create_table_sql)
            conn.commit()
        logging.info(f"âœ… í…Œì´ë¸” í™•ì¸/ìƒì„± ì™„ë£Œ: {conn_id} - {TARGET_SCHEMA}.{TARGET_TABLE}")
    except Exception as e:
        logging.error(f"âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {conn_id} - {str(e)}")
        raise


def process_aggregation(start_date: str, end_date: str) -> dict:
    """10ë¶„ ë‹¨ìœ„ ì§‘ê³„ ì²˜ë¦¬
    
    Args:
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD HH:MM:SS)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD HH:MM:SS)
    
    Returns:
        Processing result dictionary
    """
    total_rows_processed = 0
    target_results = {}
    
    try:
        # 1. PostgreSQLì—ì„œ ë°ì´í„° ì¶”ì¶œ
        logging.info(f"ğŸ“¥ ë°ì´í„° ì¶”ì¶œ ì‹œì‘: {start_date} ~ {end_date}")
        pg_source = PostgresHelper(conn_id=SOURCE_POSTGRES_CONN_ID)
        
        extraction_sql = build_extraction_sql(start_date, end_date)
        logging.info(f"ì¶”ì¶œ ì¿¼ë¦¬:\n{extraction_sql}")
        
        extracted_data = pg_source.execute_query(extraction_sql, task_id="extract_data", xcom_key=None)
        
        if not extracted_data:
            logging.warning("âš ï¸ ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "status": "success",
                "start_date": start_date,
                "end_date": end_date,
                "rows_processed": 0,
                "extract_time": datetime.utcnow().isoformat(),
                "targets": {}
            }
        
        logging.info(f"âœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {len(extracted_data)} rows")
        
        # ì¶”ì¶œëœ ë°ì´í„°ì˜ ì‹¤ì œ ë§ˆì§€ë§‰ ì‹œê°„ ì°¾ê¸° (timeì€ ì¸ë±ìŠ¤ 1)
        actual_last_time = None
        if extracted_data:
            # timeì€ datetime ê°ì²´ì´ë¯€ë¡œ ì§ì ‘ ë¹„êµ ê°€ëŠ¥
            actual_last_time = max(row[1] for row in extracted_data)
            # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            if isinstance(actual_last_time, datetime):
                actual_last_time = actual_last_time.strftime("%Y-%m-%d %H:%M:%S")
            else:
                actual_last_time = str(actual_last_time)
            logging.info(f"ğŸ“Œ ì‹¤ì œ ì ì¬ë  ë§ˆì§€ë§‰ ì‹œê°„: {actual_last_time}")
        
        # 2. í™œì„±í™”ëœ MySQL ì—°ê²°ì— ë°ì´í„° ì ì¬
        insert_sql = build_insert_sql_mysql()
        enabled_targets = [t for t in TARGET_MYSQL_CONNECTIONS if t.get("enabled", False)]
        
        if not enabled_targets:
            logging.warning("âš ï¸ í™œì„±í™”ëœ ì ì¬ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "status": "success",
                "start_date": start_date,
                "end_date": end_date,
                "actual_last_time": None,
                "rows_processed": 0,
                "extract_time": datetime.utcnow().isoformat(),
                "targets": {}
            }
        
        for target_config in enabled_targets:
            conn_id = target_config["conn_id"]
            try:
                logging.info(f"ğŸ’¾ ë°ì´í„° ì ì¬ ì‹œì‘: {conn_id}")
                mysql_target = MySQLHelper(conn_id=conn_id)
                
                # í…Œì´ë¸” ìƒì„± (ì—†ìœ¼ë©´)
                create_target_table_if_not_exists(mysql_target, conn_id)
                
                # ë°ì´í„°ë¥¼ íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (etl_extract_time ì œì™¸)
                insert_data = []
                for row in extracted_data:
                    # rowëŠ” (sensor_id, time, temp, humidity, particle, etl_extract_time)
                    # í…Œì´ë¸”ì€ etl_extract_timeì´ ì—†ê³  created_atì€ ìë™ ìƒì„±ë˜ë¯€ë¡œ ë§ˆì§€ë§‰ í•­ëª© ì œê±°
                    insert_data.append(row[:5])  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
                
                # ë°°ì¹˜ë¡œ INSERT ì‹¤í–‰
                with mysql_target.hook.get_conn() as conn, conn.cursor() as cursor:
                    cursor.executemany(insert_sql, insert_data)
                    rows_affected = cursor.rowcount
                    conn.commit()
                
                # ê²°ê³¼ í™•ì¸ì„ ìœ„í•œ COUNT ì¿¼ë¦¬
                count_sql = f"""
                    SELECT COUNT(*) 
                    FROM {TARGET_SCHEMA}.{TARGET_TABLE}
                    WHERE `time` >= '{start_date}' 
                      AND `time` < '{end_date}'
                """
                count_result = mysql_target.execute_query(count_sql, task_id=f"count_records_{conn_id}", xcom_key=None)
                row_count = count_result[0][0] if count_result and len(count_result) > 0 else 0
                
                total_rows_processed = max(total_rows_processed, row_count)
                target_results[conn_id] = {
                    "status": "success",
                    "rows_inserted": rows_affected,
                    "rows_counted": row_count
                }
                
                logging.info(f"âœ… ë°ì´í„° ì ì¬ ì™„ë£Œ: {conn_id} - {row_count} ê°œì˜ 10ë¶„ ë‹¨ìœ„ ë ˆì½”ë“œ")
                
            except Exception as e:
                logging.error(f"âŒ ë°ì´í„° ì ì¬ ì‹¤íŒ¨: {conn_id} - {str(e)}", exc_info=True)
                target_results[conn_id] = {
                    "status": "failed",
                    "error": str(e)
                }
                # í•œ íƒ€ê²Ÿ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ íƒ€ê²Ÿì€ ê³„ì† ì²˜ë¦¬
        
        return {
            "status": "success",
            "start_date": start_date,
            "end_date": end_date,
            "actual_last_time": actual_last_time,  # ì‹¤ì œ ì ì¬ëœ ë§ˆì§€ë§‰ ì‹œê°„
            "rows_processed": total_rows_processed,
            "extract_time": datetime.utcnow().isoformat(),
            "targets": target_results
        }
        
    except Exception as e:
        logging.error(f"âŒ ì§‘ê³„ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        return {
            "status": "failed",
            "error": str(e),
            "start_date": start_date,
            "end_date": end_date,
            "targets": target_results
        }


def update_variable(end_date: str) -> None:
    """Airflow Variable ì—…ë°ì´íŠ¸"""
    Variable.set(INCREMENT_KEY, end_date)
    logging.info(f"ğŸ“Œ Variable `{INCREMENT_KEY}` ì—…ë°ì´íŠ¸: {end_date}")
