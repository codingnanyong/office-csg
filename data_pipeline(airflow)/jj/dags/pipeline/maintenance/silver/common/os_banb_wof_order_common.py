"""ê³µí†µ í•¨ìˆ˜ ëª¨ë“ˆ - OS BANB WOF Order Silver"""
import logging
from datetime import datetime, timedelta, timezone
from plugins.hooks.postgres_hook import PostgresHelper
from airflow.models import Variable


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1ï¸âƒ£ Configuration Constants
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Default Configuration
POSTGRES_CONN_ID = "pg_jj_maintenance_dw"
SOURCE_SCHEMA = "bronze"
SOURCE_TABLE = "wof_order_raw"
TARGET_SCHEMA = "silver"
TARGET_TABLE = "os_banb_wof_order"
INDO_TZ = timezone(timedelta(hours=7))

# Filter Conditions
TARGET_MACH_IDS = ['3110COP00009', '3110COP00001', '3110COP00015']


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2ï¸âƒ£ Utility Functions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_datetime(dt_str: str) -> datetime:
    """Parse datetime string with microsecond support"""
    try:
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S.%f")
    except ValueError:
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3ï¸âƒ£ Data Extraction (Bronze â†’ Transform)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_extract_sql(start_date: str = None, end_date: str = None) -> str:
    """Build SQL query for extracting and transforming data from Bronze
    - Bronzeì˜ upd_date ê¸°ì¤€ ì¦ë¶„ ìˆ˜ì§‘ (ìˆ˜ì •ëœ ë°ì´í„°)
    - upd_date IS NULL í•­ìƒ ìˆ˜ì§‘ (í•œ ë²ˆë„ ìˆ˜ì • ì•ˆ ëœ ë°ì´í„°)
    - íŠ¹ì • ì„¤ë¹„(MACH_ID) í•„í„°ë§
    
    Args:
        start_date: ì‹œìž‘ ë‚ ì§œ (incrementalìš©, Noneì´ë©´ ì „ì²´ ì¡°íšŒ)
        end_date: ì¢…ë£Œ ë‚ ì§œ (incrementalìš©, Noneì´ë©´ ì „ì²´ ì¡°íšŒ)
    """
    mach_id_list = "', '".join(TARGET_MACH_IDS)
    
    base_sql = f'''
        SELECT 
            wo_date,
            wo_yymm,
            wo_orgn,
            wo_no,
            mach_id,
            defe_date,
            defe_cd,
            defe_content,
            defe_cd1,
            defe_content1,
            defe_cd2,
            defe_content2,
            defe_cd3,
            defe_content3,
            defe_cd4,
            defe_content4,
            solu_date,
            solu_cd,
            solu_content,
            etl_extract_time
        FROM {SOURCE_SCHEMA}.{SOURCE_TABLE}
        WHERE mach_id IN ('{mach_id_list}')
    '''
    
    if start_date and end_date:
        # Incremental: date range filter
        base_sql += f'''
          AND (
              upd_date BETWEEN '{start_date}'::timestamp AND '{end_date}'::timestamp
              OR upd_date IS NULL
          )
        '''
    
    base_sql += '''
        ORDER BY wo_date, wo_yymm, wo_orgn, wo_no
    '''
    
    return base_sql


def extract_and_transform_data(
    postgres: PostgresHelper, 
    start_date: str = None, 
    end_date: str = None
) -> tuple:
    """Extract data from Bronze and transform"""
    sql = build_extract_sql(start_date, end_date)
    
    if start_date and end_date:
        logging.info("ðŸ” Bronze ë°ì´í„° ì¶”ì¶œ ë° ë³€í™˜ ì‹œìž‘ (Incremental)")
        logging.info(f"ê¸°ê°„: {start_date} ~ {end_date}")
    else:
        logging.info("ðŸ” Bronze ì „ì²´ ë°ì´í„° ì¶”ì¶œ ë° ë³€í™˜ ì‹œìž‘ (Backfill)")
    
    logging.info(f"ëŒ€ìƒ ì„¤ë¹„: {TARGET_MACH_IDS}")
    logging.info(f"ì‹¤í–‰ ì¿¼ë¦¬:\n{sql}")
    
    try:
        with postgres.hook.get_conn() as conn, conn.cursor() as cursor:
            cursor.execute(sql)
            data = cursor.fetchall()
            row_count = len(data) if data else 0
            
            logging.info(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {row_count:,} rows")
            return data, row_count
            
    except Exception as e:
        logging.error(f"âŒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        raise


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4ï¸âƒ£ Data Loading (Silver)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def prepare_insert_data(data: list, ingest_time: datetime) -> list:
    """Prepare data for Silver layer insertion"""
    if not data:
        return []
    
    # Add etl_ingest_time to each row
    return [
        (*row, ingest_time)
        for row in data
    ]


def get_column_names() -> list:
    """Get column names for Silver table"""
    return [
        'wo_date', 'wo_yymm', 'wo_orgn', 'wo_no', 'mach_id',
        'defe_date', 'defe_cd', 'defe_content',
        'defe_cd1', 'defe_content1', 'defe_cd2', 'defe_content2',
        'defe_cd3', 'defe_content3', 'defe_cd4', 'defe_content4',
        'solu_date', 'solu_cd', 'solu_content',
        'etl_extract_time', 'etl_ingest_time'
    ]


def load_data_to_silver(
    postgres: PostgresHelper, 
    data: list,
    schema_name: str = TARGET_SCHEMA,
    table_name: str = TARGET_TABLE
) -> int:
    """Load data to Silver layer using PostgresHelper"""
    if not data:
        logging.warning("âš ï¸ ì ìž¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0
    
    columns = get_column_names()
    conflict_columns = ['wo_yymm', 'wo_orgn', 'wo_no']
    
    logging.info(f"ðŸ“¦ Silver ë ˆì´ì–´ ì ìž¬ ì‹œìž‘: {len(data):,} rows")
    
    try:
        postgres.insert_data(
            schema_name=schema_name,
            table_name=table_name,
            data=data,
            columns=columns,
            conflict_columns=conflict_columns,
            chunk_size=1000
        )
        
        loaded_count = len(data)
        logging.info(f"âœ… Silver ì ìž¬ ì™„ë£Œ: {loaded_count:,} rows")
        return loaded_count
        
    except Exception as e:
        logging.error(f"âŒ Silver ì ìž¬ ì‹¤íŒ¨: {str(e)}")
        raise


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5ï¸âƒ£ Variable Management
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def update_variable(variable_key: str, end_extract_time: str) -> None:
    """Update Airflow variable with last extract time"""
    Variable.set(variable_key, end_extract_time)
    logging.info(f"ðŸ“Œ Variable `{variable_key}` Update: {end_extract_time}")

