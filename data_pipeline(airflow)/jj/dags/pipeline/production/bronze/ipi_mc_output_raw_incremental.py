import logging
from airflow import DAG
from airflow.exceptions import AirflowSkipException
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from plugins.hooks.mssql_hook import MSSQLHelper
from plugins.hooks.postgres_hook import PostgresHelper
from dags.pipeline.production.bronze.common.ipi_mc_output_raw_common import (
    parse_datetime,
    extract_data,
    load_data,
    update_variable,
    INDO_TZ,
    MSSQL_CONN_ID,
    POSTGRES_CONN_ID
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ Configuration Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_ARGS = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'sla': timedelta(hours=2)
}

# Database Configuration
INCREMENT_KEY = "last_extract_time_ipi_mc_output_raw"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ Daily Incremental Collection
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def daily_incremental_collection_task(**kwargs) -> dict:
    """ë§¤ì¼ ìµœì‹  ë°ì´í„°ë§Œ ìˆ˜ì§‘í•˜ëŠ” íƒœìŠ¤í¬"""
    # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë‚  ë°ì´í„° ìˆ˜ì§‘
    last_extract_time_str = Variable.get(INCREMENT_KEY, default_var=None)
    
    if last_extract_time_str:
        # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì„ íŒŒì‹±
        last_extract_time = parse_datetime(last_extract_time_str)
        
        # ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„ì˜ ë‹¤ìŒ ë‚  00:00:00ë¶€í„° 23:59:59ê¹Œì§€
        start_date = last_extract_time.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
        end_date = start_date.replace(hour=23, minute=59, second=59, microsecond=999999)
        
        logging.info(f"ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„: {last_extract_time_str}")
        logging.info(f"ë‹¤ìŒ ë‚  ë°ì´í„° ìˆ˜ì§‘: {start_date.strftime('%Y-%m-%d')}")
    else:
        # Variableì´ ì—†ìœ¼ë©´ ì–´ì œ ë°ì´í„° ìˆ˜ì§‘
        yesterday = datetime.now(INDO_TZ) - timedelta(days=1)
        start_date = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = yesterday.replace(hour=23, minute=59, second=59, microsecond=999999)
        logging.info(f"Variableì´ ì—†ì–´ì„œ ì–´ì œ ë°ì´í„° ìˆ˜ì§‘: {yesterday.strftime('%Y-%m-%d')}")
    
    start_str = start_date.strftime("%Y-%m-%d %H:%M:%S")
    end_str = end_date.strftime("%Y-%m-%d %H:%M:%S")
    
    logging.info(f"ğŸ“… ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {start_str} ~ {end_str}")
    logging.info(f"ğŸ“Š ì²˜ë¦¬ ë‚ ì§œ: {start_date.strftime('%Y-%m-%d')}")
    
    try:
        mssql = MSSQLHelper(conn_id=MSSQL_CONN_ID)
        pg = PostgresHelper(conn_id=POSTGRES_CONN_ID)
        
        # ë°ì´í„° ì¶”ì¶œ ë° ì ì¬
        data, row_count = extract_data(mssql, start_str, end_str)
        
        if row_count > 0:
            extract_time = datetime.utcnow()
            load_data(pg, data, extract_time)
            logging.info(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {row_count} rows")
            
            # Variable ì—…ë°ì´íŠ¸
            update_variable(INCREMENT_KEY, end_str)
            
            return {
                "status": "daily_incremental_completed",
                "date": start_date.strftime("%Y-%m-%d"),
                "rows_processed": row_count,
                "start_time": start_str,
                "end_time": end_str,
                "extract_time": extract_time.isoformat()
            }
        else:
            logging.info(f"âš ï¸ ìˆ˜ì§‘í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {start_str} ~ {end_str}")
            
            # Variable ì—…ë°ì´íŠ¸ (ë°ì´í„°ê°€ ì—†ì–´ë„ ì‹œê°„ì€ ì—…ë°ì´íŠ¸)
            update_variable(INCREMENT_KEY, end_str)
            
            return {
                "status": "daily_incremental_completed_no_data",
                "date": start_date.strftime("%Y-%m-%d"),
                "rows_processed": 0,
                "start_time": start_str,
                "end_time": end_str,
                "message": "ìˆ˜ì§‘í•  ë°ì´í„°ê°€ ì—†ìŒ"
            }
    except Exception as e:
        # ì—°ê²° ì‹¤íŒ¨ ë“± ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ì¸ ê²½ìš° Skip ì²˜ë¦¬
        error_str = str(e)
        error_lower = error_str.lower()
        is_connection_error = (
            "connection" in error_lower or
            "timeout" in error_lower or
            "timed out" in error_lower or
            "connection reset" in error_lower or
            "reset by peer" in error_lower or
            "errno" in error_lower or
            "network" in error_lower or
            "could not connect" in error_lower or
            "unable to connect" in error_lower or
            "login failed" in error_lower or
            "server is not found" in error_lower
        )
        
        if is_connection_error:
            logging.warning(f"âš ï¸ ì—°ê²° ì‹¤íŒ¨: {error_str} - íƒœìŠ¤í¬ Skip")
            # Skip ì „ì— Variable ì—…ë°ì´íŠ¸ (ì—°ê²° ì‹¤íŒ¨í•´ë„ ì‹œê°„ì€ ì—…ë°ì´íŠ¸í•˜ì—¬ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì˜¬ë°”ë¥¸ ì‹œì ë¶€í„° ì¬ì‹œë„)
            try:
                update_variable(INCREMENT_KEY, end_str)
                logging.info(f"âœ… Variable '{INCREMENT_KEY}' ì—…ë°ì´íŠ¸ (ì—°ê²° ì‹¤íŒ¨ë¡œ Skip): {end_str}")
            except Exception as var_err:
                logging.warning(f"âš ï¸ Variable ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ë¬´ì‹œ): {var_err}")
            
            skip_msg = (
                f"â­ï¸ IPI MC Output ETL ì¤‘ ì—°ê²° ë¶ˆê°€ - íƒœìŠ¤í¬ Skip\n"
                f"ì›ì¸: {error_str}\n"
                f"ì„¤ëª…: ì†ŒìŠ¤ ë˜ëŠ” íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\n"
                f"      Variableì€ ì—…ë°ì´íŠ¸ë˜ì—ˆìœ¼ë¯€ë¡œ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì¬ì‹œë„ë©ë‹ˆë‹¤."
            )
            logging.warning(skip_msg)
            raise AirflowSkipException(skip_msg) from e
        
        # ê·¸ ì™¸ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ raise
        logging.error(f"âŒ IPI MC Output ETL ì‹¤íŒ¨: {e}")
        raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ DAG Definition
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with DAG(
    dag_id="ipi_mc_output_raw_incremental",
    default_args=DEFAULT_ARGS,
    schedule_interval=None,  # ë§¤ì¼ ì‹¤í–‰
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["CKP","IP","raw", "bronze layer", "incremental", "production"]
) as dag:
    
    daily_collection = PythonOperator(
        task_id="daily_incremental_collection",
        python_callable=daily_incremental_collection_task,
        provide_context=True,
    )
    
    daily_collection
