import logging
import io
from typing import Optional
from datetime import datetime
from airflow.hooks.postgres_hook import PostgresHook
from airflow.plugins_manager import AirflowPlugin
from psycopg2.extras import execute_values, execute_batch

logger = logging.getLogger(__name__)

class PostgresHelper:
    def __init__(self, conn_id: str):
        self.conn_id = conn_id
        self.hook = PostgresHook(postgres_conn_id=self.conn_id)

    def check_table(self, schema_name: str, table_name: str) -> bool:
        check_table_sql = """
            SELECT EXISTS (
                SELECT 1 FROM information_schema.tables 
                WHERE table_schema = %(schema_name)s
                AND table_name = %(table_name)s
            );
        """
        try:
            with self.hook.get_conn() as conn, conn.cursor() as cursor:
                cursor.execute(check_table_sql, {"schema_name": schema_name, "table_name": table_name})
                table_exists = cursor.fetchone()[0]

                if not table_exists:
                    logger.warning(f"âš ï¸ Table `{schema_name}.{table_name}` does not exist.")
                    return False

                logger.info(f"âœ… Table `{schema_name}.{table_name}` exists in the database.")
                return True

        except Exception as e:
            logger.error(f"âŒ Table check failed: {str(e)}")
            raise

    def clean_table(self, schema_name: str, table_name: str):
        delete_sql = f"DELETE FROM {schema_name}.{table_name};"
        try:
            with self.hook.get_conn() as conn, conn.cursor() as cursor:
                cursor.execute(delete_sql)
                conn.commit()
                logger.info(f"ğŸ—‘ï¸ Table `{schema_name}.{table_name}` cleaned!")

        except Exception as e:
            conn.rollback()
            logger.error(f"âŒ Cleaning table `{schema_name}.{table_name}` failed: {str(e)}")
            raise

    def execute_query(self, sql: str, task_id: str, xcom_key: Optional[str], **kwargs):
        try:
            with self.hook.get_conn() as conn, conn.cursor() as cursor:
                cursor.execute(sql)
                records = cursor.fetchall()

                if not records:
                    logger.warning(f"âš ï¸ Warning: No records found for `{task_id}`!")
                    return None

                logger.info(f"âœ… `{task_id}` Data: {records[:5]} ... (Total: {len(records)})")

                ti = kwargs.get("ti")
                if ti and xcom_key: 
                    ti.xcom_push(key=xcom_key, value=records)
                elif ti:
                    logger.info(f"[INFO] Skipping XCom push for `{task_id}` because xcom_key is None.")
                else:
                    logger.warning("âš ï¸ TaskInstance (`ti`) not found, XCom push skipped.")

                return records

        except Exception as e:
            logger.error(f"âŒ Query execution failed for `{task_id}`: {str(e)}")
            raise


    def insert_data(self, schema_name: str, table_name: str, data: list, columns: list = None, conflict_columns: list = None, chunk_size: int = 1000) -> None:
        """
        INSERT ë¬¸ì„ ì‚¬ìš©í•œ ë°ì´í„° ì‚½ì… (ON CONFLICT ì§€ì›)
        execute_valuesë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ í–‰ì„ í•œ ë²ˆì— ì‚½ì…
        """
        if not data:
            logger.warning(f"âš ï¸ Warning: No data to insert into `{schema_name}.{table_name}`!")
            return

        if not isinstance(data, list) or not all(isinstance(row, tuple) for row in data):
            logger.error(f"âŒ Data format error: Expected list of tuples but got {type(data)} with first element {type(data[0])}")
            return

        if conflict_columns:
            # Get all columns except conflict columns for UPDATE
            if columns:
                update_columns = [col for col in columns if col not in conflict_columns]
                if update_columns:
                    update_clause = f"DO UPDATE SET {', '.join([f'{col} = EXCLUDED.{col}' for col in update_columns])}"
                else:
                    update_clause = "DO NOTHING"
            else:
                update_clause = "DO NOTHING"
            conflict_clause = f"ON CONFLICT ({', '.join(conflict_columns)}) {update_clause}"
        else:
            conflict_clause = ""

        insert_sql = f"""
            INSERT INTO {schema_name}.{table_name} VALUES %s {conflict_clause}
        """

        try:
            with self.hook.get_conn() as conn, conn.cursor() as cursor:
                # Process data in chunks for large datasets
                total_inserted = 0
                for i in range(0, len(data), chunk_size):
                    chunk = data[i:i + chunk_size]
                    execute_values(cursor, insert_sql, chunk)
                    total_inserted += len(chunk)
                    logger.info(f"ğŸ“¦ Processed chunk {i//chunk_size + 1}: {len(chunk):,} records")
                
                conn.commit()
                logger.info(f"âœ… Successfully inserted {total_inserted:,} records into `{schema_name}.{table_name}`.")

        except Exception as e:
            logger.error(f"âŒ INSERT into `{schema_name}.{table_name}` failed: {str(e)}")
            raise

    def bulk_insert(self, schema_name: str, table_name: str, data: list, columns: list = None, chunk_size: int = 10000) -> None:
        """
        PostgreSQL COPY ëª…ë ¹ì„ ì‚¬ìš©í•œ ì§„ì •í•œ Bulk Insert
        INSERTë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ì§€ë§Œ, ON CONFLICTëŠ” ì§€ì›í•˜ì§€ ì•ŠìŒ
        
        Args:
            schema_name: ìŠ¤í‚¤ë§ˆ ì´ë¦„
            table_name: í…Œì´ë¸” ì´ë¦„
            data: ì‚½ì…í•  ë°ì´í„° (list of tuples)
            columns: ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ì»¬ëŸ¼)
            chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’: 10000)
        
        Returns:
            None
        """
        if not data:
            logger.warning(f"âš ï¸ Warning: No data to bulk insert into `{schema_name}.{table_name}`!")
            return

        if not isinstance(data, list) or not all(isinstance(row, tuple) for row in data):
            logger.error(f"âŒ Data format error: Expected list of tuples but got {type(data)} with first element {type(data[0])}")
            return

        try:
            with self.hook.get_conn() as conn, conn.cursor() as cursor:
                # ì»¬ëŸ¼ ì§€ì • ì—¬ë¶€ì— ë”°ë¼ COPY ë¬¸ êµ¬ì„±
                if columns:
                    columns_str = f"({', '.join(columns)})"
                else:
                    columns_str = ""
                
                # COPY FROM STDIN ì‚¬ìš©
                copy_sql = f"COPY {schema_name}.{table_name} {columns_str} FROM STDIN WITH (FORMAT CSV, DELIMITER E'\\t', NULL '')"
                
                total_inserted = 0
                # ë°ì´í„°ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
                for i in range(0, len(data), chunk_size):
                    chunk = data[i:i + chunk_size]
                    
                    # StringIOë¥¼ ì‚¬ìš©í•˜ì—¬ CSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    csv_buffer = io.StringIO()
                    for row in chunk:
                        # ê° ê°’ì„ íƒ­ìœ¼ë¡œ êµ¬ë¶„í•˜ê³ , Noneì€ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                        csv_row = '\t'.join([
                            '' if val is None else str(val).replace('\t', ' ').replace('\n', ' ').replace('\r', ' ')
                            for val in row
                        ])
                        csv_buffer.write(csv_row + '\n')
                    
                    csv_buffer.seek(0)
                    
                    # COPY FROM STDIN ì‹¤í–‰
                    cursor.copy_expert(copy_sql, csv_buffer)
                    total_inserted += len(chunk)
                    logger.info(f"ğŸ“¦ Bulk insert chunk {i//chunk_size + 1}: {len(chunk):,} records")
                
                conn.commit()
                logger.info(f"âœ… Successfully bulk inserted {total_inserted:,} records into `{schema_name}.{table_name}` using COPY command.")

        except Exception as e:
            logger.error(f"âŒ Bulk INSERT into `{schema_name}.{table_name}` failed: {str(e)}")
            raise


    def execute_update(self, sql: str, task_id: str, parameters: tuple = None):
        try:
            with self.hook.get_conn() as conn, conn.cursor() as cursor:
                cursor.execute(sql, parameters)
                conn.commit()
                logger.info(f"âœ… `{task_id}` update executed successfully.")

        except Exception as e:
            logger.error(f"âŒ `{task_id}` update failed: {str(e)}")
            raise

    def clean_table_with_condition(self, schema_name: str, table_name: str, column_name: str, target_date: str):
        delete_sql = f"DELETE FROM {schema_name}.{table_name} WHERE {column_name} = '{target_date}';"
        
        try:
            with self.hook.get_conn() as conn, conn.cursor() as cursor:
                cursor.execute(delete_sql)
                conn.commit()
                logger.info(f"ğŸ§¹ Table `{schema_name}.{table_name}` cleaned where `{column_name}` = '{target_date}'")
        except Exception as e:
            conn.rollback()
            logger.error(f"âŒ Conditional clean failed on `{schema_name}.{table_name}`: {str(e)}")
            raise
