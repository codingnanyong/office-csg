import sys
import logging
import pendulum
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from datetime import datetime, timedelta
from plugins.hooks.postgres_hook import PostgresHelper
from plugins.hooks.mysql_hook import MySQLHelper

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ ì„¤ì • (Config & Logging)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

logger = logging.getLogger(__name__)

CONN_ID_v1 = "mdb_hq_rtls"  
CONN_ID_v2 = "pg_fdw_v1_hq"

EXTRACT_SCHEMA_NAME = "dskorea"
LOAD_SCHEMA_NAME = "rtls"  
TARGET_TABLE_NAME = "at_tag_xy"

mysql_helper = MySQLHelper(CONN_ID_v1)
postgres_helper_v2 = PostgresHelper(CONN_ID_v2)  

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ ë§ˆì§€ë§‰ ì ì¬ ì‹œê°„ ê°€ì ¸ì˜¤ê¸° (Airflow Variables ì‚¬ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_last_ingested_time():
    try:
        last_ingested_time = Variable.get("epaper_raw_last_ingested_time", default_var="2024-06-26 00:00:00")
        logger.info(f"âœ… Retrieved last_ingested_time from Airflow Variables: {last_ingested_time}")
    except Exception as e:
        last_ingested_time = "2025-04-24 00:00:00"
        logger.warning(f"ğŸš€ Variable not found. Using default: {last_ingested_time}")

    return last_ingested_time

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ ì¦ë¶„ ë°ì´í„° ì¡°íšŒ ë° XCom ì €ì¥ (ë°ì´í„° ì—†ì„ ë•Œ ê¸°ë³¸ê°’ ì €ì¥)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_incremental_epaper(**kwargs):
    ti = kwargs["ti"]
    last_ingested_time = ti.xcom_pull(task_ids="get_last_ingested_time", key="last_ingested_time")
    logger.info(f"ğŸ“¥ Fetching incremental epaper data from `{EXTRACT_SCHEMA_NAME}.at_tag_xy` since {last_ingested_time}...")    

    query = f"""
        SELECT tagid, pos_ts, event_Id, OWNER_TAG_NAME, position_xy, zone_id, zone, zone_before, coordi_sys, send_yn, IN_DATE, MOD_DATE
        FROM {EXTRACT_SCHEMA_NAME}.at_tag_xy
        WHERE IN_DATE >= '{last_ingested_time}'
        ORDER BY IN_DATE ASC;
    """

    records = mysql_helper.execute_query(query, task_id="fetch_epaper", xcom_key="epaper_data", **kwargs)

    if records:
        # í˜„ì¬ ì‹œê°„ì„ ì¶”ì¶œ ì‹œì ìœ¼ë¡œ ì„¤ì •
        current_time = datetime.now().isoformat()
        
        processed_records = [
            (tagid, 
             datetime.fromtimestamp(pos_ts / 1000).isoformat() if pos_ts else None,  # pos_tsë¥¼ timestampë¡œ ë³€í™˜ (ë°€ë¦¬ì´ˆ â†’ ì´ˆ)
             event_Id, OWNER_TAG_NAME, 
             position_xy.replace(',', '/') if position_xy and position_xy != 'N/A' else position_xy,  # position_xy í˜•ì‹ ë³€í™˜: 17.83,87.53 â†’ 17.83/87.53
             zone_id, zone, zone_before, coordi_sys, send_yn, IN_DATE.isoformat() if IN_DATE else None, MOD_DATE.isoformat() if MOD_DATE else None,
             current_time,  # extract_time - í˜„ì¬ task ë™ì‘ ì‹œì 
             None   # transform_time
             # load_timeì€ DEFAULT CURRENT_TIMESTAMPì´ë¯€ë¡œ ìë™ìœ¼ë¡œ ì„¤ì •ë¨
            ) 
            for tagid, pos_ts, event_Id, OWNER_TAG_NAME, position_xy, zone_id, zone, zone_before, coordi_sys, send_yn, IN_DATE, MOD_DATE in records
        ]

        logger.info(f"âœ… Retrieved {len(records)} records from `{EXTRACT_SCHEMA_NAME}.at_tag_xy`.")
        ti.xcom_push(key="incremental_epaper_data", value=processed_records)
    else:
        logger.warning("âš ï¸ No new epaper data found.")
        ti.xcom_push(key="incremental_epaper_data", value=[])  

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4ï¸âƒ£ ë§ˆì§€ë§‰ ì ì¬ ì‹œê°„ ì—…ë°ì´íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def update_last_ingested_time(**kwargs):
    ti = kwargs["ti"]
    latest_time = datetime.now().isoformat()
    logger.info(f"âœ… Updating last_ingested_time in Airflow Variables: {latest_time}")
    Variable.set("epaper_raw_last_ingested_time", latest_time)
    logger.info("âœ… Successfully updated last_ingested_time in Airflow Variables.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5ï¸âƒ£ DAG ì •ì˜ (DAG Definition & Task Dependencies)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with DAG(
    dag_id="hq_rtls_epaper_raw",
    default_args=default_args,
    description="HQ RTLS E-Paper Raw Data Ingest Every Minute",
    schedule_interval="*/5 * * * *",  
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["HQ","RTLS","Raw"],
) as dag:

    task_get_last_ingested_time = PythonOperator(
        task_id="get_last_ingested_time",
        python_callable=get_last_ingested_time,
        provide_context=True,
    )

    task_fetch_epaper = PythonOperator(
        task_id="fetch_epaper",
        python_callable=fetch_incremental_epaper,
        provide_context=True,
    )

    task_insert_epaper = PythonOperator(
        task_id="insert_epaper_data",   
        python_callable=lambda ti: postgres_helper_v2.insert_data(
            LOAD_SCHEMA_NAME,
            TARGET_TABLE_NAME,
            [tuple(row) for row in (ti.xcom_pull(task_ids="fetch_epaper", key="incremental_epaper_data") or [])],
            conflict_columns=None  # Raw ë°ì´í„°ëŠ” ì¤‘ë³µ ì œê±° ì—†ì´ ëª¨ë“  ë°ì´í„° ì €ì¥
        ),
        trigger_rule="all_success",
    )

    task_update_last_ingested_time = PythonOperator(
        task_id="update_last_ingested_time",
        python_callable=update_last_ingested_time,
        provide_context=True,
    )

    task_get_last_ingested_time >> task_fetch_epaper >> task_insert_epaper >> task_update_last_ingested_time